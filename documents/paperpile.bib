@ARTICLE{Basavarajeshwari2018-ri,
  title     = "A Survey on weed Detection using Image Processing",
  author    = "{Basavarajeshwari} and Madhavanavar, S P",
  abstract  = "A Survey on weed Detection using Image Processing - written by
               Basavarajeshwari, Prof. S. P. Madhavanavar published on
               2018/04/24 download full article with reference data and
               citations",
  journal   = "International Journal of Engineering Research \& Technology",
  publisher = "IJERT-International Journal of Engineering Research \&
               Technology",
  volume    =  5,
  number    =  6,
  month     =  apr,
  year      =  2018,
  url       = "https://www.ijert.org/research/a-survey-on-weed-detection-using-image-processing-IJERTCONV5IS06011.pdf",
  issn      = "2278-0181"
}

@ARTICLE{Hamuda2016-dw,
  title     = "A survey of image processing techniques for plant extraction and
               segmentation in the field",
  author    = "Hamuda, E and Glavin, M and Jones, E",
  abstract  = "In this review, we present a comprehensive and critical survey
               on image-based plant segmentation techniques. In this context,
               ``segmentation'' refers to the process of classifying an image
               into plant and non-plant pixels. Good performance in this
               process is crucial for further analysis of the plant such as
               plant classification (i.e. identifying the plant as either crop
               or weed), and effective action based on this analysis, e.g.
               precision application of herbicides in smart agriculture
               applications. The survey briefly discusses pre-processing of
               images, before focusing on segmentation. The segmentation stage
               involves the segmentation of plant against the background
               (identifying plant from a background of soil and other
               residues). Three primary plant extraction algorithms, namely,
               (i) colour index-based segmentation, (ii) threshold-based
               segmentation, (iii) learning-based segmentation are discussed.
               Based on its prevalence in the literature, this review focuses
               in particular on colour index-based approaches. Therefore, a
               detailed discussion of the segmentation performance of colour
               index-based approaches is presented, based on studies from the
               literature conducted in the recent past, particularly from 2008
               to 2015. Finally, we identify the challenges and some
               opportunities for future developments in this space.
               \copyright{} 2016 Elsevier B.V.",
  journal   = "Comput. Electron. Agric.",
  publisher = "Elsevier B.V.",
  volume    =  125,
  pages     = "184--199",
  year      =  2016,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969174961&doi=10.1016%2fj.compag.2016.04.024&partnerID=40&md5=f2f1093d3d7096ea81b8da16a8110f4d",
  keywords  = "Colour index-based segmentation; Learning-based segmentation;
               Plant extraction; Plant pixels; Segmentation quality;
               Threshold-based segmentation; Color; Extraction; Image
               segmentation; Pixels; Surveys; Image processing technique;
               Learning-based segmentation; Plant classification; Plant
               extraction; Precision applications; Segmentation performance;
               Segmentation quality; Segmentation techniques; Image processing;
               algorithm; color; crop processing; field survey; herbicide;
               image processing; pixel; plant extract; precision; segmentation;
               threshold;585;segmentation",
  issn      = "0168-1699",
  doi       = "10.1016/j.compag.2016.04.024"
}

@MISC{noauthor_2022-cz,
  title        = "{YIQ}",
  booktitle    = "Wikipedia",
  month        =  dec,
  year         =  2022,
  url          = "https://en.wikipedia.org/wiki/YIQ",
  howpublished = "\url{https://en.wikipedia.org/wiki/YIQ}"
}

@BOOK{Muller2016-ui,
  title     = "Introduction to Machine Learning with Python",
  author    = "M{\"u}ller, Andreas C and Guido, Sarah",
  publisher = "O'Reilly Media",
  edition   =  1,
  month     =  nov,
  year      =  2016,
  isbn      = "9781449369415"
}

@BOOK{Perkins1997-jg,
  title     = "Understanding {SNMP} {MIBs}",
  author    = "Perkins, David",
  publisher = "Prentice Hall PTR",
  year      =  1997,
  url       = "https://arizona-primo.hosted.exlibrisgroup.com/primo-explore/fulldisplay?docid=01UA_ALMA21428506670003843&context=L&vid=01UA&lang=en_US&search_scope=Everything&adaptor=Local%20Search%20Engine&tab=default_tab&query=any,contains,mcginnis,%20evan",
  address   = "Upper Saddle River, N.J.",
  keywords  = "Simple Network Management Protocol (Computer network
               protocol);Computer networks--Specifications;Computer
               networks--Management",
  isbn      = "9780134377087"
}

@ARTICLE{Otsu1979-io,
  title    = "A Threshold Selection Method from {Gray-Level} Histograms",
  author   = "Otsu, N",
  journal  = "IEEE Trans. Syst. Man Cybern.",
  volume   =  9,
  number   =  1,
  pages    = "62--66",
  month    =  jan,
  year     =  1979,
  url      = "http://dx.doi.org/10.1109/TSMC.1979.4310076",
  keywords = "Histograms;Marine vehicles;Radar tracking;Least squares
              approximation;Surveillance;Target tracking;Gaussian
              distribution;Displays;Q measurement;Sea measurements;585;image
              processing",
  issn     = "0018-9472, 2168-2909",
  doi      = "10.1109/TSMC.1979.4310076"
}

@INCOLLECTION{Anguita2012-nb,
  title     = "Human activity recognition on smartphones using a multiclass
               hardware-friendly support vector machine",
  booktitle = "4th International Workshop on Ambient Assisted Living, {IWAAL}
               2012",
  author    = "Anguita, D and Ghio, A and Oneto, L and Parra, X and
               Reyes-Ortiz, J L",
  abstract  = "Activity-Based Computing [1] aims to capture the state of the
               user and its environment by exploiting heterogeneous sensors in
               order to provide adaptation to exogenous computing resources.
               When these sensors are attached to the subject's body, they
               permit continuous monitoring of numerous physiological signals.
               This has appealing use in healthcare applications, e.g. the
               exploitation of Ambient Intelligence (AmI) in daily activity
               monitoring for elderly people. In this paper, we present a
               system for human physical Activity Recognition (AR) using
               smartphone inertial sensors. As these mobile phones are limited
               in terms of energy and computing power, we propose a novel
               hardware-friendly approach for multiclass classification. This
               method adapts the standard Support Vector Machine (SVM) and
               exploits fixed-point arithmetic for computational cost
               reduction. A comparison with the traditional SVM shows a
               significant improvement in terms of computational costs while
               maintaining similar accuracy, which can contribute to develop
               more sustainable systems for AmI. \copyright{} 2012
               Springer-Verlag.",
  volume    = "7657 LNCS",
  pages     = "216--223",
  year      =  2012,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870866072&doi=10.1007%2f978-3-642-35395-6_30&partnerID=40&md5=f8cbae6e09f9a2886212e285d1a7ed10",
  address   = "Vitoria-Gasteiz",
  keywords  = "Activity Recognition; Hardware-Friendly; Smartphones; SVM;
               Activity recognition; Activity-based computing; Ambient
               intelligence; Computational costs; Computing power; Computing
               resource; Continuous monitoring; Daily activity; Elderly people;
               Hardware-Friendly; Health care application; Heterogeneous
               sensors; Human activity recognition; Inertial sensor;
               Multi-class; Multi-class classification; Physical activity;
               Physiological signals; Sustainable systems; SVM; Hardware;
               Health care; Sensors; Smartphones; Support vector
               machines;INFO523",
  isbn      = "9783642353949",
  doi       = "10.1007/978-3-642-35395-6\_30"
}

@ARTICLE{Monteiro2021-sk,
  title     = "A new alternative to determine weed control in agricultural
               systems based on artificial neural networks ({ANNs})",
  author    = "Monteiro, A L and Souza, Freitas and Lins, H A and Te{\'o}filo,
               T M D S and Barros J{\'u}nior, A P and Silva, D V and Mendon{\c
               c}a, V",
  abstract  = "Weed control is a necessary practice to avoid crop yield losses.
               Therefore, farmers should answer the following question: when to
               start weed control? Currently, there are no learning models to
               assist the producer to answer this question. Thus, the
               objectives were to: 1) evaluate the ability of artificial neural
               networks (ANNs) to estimate the beginning of weed control for
               different classes of acceptable yield losses; 2) validate a new
               alternative for modeling and predicting competition between
               weeds and crops. ANNs determined the ideal moment to control
               weeds based on non-destructive and destructive variables. The
               inputs C3/C4 ratio, coexistence period, density of weeds, and
               crop (categorical variable to differentiate sesame and melon)
               provided accuracy and F-score values above 0.95 during training,
               validation, and testing steps for ANN in non-destructive method.
               When using the destructive variables, C3/C4 ratio plus
               coexistence period, fresh matter of weeds, and crop provided
               accuracy and F-score values above 0.90 during training,
               validation, and testing steps. The combination of
               non-destructive and destructive inputs also generated an ANN
               with high accuracy and F-score, above 0.95, during training,
               validation, and testing steps. Machine learning can be used in
               crop-weed competition modeling. \copyright{} 2021 Elsevier B.V.",
  journal   = "Field Crops Res.",
  publisher = "Elsevier B.V.",
  volume    =  263,
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099622811&doi=10.1016%2fj.fcr.2021.108075&partnerID=40&md5=460bcd2ef6e727c29048a987c1715a96",
  keywords  = "Artificial intelligence; Decision making; Modeling; Weed
               interference; accuracy assessment; artificial neural network;
               crop yield; farming system; machine learning; nondestructive
               testing; weed control; yield response; Sesamum indicum",
  issn      = "0378-4290",
  doi       = "10.1016/j.fcr.2021.108075"
}

@ARTICLE{Flores2021-fk,
  title     = "Distinguishing seedling volunteer corn from soybean through
               greenhouse color, color-infrared, and fused images using machine
               and deep learning",
  author    = "Flores, P and Zhang, Z and Igathinathane, C and Jithin, M and
               Naik, D and Stenger, J and Ransom, J and Kiran, R",
  abstract  = "Volunteer corn (VC; Zea mays L.), as a weed in corn-soybean
               (Glycine max (L.) Merr.) rotation, has negatively impacted
               soybean production by reducing yield, lowering grain quality,
               and increasing the production costs. To assess the level of VC
               infestation to guide chemical applications, farmers usually rely
               on field visual observation. Since the field visual inspection
               is labor-intensive, time-consuming, and subjective, an automatic
               solution to differentiate VC from soybeans is required. As a
               first step toward that goal, this study aimed to develop a
               solution to differentiate those crops at seedling stage under
               greenhouse conditions. Color RGB and color-infrared (CIR) images
               of VC and soybean seedlings were collected in a greenhouse. A
               fused image dataset was created by blending the RGB and CIR
               image datasets. The top 20 extracted relevant features and the
               image datasets were fed into four different machine learning
               (ML) classifiers and four deep learning (DL) algorithms,
               respectively. All ML classifiers resulted in the highest
               accuracies on the fused images, with support vector machine
               (SVM) outperforming the other classifiers. Similarly, all the DL
               algorithms had a superior performance on the fused images, with
               GoogLeNet as the best algorithm. Overall, GoogLeNet was selected
               for its high accuracy 99.9\%, reasonable computation time (0.02
               s per plant), and simple and direct application. The application
               of fused images along with GoogLeNet can be used as a novel tool
               to automatically distinguish VC from soybean. Future research
               should focus on field testing this methodology in a real-time
               mode. \copyright{} 2021 Elsevier B.V.",
  journal   = "Ind. Crops Prod.",
  publisher = "Elsevier B.V.",
  volume    =  161,
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099201868&doi=10.1016%2fj.indcrop.2020.113223&partnerID=40&md5=e0377ffac0a6fd0fb90ecf844494501d",
  keywords  = "Deep learning; Image fusion; Image registration; Image
               segmentation; Machine learning; Soybean; Volunteer corn; Amino
               acids; Classification (of information); Color; Greenhouses;
               Learning systems; Nitrogen fixation; Support vector machines;
               Chemical applications; Greenhouse conditions; Relevant features;
               Soybean (Glycine max (L.) Merr.); Soybean production; Soybean
               seedlings; Visual inspection; Visual observations; Deep
               learning; algorithm; automation; food quality; greenhouse
               ecosystem; image analysis; machine learning; seeding; soybean;
               support vector machine; weed; Glycine max; Zea mays",
  issn      = "0926-6690",
  doi       = "10.1016/j.indcrop.2020.113223"
}

@ARTICLE{Sohail2021-cj,
  title     = "A review on machine vision and image processing techniques for
               weed detection in agricultural crops",
  author    = "Sohail, R and Nawaz, Q and Hamid, I and Gilani, S M M and
               Mumtaz, I and Mateen, A and Nawaz, J",
  abstract  = "The advancement in the field of precision agriculture has opened
               doors for site-specific weed management. There is a growing need
               to control the amount of herbicide sprayed on weeds to reduce
               economic and environmental losses. In the field of precision
               agriculture, incorporation of machine learning techniques has
               enabled the farmers to automate the process of controlling weed
               using an adequate number of herbicides for different species
               in-situ. This study aims to explore various parameters of
               Computer Vision and Machine Learning algorithms and methods used
               by researchers to develop Artificial Intelligence models to
               remove weeds from agricultural fields. More than twenty
               state-of-the-art algorithms have been studied in this paper. We
               categorized these algorithms into five categories based on
               different features i.e. visual, shape, spatial, and spectral. At
               the end of this study, a comprehensive table is presented
               containing details of algorithms in terms of limitations and
               accuracy. \copyright{} 2021, University of Agriculture. All
               rights reserved.",
  journal   = "Pakistan Journal of Agricultural Sciences",
  publisher = "University of Agriculture",
  volume    =  58,
  number    =  1,
  pages     = "187--204",
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099593950&doi=10.21162%2fPAKJAS%2f21.305&partnerID=40&md5=e2a7aba39d94f23a2e7f35556813a4b1",
  keywords  = "Image processing; Machine vision; Robotic weed control; Weed
               detection",
  issn      = "0552-9034",
  doi       = "10.21162/PAKJAS/21.305"
}

@PROCEEDINGS{Beeharry2020-nn,
  title      = "Performance of {ANN} and {AlexNet} for weed detection using
                {UAV-based} images",
  author     = "Beeharry, Y and Bassoo, V",
  editor     = "{Pudaruth S.} and {Mungur A.} and {Ah King R.T.F.} and {Fowdur
                T.P.} and {Bojkovic Z.} and {Milovanovic D.} and {Hurbungs V.}",
  abstract   = "Unmanned Aerial Vehicles (UAVs) have become an integral part of
                several real-world applications. Their combination with other
                evolving paradigms such as image recognition using machine
                learning or deep learning algorithms has contributed to the
                suitability for use in smart agriculture and weed detection
                applications. In this paper, the performances of the Artificial
                Neural Network (ANN) and AlexNet algorithms for weed detection
                using UAV-based images have been studied. An image dataset
                containing 15336 segments with the following breakdown: 3249 of
                soil, 7376 of soybean, 3520 grass and 1191 of broadleaf weeds
                has been used. The partitioning for train and test sets has
                been done in the ratio of 70:30. Simulation results show that
                the conventional ANN algorithm provide an accuracy of 48.09\%
                while the AlexNet algorithms gives an accuracy 99.8\% on the
                test dataset. \copyright{} 2020 IEEE.",
  publisher  = "Institute of Electrical and Electronics Engineers Inc.",
  year       =  2020,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099571177&doi=10.1109%2fELECOM49001.2020.9296994&partnerID=40&md5=d5ab3b49d9fd7a33fd2f26e383c2ca09",
  keywords   = "AlexNet; Artificial Neural Network; weed detection;
                Agricultural robots; Aircraft detection; Antennas; Deep
                learning; Image recognition; Neural networks; Statistical
                tests; Unmanned aerial vehicles (UAV); Weed control; ANN
                algorithm; Broadleaf weeds; Image datasets; Integral part;
                Real-world; Smart agricultures; Test sets; Weed detection;
                Learning algorithms",
  conference = "3rd International Conference on Emerging Trends in Electrical,
                Electronic and Communications Engineering, ELECOM 2020",
  isbn       = "9781728157078",
  doi        = "10.1109/ELECOM49001.2020.9296994"
}

@PROCEEDINGS{Perez-Ortiz2017-pg,
  title      = "Machine learning paradigms for weed mapping via unmanned aerial
                vehicles",
  author     = "Perez-Ortiz, M and Gutierrez, P A and Pena, J M and
                Torres-Sanchez, J and Lopez-Granados, F and Hervas-Martinez, C",
  abstract   = "This paper presents a novel strategy for weed monitoring, using
                images taken with unmanned aerial vehicles (UAVs) and concepts
                of image analysis and machine learning. Weed control in
                precision agriculture designs site-specific treatments based on
                the coverage of weeds, where the key is to provide precise weed
                maps timely. Most traditional remote platforms, e.g. piloted
                planes or satellites, are, however, not suitable for early weed
                monitoring, given their low temporal and spatial resolutions,
                as opposed to he ultra-high spatial resolution of UAVs. The
                system here proposed makes use of UAV-imagery and is based on:
                1) Divide the image, 2) compute and binarise the vegetation
                indexes, 3) detect crop rows, 4) optimise the parameters and 4)
                learn a classification model. Since crops are usually organised
                in rows, the use of a crop row detection algorithm helps to
                separate properly weed and crop pixels, which is a common
                handicap given the spectral similitude of both. Several
                artificial intelligence paradigms are compared in this paper to
                identify the most suitable strategy for this topic (i.e.
                unsupervised, supervised and semi-supervised approaches). Our
                experiments also study the effect of different parameteres: the
                flight altitude, the sensor and the use of previously trained
                models at a different height. Our results show that 1) very
                promising performance can be obtained, even when using very few
                labelled data and 2) the classification model can be learnt in
                a subplot of the experimental field at low altitude and then
                applied to the whole field at a higher height, which simplifies
                the whole process. These results motivate the use of this
                strategy to design weed monitoring strategies for early
                post-emergence weed control. \copyright{} 2016 IEEE.",
  publisher  = "Institute of Electrical and Electronics Engineers Inc.",
  year       =  2017,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015991782&doi=10.1109%2fSSCI.2016.7849987&partnerID=40&md5=bc4aab58904adb41c5f577a715b76d0e",
  keywords   = "Artificial intelligence; Crops; Learning systems; Unmanned
                aerial vehicles (UAV); Classification models; Crop row
                detection; Different heights; Flight altitudes; Monitoring
                strategy; Novel strategies; Precision Agriculture; Temporal and
                spatial; Weed control",
  conference = "2016 IEEE Symposium Series on Computational Intelligence, SSCI
                2016",
  isbn       = "9781509042401",
  doi        = "10.1109/SSCI.2016.7849987"
}

@PROCEEDINGS{Espinoza2020-es,
  title      = "Weed identification and removal using machine learning
                techniques and unmanned ground vehicles",
  author     = "Espinoza, M A M and Le, C Z and Raheja, A and Bhandari, S",
  editor     = "{Thomasson J.A.} and {Torres-Rua A.F.}",
  abstract   = "This paper presents the use of unmanned ground vehicle (UGV)
                and machine learning techniques for the identification removal
                of weeds in lettuce crop. In recent years, breakthroughs in
                deep learning, computer vision, and miniaturization of
                electronic devices have paved the way for use of unmanned
                systems and machine learning techniques for applications that
                are dull, dirty, and dangerous for humans including
                agricultural applications. Unmanned systems and machine
                learning techniques have potential to transform and modernize
                how the crops are grown and cared. One of the problems every
                farmer encounters is invasive weeds that can kill or hinder the
                growth of crops by stealing water, nutrients, and sunlight from
                the plants. Herbicides are used to kill and stop the growth of
                weeds. However, use of herbicides increases the cost of
                production, is labor intensive, and exposes human to dangerous
                chemicals. Manually removing the weeds is also very labor
                intensive. Using machine learning techniques and UGVs for the
                identification and removal of weeds will reduce the cost of
                production, human exposure to dangerous chemicals, and
                dependence on human labor. Models were trained using YOLO,
                Faster R-CNN, and SSD Mobile object detection techniques. For
                the training of machine learning models, images of the weeds in
                an experimental lettuce plot was collected throughout the
                growing season. Validation of the developed models was
                performed using different data sets than the training data sets
                in the same plot as well as a different plot. The identified
                weeds were then removed using the UGV through teleoperation.
                \copyright{} 2020 SPIE.",
  publisher  = "SPIE",
  volume     =  11414,
  year       =  2020,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089075752&doi=10.1117%2f12.2557625&partnerID=40&md5=2fdd9ed15e473720280303442772ae45",
  keywords   = "CNN,SSD; Machine Learning; Robotics; UGV; Weed Removal; Weeds;
                YOLO; Agricultural robots; Crops; Deep learning; Ground
                vehicles; Herbicides; Intelligent vehicle highway systems;
                Object detection; Weed control; Cost of productions; Electronic
                device; Identification and removal; Machine learning models;
                Machine learning techniques; Training data sets; Unmanned
                ground vehicles; Weed identification; Learning systems",
  conference = "Autonomous Air and Ground Sensing Systems for Agricultural
                Optimization and Phenotyping V 2020",
  isbn       = "9781510636057",
  doi        = "10.1117/12.2557625"
}

@MISC{noauthor_undated-xq,
  title        = "Paperpile",
  url          = "https://paperpile.com/app/p/da8a0891-8580-0eb5-9d66-3438a62cdebe",
  howpublished = "\url{https://paperpile.com/app/p/da8a0891-8580-0eb5-9d66-3438a62cdebe}",
  note         = "Accessed: 2021-2-14"
}

@PROCEEDINGS{Perez-Ortiz2017-wj,
  title      = "Machine learning paradigms for weed mapping via unmanned aerial
                vehicles",
  author     = "Perez-Ortiz, M and Gutierrez, P A and Pena, J M and
                Torres-Sanchez, J and Lopez-Granados, F and Hervas-Martinez, C",
  abstract   = "This paper presents a novel strategy for weed monitoring, using
                images taken with unmanned aerial vehicles (UAVs) and concepts
                of image analysis and machine learning. Weed control in
                precision agriculture designs site-specific treatments based on
                the coverage of weeds, where the key is to provide precise weed
                maps timely. Most traditional remote platforms, e.g. piloted
                planes or satellites, are, however, not suitable for early weed
                monitoring, given their low temporal and spatial resolutions,
                as opposed to he ultra-high spatial resolution of UAVs. The
                system here proposed makes use of UAV-imagery and is based on:
                1) Divide the image, 2) compute and binarise the vegetation
                indexes, 3) detect crop rows, 4) optimise the parameters and 4)
                learn a classification model. Since crops are usually organised
                in rows, the use of a crop row detection algorithm helps to
                separate properly weed and crop pixels, which is a common
                handicap given the spectral similitude of both. Several
                artificial intelligence paradigms are compared in this paper to
                identify the most suitable strategy for this topic (i.e.
                unsupervised, supervised and semi-supervised approaches). Our
                experiments also study the effect of different parameteres: the
                flight altitude, the sensor and the use of previously trained
                models at a different height. Our results show that 1) very
                promising performance can be obtained, even when using very few
                labelled data and 2) the classification model can be learnt in
                a subplot of the experimental field at low altitude and then
                applied to the whole field at a higher height, which simplifies
                the whole process. These results motivate the use of this
                strategy to design weed monitoring strategies for early
                post-emergence weed control. \copyright{} 2016 IEEE.",
  publisher  = "Institute of Electrical and Electronics Engineers Inc.",
  year       =  2017,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015991782&doi=10.1109%2fSSCI.2016.7849987&partnerID=40&md5=bc4aab58904adb41c5f577a715b76d0e",
  keywords   = "Artificial intelligence; Crops; Learning systems; Unmanned
                aerial vehicles (UAV); Classification models; Crop row
                detection; Different heights; Flight altitudes; Monitoring
                strategy; Novel strategies; Precision Agriculture; Temporal and
                spatial; Weed control",
  conference = "2016 IEEE Symposium Series on Computational Intelligence, SSCI
                2016",
  isbn       = "9781509042401",
  doi        = "10.1109/SSCI.2016.7849987"
}

@ARTICLE{Pantazi2016-yx,
  title    = "Active learning system for weed species recognition based on
              hyperspectral sensing",
  author   = "Pantazi, Xanthoula-Eirini and Moshou, Dimitrios and Bravo, Cedric",
  abstract = "Weeds have a devastating impact in crop production and yield in
              general. Current practice uses uniform application of herbicides
              leading to high costs and degradation of the environment and the
              field productivity. Site-specific treatments can be regarded as
              solutions either for reducing inputs or enable alternative
              non-chemical treatments. However, site-specific treatment needs
              accurate targeting through sensing. A new machine learning method
              is proposed, which discriminates between crop and weed species
              relying on their spectral reflectance differences. Spectral
              features were extracted from a hyperspectral imaging system that
              was mounted on a robotic platform. The proposed machine learning
              method suggests active learning by combining novelty detection
              and incremental class augmentation. Novelty detection was based
              on one-class classifiers constructed by neural networks. Best
              results for the active learning were obtained for the one-class
              MOG (mixture of Gaussians) and one-class SOM (self-organising
              map) classifiers when compared with one-class support vector
              machines and the auto-encoder network. The SOM and MOG
              performance in crop recognition was found to be 100\% and 100\%
              respectively. The recognition performance for different weed
              species varied between 31\% and 98\% (MOG) and 53\%--94\% (SOM).",
  journal  = "Biosystems Eng.",
  volume   =  146,
  pages    = "193--202",
  month    =  jun,
  year     =  2016,
  url      = "https://www.sciencedirect.com/science/article/pii/S1537511016000143",
  keywords = "Spectrograph; Self-organising map; Mixture of Gaussians;
              One-class classifier; Auto-encoder network; Support vector
              machines",
  issn     = "1537-5110",
  doi      = "10.1016/j.biosystemseng.2016.01.014"
}

@MISC{noauthor_undated-te,
  title = "fulltext.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hunt2013-ih,
  title    = "A visible band index for remote sensing leaf chlorophyll content
              at the canopy scale",
  author   = "Hunt, E Raymond and Doraiswamy, Paul C and McMurtrey, James E and
              Daughtry, Craig S T and Perry, Eileen M and Akhmedov, Bakhyt",
  abstract = "Leaf chlorophyll content is an important variable for
              agricultural remote sensing because of its close relationship to
              leaf nitrogen content. The triangular greenness index (TGI) was
              developed based on the area of a triangle surrounding the
              spectral features of chlorophyll with points at (670nm, R670),
              (550nm, R550), and (480nm, R480), where R$\lambda$ is the
              spectral reflectance at wavelengths of 670, 550 and 480,
              respectively. The equation is
              TGI=−0.5[(670−480)(R670−R550)−(670−550)(R670−R480)]. In 1999,
              investigators funded by NASA's Earth Observations
              Commercialization and Applications Program collaborated on a
              nitrogen fertilization experiment with irrigated maize in
              Nebraska. Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)
              data and Landsat 5 Thematic Mapper (TM) data were acquired along
              with leaf chlorophyll meter and other data on three dates in July
              during late vegetative growth and early reproductive growth. TGI
              was consistently correlated with plot-averaged chlorophyll-meter
              values at the spectral resolutions of AVIRIS, Landsat TM, and
              digital cameras. Simulations using the Scattering by Arbitrarily
              Inclined Leaves (SAIL) canopy model indicate an interaction among
              TGI, leaf area index (LAI) and soil type at low crop LAI, whereas
              at high LAI and canopy closure, TGI was only affected by leaf
              chlorophyll content. Therefore, TGI may be the best spectral
              index to detect crop nitrogen requirements with low-cost digital
              cameras mounted on low-altitude airborne platforms.",
  journal  = "Int. J. Appl. Earth Obs. Geoinf.",
  volume   =  21,
  pages    = "103--112",
  month    =  apr,
  year     =  2013,
  url      = "https://www.sciencedirect.com/science/article/pii/S0303243412001791",
  keywords = "Spectral indices; Triangular greenness index (TGI); Airborne
              Visible/Infrared Imaging Spectrometer (AVIRIS); PROSPECT; SAIL;
              Landsat Thematic Mapper (TM); Nitrogen fertilization;585",
  issn     = "0303-2434",
  doi      = "10.1016/j.jag.2012.07.020"
}

@ARTICLE{Osorio2020-bt,
  title     = "A Deep Learning Approach for Weed Detection in Lettuce Crops
               Using Multispectral Images",
  author    = "Osorio, Kavir and Puerto, Andr{\'e}s and Pedraza, Cesar and
               Jamaica, David and Rodr{\'\i}guez, Leonardo",
  abstract  = "Weed management is one of the most important aspects of crop
               productivity; knowing the amount and the locations of weeds has
               been a problem that experts have faced for several decades. This
               paper presents three methods for weed estimation based on deep
               learning image processing in lettuce crops, and we compared them
               to visual estimations by experts. One method is based on support
               vector machines (SVM) using histograms of oriented gradients
               (HOG) as feature descriptor. The second method was based in
               YOLOV3 (you only look once V3), taking advantage of its robust
               architecture for object detection, and the third one was based
               on Mask R-CNN (region based convolutional neural network) in
               order to get an instance segmentation for each individual. These
               methods were complemented with a NDVI index (normalized
               difference vegetation index) as a background subtractor for
               removing non photosynthetic objects. According to chosen
               metrics, the machine and deep learning methods had F1-scores of
               88\%, 94\%, and 94\% respectively, regarding to crop detection.
               Subsequently, detected crops were turned into a binary mask and
               mixed with the NDVI background subtractor in order to detect
               weed in an indirect way. Once the weed image was obtained, the
               coverage percentage of weed was calculated by classical image
               processing methods. Finally, these performances were compared
               with the estimations of a set from weed experts through a
               Bland--Altman plot, intraclass correlation coefficients (ICCs)
               and Dunn's test to obtain statistical measurements between every
               estimation (machine-human); we found that these methods improve
               accuracy on weed coverage estimation and minimize subjectivity
               in human-estimated data.",
  journal   = "AgriEngineering",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  2,
  number    =  3,
  pages     = "471--488",
  month     =  aug,
  year      =  2020,
  url       = "https://www.mdpi.com/2624-7402/2/3/32",
  language  = "en",
  doi       = "10.3390/agriengineering2030032"
}

@ARTICLE{Hamuda2017-nf,
  title    = "Automatic crop detection under field conditions using the {HSV}
              colour space and morphological operations",
  author   = "Hamuda, Esmael and Mc Ginley, Brian and Glavin, Martin and Jones,
              Edward",
  abstract = "Developing an automatic weeding system requires robust detection
              of the exact location of the crop to be protected from damage.
              Computer vision techniques can be an effective means of
              determining plant location. In this paper, a novel algorithm
              based on colour features and morphological erosion and dilation
              is proposed. This process segments cauliflower crop regions in
              the image from weeds and soil under natural illumination (cloudy,
              partially cloudy, and sunny). The proposed algorithm uses the HSV
              colour space for discriminating crop, weeds and soil. The region
              of interest (ROI) is defined by filtering each of the HSV
              channels between certain values (minimum and maximum threshold
              values). The region is then further refined by using a
              morphological erosion and dilation process. The moment method is
              applied to determine the position and mass distribution of
              objects in video sequences, as well as to track crops. The
              performance of the algorithm was assessed by comparing the
              obtained results with those of ground truth methods (manual
              annotation). A sensitivity of 98.91\% and precision of 99.04\%
              was achieved.",
  journal  = "Comput. Electron. Agric.",
  volume   =  133,
  pages    = "97--107",
  month    =  feb,
  year     =  2017,
  url      = "https://www.sciencedirect.com/science/article/pii/S0168169916303714",
  keywords = "Morphological; Natural illumination; HSV colour space; Crop
              detection;image processing",
  issn     = "0168-1699",
  doi      = "10.1016/j.compag.2016.11.021"
}

@ARTICLE{Chandel2021-jv,
  title     = "An integrated inter- and intra-row weeding system for row crops",
  author    = "Chandel, N S and Chandel, A K and Roul, A K and Solanki, K R and
               Mehta, C R",
  abstract  = "Weeding is critical to eliminate non-native plants that compete
               with main crops and adversely affect their production quality
               and quantity. Numerous prototypes exist for inter-row weeding
               but very limited for intra-row weed eradication. This study
               developed a tractor drawn integrated inter- and intra-row
               weeding (IIIRW) system for field crops. Active rotary tines were
               used for intra-row weeding and passive tines for inter-row
               weeding. Optimum operational configurations were obtained with
               rigorous repeated soil bin experiments, and developed system was
               evaluated for three growing seasons (2017--19) in field grown
               maize (Zea mays L.) and pigeon pea (Cajanus cajan (L.) Millsp.)
               crops. Optimum ratios of intra-row tine rotary speed to forward
               speed (u/v) were in the ranges of 0.8--1.3 that lead to weed
               mortality of 88.4 \% (Buried: 8.5 \%, Uprooted: 79.9 \%),
               negligible intact weeds, and plant damage (Pd) within 6 \%. Sole
               inter-row weeding operation resulted up to 78.1 \% of weed
               mortality (Buried: 18.1 \%, Uprooted: 60.0 \%) and negligible
               Pd. Overall weed mortality with the IIIRW system was 92.8 \%
               (Buried: 9.5 \%, Uprooted: 83.3 \%) in maize and 84.1 \%
               (Buried: 7.6 \%, Uprooted: 76.5 \%) in pigeon pea crops. Results
               suggest suitability of the IIIRW system for a range of similar
               crops for field capacity in ranges of 0.22--0.26 ha/h at
               recommended operating speeds within 0.50--0.56 m/s. The IIIRW
               system requires a single prime mover and could be potentially
               economical and efficient technology for field weeding
               operations. \copyright{} 2021 Elsevier Ltd",
  journal   = "Crop Prot.",
  publisher = "Elsevier Ltd",
  volume    =  145,
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103758089&doi=10.1016%2fj.cropro.2021.105642&partnerID=40&md5=4cae681b782ad8ba769c43fe1585bfaa",
  keywords  = "Crop damage; Economical operation; Inter and intra-row weeds;
               Mechanical weeding; Weed mortality; crop plant; detection
               method; instrumentation; legume; mortality; weed; Cajanus cajan;
               Zea mays;weeding systems",
  issn      = "0261-2194",
  doi       = "10.1016/j.cropro.2021.105642"
}

@ARTICLE{Chen2019-wg,
  title     = "Vegetable crop row extraction method based on accumulation
               threshold of Hough Transformation",
  author    = "Chen, Z and Li, W and Zhang, W and Li, Y and Li, M and Li, H",
  abstract  = "Agricultural machinery field automatic navigation technology is
               widely used in farming, sowing, weeding, fertilizing, spraying,
               harvesting and other agricultural production process. This
               technology can improve the efficiency of the mechanical
               efficiency and reduce the missing areas of operation, labor
               intensity and the complexity of the operation. Because machine
               vision can be used to obtain and perceive the relative position
               information of crop rows, current crop growth status and field
               environment in real time, it is widely applied in online crop
               detection and identification. In this paper, a method based on
               automatic accumulation threshold of Hough Transformation was
               presented in order to improve the adaptability of the crop row
               recognition algorithm for different kinds and growth periods of
               vegetables with machine vision. The method was composed of image
               preprocessing, feature point detection, optimal accumulation
               threshold acquisition and crop row extraction. Firstly, to
               reduce the adverse effects of light change and restrain the
               background noise, a* component of Lab color model was selected
               for transforming RGB image to grayscale image. Optimal adaptive
               threshold and morphology close-open operation was applied for
               minimizing error segmentation probability and eliminating
               irrelevant detail. Secondly, the feature points of crop rows
               were extracted by sectionalized vertical projection method. The
               original image was divided into several horizontal segments and
               target pixel ratio and vertical projection width were used as
               double threshold in the luminance projection view of each
               segment to determine the location of feature points and
               distinguish noise. Thirdly, the Hough transformation method with
               different accumulative thresholds was performed to fit straight
               lines for all feature points in the image coordinate system,
               then they were all converted to Hough space accumulator as
               points. These points were clustered into the same number as crop
               rows by K-means clustering method. According to the camera
               projection, the optimal accumulator threshold was acquired by
               the position relation of clustering centroid and minimum
               inter-class variance. Finally, the fitting line parameters of
               real crop rows were the clustering centroid parameters of the
               accumulation space under the optimal accumulation threshold,
               then the parameters were converted into the crop lines in the
               image coordinate system. The crop row identification tests of
               lettuce and cabbage were carried out in the greenhouse and filed
               according to the conditions of crops in different growing
               periods, different weed densities, and different light
               conditions in the field. The greenhouse experiment showed that
               the algorithm can effectively identify crop rows with an average
               recognition accuracy of 97.1\% for two crops of different growth
               periods under different weed densities. The outdoor experiment
               showed that the algorithm can also identify crop rows with
               94.6\% recognition accuracy under different row numbers and
               light conditions. Time consumption for optimal accumulator
               threshold algorithm and crop rows extraction algorithm were no
               more than 1.5 and 0.2 s, and the average accuracy rate of crop
               row detection was achieved 95.8\%. In view of the practical
               application of field operations, as the environmental parameters
               basically do not change significantly in a short time, the
               optimal accumulation threshold was only needed to be obtained
               once, which can ensure the time consumption of algorithm was
               about 0.2 s. \copyright{} 2019, Editorial Department of the
               Transactions of the Chinese Society of Agricultural Engineering.
               All right reserved.",
  journal   = "Nongye Gongcheng Xuebao/Transactions of the Chinese Society of
               Agricultural Engineering",
  publisher = "Chinese Society of Agricultural Engineering",
  volume    =  35,
  number    =  22,
  pages     = "314--322",
  year      =  2019,
  url       = "https://www.scopus.com/record/display.uri?eid=2-s2.0-85079348543&origin=resultslist&sort=plf-f&src=s&sid=7ff83019f5390ac8a8ce07580d57738d&sot=b&sdt=b&sl=35&s=TITLE-ABS-KEY%28weed+detection+ratio%29&relpos=17&citeCnt=2&searchTerm=",
  keywords  = "Algorithms; Crop row recognition; Hough transform; K-means
               clustering; Machine vision; Navigation; Precision agriculture;
               Algorithms; Computer vision; Crops; Efficiency; Extraction; Fits
               and tolerances; Greenhouses; Hough transforms; Image
               segmentation; Navigation; Precision agriculture; Vegetables;
               Accumulation threshold; Agricultural productions; Detection and
               identifications; Environmental parameter; Feature point
               detection; Greenhouse experiments; K-means clustering method;
               Mechanical efficiency; K-means clustering;image processing",
  issn      = "1002-6819"
}

@ARTICLE{Lin2017-xq,
  title     = "Detection of corn and weed species by the combination of
               spectral, shape and textural features",
  author    = "Lin, F and Zhang, D and Huang, Y and Wang, X and Chen, X",
  abstract  = "Accurate detection of weeds in farmland can help reduce
               pesticide use and protect the agricultural environment. To
               develop intelligent equipment for weed detection, this study
               used an imaging spectrometer system, which supports micro-scale
               plant feature analysis by acquiring high-resolution hyper
               spectral images of corn and a number of weed species in the
               laboratory. For the analysis, the object-oriented classification
               system with segmentation and decision tree algorithms was
               utilized on the hyper spectral images to extract shape and
               texture features of eight species of plant leaves, and then, the
               spectral identification characteristics of different species
               were determined through sensitive waveband selection and using
               vegetation indices calculated from the sensitive band data of
               the images. On the basis of the comparison and analysis of the
               combined characteristics of spectra, shape, and texture, it was
               determined that the spectral characteristics of the ratio
               vegetation index of R677/R710 and the normalized difference
               vegetation index, shape features of shape index, area, and
               length, as well as the texture feature of the entropy index
               could be used to build a discrimination model for corn and weed
               species. Results of the model evaluation showed that the Global
               Accuracy and the Kappa coefficient of the model were both over
               95\%. In addition, spectral and shape features can be regarded
               as the preferred characteristics to develop a device of weed
               identification from the view of accessibility to crop/weeds
               discriminant features, according to different roles of various
               features in classifying plants. Therefore, the results of this
               study provide valuable information for the portable device
               development of intelligent weed detection. \copyright{} 2017 by
               the authors. Licensee MDPI, Basel, Switzerland.",
  journal   = "Sustainability (Switzerland)",
  publisher = "MDPI AG",
  volume    =  9,
  number    =  8,
  year      =  2017,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026824347&doi=10.3390%2fsu9081335&partnerID=40&md5=fae4e8c733e19d3d32edd65993c456b2",
  keywords  = "Corn; Decision tree; Hyper spectral imaging; Object-oriented;
               Weed; accessibility; accuracy assessment; agricultural land;
               algorithm; decision making; detection method; discriminant
               analysis; entropy; maize; NDVI; shape; spectral analysis;
               spectrometer; texture; weed; Zea mays;reviewed;shape analysis",
  issn      = "2071-1050",
  doi       = "10.3390/su9081335"
}

@ARTICLE{El-Faki2000-lh,
  title     = "Weed detection using color machine vision",
  author    = "El-Faki, M S and Zhang, N and Peterson, D E",
  abstract  = "Many weed species have reddish stems, but stems of wheat and
               soybean are green. These color features were used in this study
               to establish a simple weed-detection method using a color
               machine-vision system. This method is more practical than
               texture- or shape-based methods because of its low sensitivity
               to canopy overlap, leaf orientation, camera focusing, and wind
               effect. Four types of relative color indices formed by RGB gray
               levels were designed. The most effective combinations of these
               color indices were selected using a statistical method. These
               combinations were used as the input variables for a statistical
               classifier based on discriminant analysis (DA) and two
               artificial neural-network (NN) classifiers. These classifiers
               were trained and tested using three weed species. (Johnsongrass,
               redroot pigweed, and yellow foxtail) with soybean and three weed
               species (wild buckwheat, cheat, and field bindweed) with wheat.
               Preprocessing and postprocessing algorithms were developed to
               shorten the processing time and to reduce noise. The results
               showed that the statistical DA classifier was more accurate than
               the NN classifiers in classification accuracy. The least-square
               means of the classification rates using the DA classifiers for
               soybean and wheat were 54.9\% and 62.2\%, respectively. The
               misclassification rates for most weed species were below 3\%.
               Because the reddish colors on the stems of some weed species
               vary as the plants grow, an in-field calibration procedure will
               be needed to make the classifiers more adaptive to different
               circumstances. Many weed species have reddish stems, but stems
               of wheat and soybean are green. These color features were used
               in this study to establish a simple weed-detection method using
               a color machine-vision system. This method is more practical
               than texture- or shape-based methods because of its low
               sensitivity to canopy overlap, leaf orientation, camera
               focusing, and wind effect. Four types of relative color indices
               formed by RGB gray levels were designed. The most effective
               combinations of these color indices were selected using a
               statistical method. These combinations were used as the input
               variables for a statistical classifier based on discriminant
               analysis (DA) and two artificial neural-network (NN)
               classifiers. These classifiers were trained and tested using
               three weed species. (Johnsongrass, redroot pigweed, and yellow
               foxtail) with soybean and three weed species (wild buckwheat,
               cheat, and field bindweed) with wheat. Preprocessing and
               postprocessing algorithms were developed to shorten the
               processing time and to reduce noise. The results showed that the
               statistical DA classifier was more accurate than the NN
               classifiers in classification accuracy. The least-square means
               of the classification rates using the DA classifiers for soybean
               and wheat were 54.9\% and 62.2\%, respectively. The
               misclassification rates for most weed species were below 3\%.
               Because the reddish colors on the stems of some weed species
               vary as the plants grow, an in-field calibration procedure will
               be needed to make the classifiers more adaptive to different
               circumstances.",
  journal   = "Trans. ASAE",
  publisher = "ASAE",
  volume    =  43,
  number    =  6,
  pages     = "1969--1978",
  year      =  2000,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034473205&partnerID=40&md5=e0134c79bd1e86417cea41fa1fb0e936",
  address   = "St. Joseph, MI, United States",
  keywords  = "Color; Discriminant analysis; Machine vision; Precision
               agriculture; Sensor; Weed; Algorithms; Color image processing;
               Computer vision; Neural networks; Pattern recognition;
               Discriminant analysis; Weed detection; Weed control;weed
               discrimination",
  issn      = "0001-2351"
}

@ARTICLE{Elstone2020-gf,
  title     = "High speed crop and weed identification in lettuce fields for
               precision weeding",
  author    = "Elstone, L and How, K Y and Brodie, S and Ghazali, M Z and
               Heath, W P and Grieve, B",
  abstract  = "Precision weeding can significantly reduce or even eliminate the
               use of herbicides in farming. To achieve high-precision,
               individual targeting of weeds, high-speed, low-cost plant
               identification is essential. Our system using the red, green,
               and near-infrared reflectance, combined with a size
               differentiation method, is used to identify crops and weeds in
               lettuce fields. Illumination is provided by LED arrays at 525,
               650, and 850 nm, and images are captured in a single-shot using
               a modified RGB camera. A kinematic stereo method is utilised to
               compensate for parallax error in images and provide accurate
               location data of plants. The system was verified in field trials
               across three lettuce fields at varying growth stages from 0.5 to
               10 km/h. In-field results showed weed and crop identification
               rates of 56\% and 69\%, respectively. Post-trial processing
               resulted in average weed and crop identifications of 81\% and
               88\%, respectively. \copyright{} 2020 by the authors. Licensee
               MDPI, Basel, Switzerland.",
  journal   = "Sensors",
  publisher = "MDPI AG",
  volume    =  20,
  number    =  2,
  year      =  2020,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078004407&doi=10.3390%2fs20020455&partnerID=40&md5=2ffd3c6fdb09c92a85a92acb98bdf1a6",
  keywords  = "Kinetic stereo imaging; Multispectral imaging; Plant detection;
               Precision weeding; Crops; Geometrical optics; Infrared devices;
               Crop identification; Differentiation methods; Multispectral
               imaging; Near infra-red reflectances; Plant detections; Plant
               identification; Precision weeding; Stereo imaging; Stereo image
               processing; article; crop; field study; growth curve; human;
               illumination; infrared radiation; lettuce; nonhuman; velocity;
               weed;weed discrimination",
  issn      = "1424-8220",
  doi       = "10.3390/s20020455",
  pmc       = "31947520"
}

@ARTICLE{Hemming2001-ue,
  title     = "Computer-vision-based weed identification under field conditions
               using controlled lighting",
  author    = "Hemming, J and Rath, T",
  abstract  = "The methods of digital image analysis were used to develop an
               identification system for weeds in crops. Two vegetable crops
               (cabbage and carrots) and a number of naturally occurring weed
               species were used to develop the classification algorithms.
               Considering the rougher environment, special attention was given
               to the open-field experiments. The images were obtained with a
               device that provided controlled lighting conditions. The
               analysis was carried out off-line. Eight different morphological
               features and three colour features were calculated for each
               single object to build a joint feature space. On the basis of
               sample data sets of each class, statistics were carried out to
               determine the features, which are suitable for discrimination. A
               membership function based on a fuzzy logic approach was formed
               and used for the classification. The experiments showed that
               colour features can help to increase the classification
               accuracy. Moreover, colour was used successfully for the
               segmentation procedure of plants and soil. Depending on growth
               stage, weed density and method of calculation between 51 and
               95\% of the plants were classified correctly. Problems still
               exists by separating and allocating single plants in plant
               stands where the plants have grown together. Compared to other
               studies the plant identification system presented is an
               improvement, especially considering that the experiments were
               carried out under field conditions. \copyright{} 2001 Silsoe
               Research Institute.",
  journal   = "J. Agric. Eng. Res.",
  publisher = "Academic Press",
  volume    =  78,
  number    =  3,
  pages     = "233--243",
  year      =  2001,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002811038&doi=10.1006%2fjaer.2000.0639&partnerID=40&md5=11eb6f8f661ae8a2b94ddcf758f67b99",
  keywords  = "image processing",
  issn      = "0021-8634",
  doi       = "10.1006/jaer.2000.0639"
}

@PROCEEDINGS{Zhang2006-lm,
  title      = "Crop/weed discrimination using near-infrared reflectance
                spectroscopy ({NIRS})",
  author     = "Zhang, Y and He, Y",
  abstract   = "The traditional uniform herbicide application often results in
                an over chemical residues on soil, crop plants and agriculture
                produce, which have imperiled the environment and food
                security. Near-infrared reflectance spectroscopy (NIRS) offers
                a promising means for weed detection and site-specific
                herbicide application. In laboratory, a total of 90 samples (30
                for each species) of the detached leaves of two weeds, i.e.,
                threeseeded mercury (Acalypha australis L.) and fourleafed
                duckweed (Marsilea quadrifolia L.), and one crop soybean
                (Glycine max) was investigated for NIRS on 325-1075 nm using a
                field spectroradiometer. 20 absorbance samples of each species
                after pretreatment were exported and the lacked Y variables
                were assigned independent values for partial least squares
                (PLS) analysis. During the combined principle component
                analysis (PCA) on 400-1000 nm, the PC1 and PC2 could together
                explain over 91\% of the total variance and detect the three
                plant species with 98.3\% accuracy. The full-cross validation
                results of PLS, i.e., standard error of prediction (SEP) 0.247,
                correlation coefficient (r) 0.954 and root mean square error of
                prediction (RMSEP) 0.245, indicated an optimum model for weed
                identification. By predicting the remaining 10 samples of each
                species in the PLS model, the results with deviation presented
                a 100\% crop/weed detection rate. Thus, it could be concluded
                that PLS was an available alternative of for qualitative weed
                discrimination on NIRS.",
  volume     = "6047 II",
  year       =  2006,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749660146&doi=10.1117%2f12.710957&partnerID=40&md5=c967c2b300ec31e5b6908f6dc68d3e2c",
  keywords   = "Near-infrared reflectance spectra; Partial least squares; Plant
                recognition; Site-specific weed management; Near-infrared
                reflectance spectra; Partial least squares; Plant recognition;
                Site-specific weed management; Crops; Herbicides; Least squares
                approximations; Plants (botany); Principal component analysis;
                Weed control; Infrared spectroscopy; Farm Crops; Herbicides;
                Infrared Spectroscopy; Plants; Weed Control;weed discrimination",
  conference = "4th International Conference on Photonics and Imaging in
                Biology and Medicine",
  location   = "Tianjin",
  isbn       = "9780819460806",
  doi        = "10.1117/12.710957"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Honig2021-df,
  title        = "Farmworkers face a life-and-death commute to Arizona's
                  lettuce fields",
  booktitle    = "Food and Environment Reporting Network",
  author       = "Honig, Esther",
  abstract     = "It's one in the morning and the stars are out as hundreds of
                  people shuffle slowly along the wall that marks the U.S.
                  border in the small Mexican city of San Luis R{\'\i}o
                  Colorado. In heavy boots and wide…",
  month        =  apr,
  year         =  2021,
  url          = "https://thefern.org/2021/04/farmworkers-face-a-life-and-death-commute-to-arizonas-lettuce-fields/",
  howpublished = "\url{https://thefern.org/2021/04/farmworkers-face-a-life-and-death-commute-to-arizonas-lettuce-fields/}",
  note         = "Accessed: 2022-2-3",
  keywords     = "field labor",
  language     = "en"
}

@INCOLLECTION{Brivot1996-af,
  title     = "Segmentation of plants and weeds using infrared images",
  booktitle = "Acta Horticulturae",
  author    = "Brivot, R and Marchant, J A",
  abstract  = "The work presented here is part of a project whose goal is to
               guide automatically a vehicle along rows of crops in order to
               target herbicide or pesticide accurately on weeds or plants.
               This paper deals with the segmentation of near-infrared images
               for discriminating plants, weeds, and soil. The algorithm
               described has been developed with a real-time parallel
               implementation and a real agricultural environment in mind. This
               algorithm uses techniques based on hysteresis thresholding and
               region labelling methods, blob-filtering and mathematical
               morphology. It can provide information on the size and the
               position of plants and weed patches as well as information that
               could be used either for guiding the vehicle or tracking plants
               and weeds. Successful segmentation has been done on several
               image sequences in diffuse light conditions. We show that it is
               possible to discriminate plants, weeds, and soil for different
               fields of view.",
  publisher = "International Society for Horticultural Science",
  volume    =  406,
  pages     = "165--172",
  year      =  1996,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013424884&doi=10.17660%2fActaHortic.1996.406.16&partnerID=40&md5=4066a07c2f9ea08f2704ccc101dbcb68"
}

@ARTICLE{Kamath2020-mx,
  title     = "Crop and weed discrimination using laws' texture masks",
  author    = "Kamath, R and Balachandra, M and Prabhu, S",
  abstract  = "Computers have become an integral part of human lives. Computers
               are used in almost every field even in agriculture. Technologies
               like computer vision-based pattern recognition are being used to
               detect diseases and pests like weeds affecting the crop. The
               Weeds are unwanted plants growing among crops competing for
               nutrients, water, and sunlight. It can significantly reduce the
               quality and yield of the crops incurring a huge loss to the
               farmers. This paper investigates the use of texture features
               extracted from Laws' texture masks for discrimination of Carrot
               crops and weeds in digital images. Laws' texture method is one
               of the popular methods used to extract texture features in
               medical image processing, though not much explored in
               plant-based images or agricultural images. This experiment was
               carried out on two categories of benchmark digital image
               datasets of Carrot crop and Carrot weed respectively, which are
               publicly available. A total of 70 texture features were
               extracted. The dimensionality reduction technique was used to
               get the optimal features. These features were then used to train
               the Random Forest classifier. The results and observations from
               the experiment showed that the classifier achieved above 94\%
               accuracy. \copyright{} 2020, Chinese Society of Agricultural
               Engineering. All rights reserved.",
  journal   = "International Journal of Agricultural and Biological Engineering",
  publisher = "Chinese Society of Agricultural Engineering",
  volume    =  13,
  number    =  1,
  pages     = "191--197",
  year      =  2020,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081009680&doi=10.25165%2fj.ijabe.20201301.4920&partnerID=40&md5=1fccce9c4eac30676cf336ee8bab310d",
  keywords  = "Classifier; Crop; Precision agriculture; Texture analysis; Weed",
  issn      = "1934-6344"
}

@INCOLLECTION{Sujith2021-gv,
  title     = "Classification of plant leaf using shape and texture features",
  booktitle = "4th International Conference on Inventive Communication and
               Computational Technologies, {ICICCT} 2020",
  author    = "Sujith, A and Neethu, R",
  editor    = "{Ranganathan G.} and {Chen J.} and {Rocha A.}",
  abstract  = "Plants are mainly classified based on their characteristics of
               plant components such as leaves, flower, stem, root, seed, etc.
               Feature or characteristics is an essential fact for plant
               classification. A good feature extraction technique can help to
               extract quality features that give clear information to
               discriminate against each class. Computer engineers can help
               botanists to identify plants and their species through advanced
               computational techniques with the stipulated time. The proposed
               method gives efficient hybrid feature extraction using the PHOG,
               LBP, and GLCM feature extraction techniques. The fused feature
               vector is normalized and reduced size by Neighborhood Components
               Analysis (NCA). The efficient feature extraction and feature
               selection techniques have helped to improve the classification
               performance and reduced the model complexity. Two benchmark
               plant dataset Flavia and Swedish Leaves used to evaluate the
               proposed work. The primary contributions of this paper are
               introducing a multi-feature fusion shape and texture method for
               plant leaf image classification. The experimental result shows
               the average accuracy of the proposed method is 98.23\%, and the
               average computational complexity is 147.98 s. \copyright{} The
               Editor(s) (if applicable) and The Author(s), under exclusive
               license to Springer Nature Singapore Pte Ltd 2021.",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  volume    =  145,
  pages     = "269--282",
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092080888&doi=10.1007%2f978-981-15-7345-3_22&partnerID=40&md5=f92582e3689f8f5ec3bc7353a4113168",
  keywords  = "Gray level co-occurrence matrix; Local binary pattern;
               Neighborhood components analysis; Plant classification; Pyramid
               histogram of oriented gradients;weed discrimination",
  isbn      = "9789811573446",
  doi       = "10.1007/978-981-15-7345-3\_22"
}

@ARTICLE{Yang2021-oc,
  title     = "Plant leaf recognition by integrating shape and texture features",
  author    = "Yang, C",
  abstract  = "Plant leaf identification is a significant challenge in the
               fields of computer vision and pattern recognition. This article
               presents a new approach to plant leaf identification, one that
               integrates shape and texture characteristics. First, we
               introduce the shape and texture features used by the proposed
               plant leaf recognition method. The proposed multiscale triangle
               descriptor (MTD) is employed to characterize the shape
               information of a plant leaf, and the local binary pattern
               histogram Fourier (LBP-HF) is used as the texture feature. Then,
               the shape and texture features of a leaf image are combined by
               weighted distance measurement, where L1 distance and chi-square
               distance are used for shape and texture features, respectively.
               The proposed approach provides a robust descriptor for the task
               of plant leaf recognition by combining the complementary MTD and
               LBP-HF features. The proposed approach has been thoroughly
               evaluated on three benchmark leaf datasets, including the
               Flavia, Swedish and MEW2012 leaf datasets. Our method achieves
               77.6\%, 85.7\%, and 67.5\% retrieval accuracy on the Flavia,
               Swedish and MEW2012 leaf datasets, respectively, while the
               corresponding classification accuracy is 99.1\%, 98.4\%, 95.6\%.
               The recognition performance of our method is better or
               comparable to prior state-of-the-art plant leaf recognition
               method. \copyright{} 2020",
  journal   = "Pattern Recognit.",
  publisher = "Elsevier Ltd",
  volume    =  112,
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099478543&doi=10.1016%2fj.patcog.2020.107809&partnerID=40&md5=b0e8efbb8fe5fada80171a23a489b260",
  keywords  = "Local binary pattern; Multiscale triangle descriptor; Plant leaf
               recognition; Shape descriptor; Texture feature; Classification
               (of information); Textures; Chi Square distance; Classification
               accuracy; Local binary patterns; Retrieval accuracy; Shape and
               textures; Shape information; State of the art; Texture features;
               Pattern recognition;weed discrimination",
  issn      = "0031-3203",
  doi       = "10.1016/j.patcog.2020.107809"
}

@MISC{Hall-Beyer2017-rc,
  title        = "{GLCM} Tutorial",
  author       = "Hall-Beyer, Mryka",
  month        =  mar,
  year         =  2017,
  url          = "http://dx.doi.org/10.11575/PRISM/33280",
  howpublished = "\url{http://dx.doi.org/10.11575/PRISM/33280}",
  note         = "Accessed: NA-NA-NA",
  doi          = "10.11575/PRISM/33280"
}

@INCOLLECTION{Montalvo2016-yo,
  title     = "Identification of plant textures in agricultural images by
               principal component analysis",
  booktitle = "11th International Conference on Hybrid Artificial Intelligent
               Systems, {HAIS} 2016",
  author    = "Montalvo, M and Guijarro, M and Guerrero, J M and Ribeiro, {\'A}",
  editor    = "{Martinez-Alvarez F.} and {Troncoso A.} and {Quintian H.} and
               {Corchado E.}",
  abstract  = "In precision agriculture the extraction of green parts is a very
               important task. One of the biggest issues, when it comes to
               computer vision, is image segmentation, which has motivated the
               research conducted in this work. Our goal is the segmentation of
               vegetative and soil parts in the images. For this proposal a
               novel method of segmentation is defined in which different
               vegetation indices are calculated and through the reduction of
               components by principal component analysis (PCA) we obtain an
               enhanced greyscale image. Finally, by Otsu thresholding, we
               binarize the grayscale image isolating the green parts from the
               other elements in the image. \copyright{} Springer International
               Publishing Switzerland 2016.",
  publisher = "Springer Verlag",
  volume    =  9648,
  pages     = "391--401",
  year      =  2016,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964036740&doi=10.1007%2f978-3-319-32034-2_33&partnerID=40&md5=a6843a72b7feb95dce515571d1d10454",
  keywords  = "Agricultural images; Precision agriculture; Principal component
               analysis; Segmentation image; Thresholding; Agriculture;
               Artificial intelligence; Computer vision; Image analysis; Image
               segmentation; Intelligent systems; Agricultural images;
               Gray-scale images; Grey scale images; Otsu thresholding;
               Precision Agriculture; Segmentation images; Thresholding;
               Vegetation index; Principal component analysis;reviewed",
  isbn      = "9783319320335",
  doi       = "10.1007/978-3-319-32034-2\_33"
}

@ARTICLE{Chaki2015-bc,
  title     = "Plant leaf recognition using texture and shape features with
               neural classifiers",
  author    = "Chaki, J and Parekh, R and Bhattacharya, S",
  abstract  = "Abstract This paper proposes a novel methodology of
               characterizing and recognizing plant leaves using a combination
               of texture and shape features. Texture of the leaf is modeled
               using Gabor filter and gray level co-occurrence matrix (GLCM)
               while shape of the leaf is captured using a set of curvelet
               transform coefficients together with invariant moments. Since
               these features are in general sensitive to the orientation and
               scaling of the leaf image, a pre-processing stage prior to
               feature extraction is applied to make corrections for varying
               translation, rotation and scaling factors. Efficacy of the
               proposed methods is studied by using two neural classifiers: a
               neuro-fuzzy controller (NFC) and a feed-forward back-propagation
               multi-layered perceptron (MLP) to discriminate between 31
               classes of leaves. The features have been applied individually
               as well as in combination to investigate how recognition
               accuracies can be improved. Experimental results demonstrate
               that the proposed approach is effective in recognizing leaves
               with varying texture, shape, size and orientations to an
               acceptable degree. \copyright{} 2015 Elsevier B.V. All rights
               reserved.",
  journal   = "Pattern Recognit. Lett.",
  publisher = "Elsevier",
  volume    =  58,
  pages     = "61--68",
  year      =  2015,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925386662&doi=10.1016%2fj.patrec.2015.02.010&partnerID=40&md5=e14376e23185d1f11eb229149fcd2d61",
  keywords  = "Curvelet transform; Feed-forward back-propagation multi-layered
               perceptron; Gabor filter; Gray level co-occurrence matrix;
               Invariant moments; Neuro-fuzzy controller; Backpropagation;
               Feature extraction; Fuzzy filters; Gabor filters; Plants
               (botany); Textures; Curvelet transforms; Gray level
               co-occurrence matrix; Invariant moment; Multi-layered
               Perceptron; Neuro-fuzzy controller; Fuzzy inference",
  issn      = "0167-8655",
  doi       = "10.1016/j.patrec.2015.02.010"
}

@ARTICLE{Chaki2015-gf,
  title     = "Plant leaf recognition using texture and shape features with
               neural classifiers",
  author    = "Chaki, J and Parekh, R and Bhattacharya, S",
  abstract  = "Abstract This paper proposes a novel methodology of
               characterizing and recognizing plant leaves using a combination
               of texture and shape features. Texture of the leaf is modeled
               using Gabor filter and gray level co-occurrence matrix (GLCM)
               while shape of the leaf is captured using a set of curvelet
               transform coefficients together with invariant moments. Since
               these features are in general sensitive to the orientation and
               scaling of the leaf image, a pre-processing stage prior to
               feature extraction is applied to make corrections for varying
               translation, rotation and scaling factors. Efficacy of the
               proposed methods is studied by using two neural classifiers: a
               neuro-fuzzy controller (NFC) and a feed-forward back-propagation
               multi-layered perceptron (MLP) to discriminate between 31
               classes of leaves. The features have been applied individually
               as well as in combination to investigate how recognition
               accuracies can be improved. Experimental results demonstrate
               that the proposed approach is effective in recognizing leaves
               with varying texture, shape, size and orientations to an
               acceptable degree. \copyright{} 2015 Elsevier B.V. All rights
               reserved.",
  journal   = "Pattern Recognit. Lett.",
  publisher = "Elsevier",
  volume    =  58,
  pages     = "61--68",
  year      =  2015,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925386662&doi=10.1016%2fj.patrec.2015.02.010&partnerID=40&md5=e14376e23185d1f11eb229149fcd2d61",
  keywords  = "Curvelet transform; Feed-forward back-propagation multi-layered
               perceptron; Gabor filter; Gray level co-occurrence matrix;
               Invariant moments; Neuro-fuzzy controller; Backpropagation;
               Feature extraction; Fuzzy filters; Gabor filters; Plants
               (botany); Textures; Curvelet transforms; Gray level
               co-occurrence matrix; Invariant moment; Multi-layered
               Perceptron; Neuro-fuzzy controller; Fuzzy inference",
  issn      = "0167-8655",
  doi       = "10.1016/j.patrec.2015.02.010"
}

@ARTICLE{Concepcion2020-qo,
  title   = "Lettuce Canopy Area Measurement Using Static Supervised Neural
             Networks Based on Numerical Image Textural Feature Analysis of
             Haralick and Gray Level {Co-Occurrence} Matrix",
  author  = "Concepcion, II, Ronnie S and Lauguico, Sandy C and Alejandrino,
             Jonnel D and Dadios, Elmer P and Sybingco, Edwin",
  journal = "Malang",
  volume  =  42,
  number  =  3,
  pages   = "472--486",
  month   =  oct,
  year    =  2020,
  url     = "http://dx.doi.org/10.17503/agrivita.v42i3.2528",
  doi     = "10.17503/agrivita.v42i3.2528"
}

@ARTICLE{Raja2020-wi,
  title     = "Real-time weed-crop classification and localisation technique
               for robotic weed control in lettuce",
  author    = "Raja, R and Nguyen, T T and Slaughter, D C and Fennimore, S A",
  abstract  = "Robotic weed control for vegetables is necessary to increase
               crop productivity, avoid intensive hand weeding as labour
               shortages in developed countries such as United States has led
               to a surge in food production costs. However, development of a
               reliable, intelligent robotic system for weed control in
               real-time for vegetables still remains a challenging task. The
               main issue arises while distinguishing crops from weeds in
               real-time. In this paper, a novel technique to crop signalling
               to distinguish crops from in-row weeds in complex natural
               scenarios, such as high weed densities commonly found on organic
               farms, in real-time using machine vision is presented. Crop
               signalling is a simple and low-cost technique in which a
               signalling compound is produced by or applied to the crop and
               where the signalling compound is machine readable and helps to
               create visual features that uniquely distinguish the crops from
               weeds. The crop and weed mapping algorithm presented here were
               specially designed and developed for a vision-based weeding
               robot equipped with a micro-jet herbicide-spraying system for
               weed control in a lettuce field. The proposed technique involves
               weed/crop mapping and decision making. Experimental results show
               that the crop detection accuracy was 99.75\%, and 98.11\% of
               sprayable weeds were detected. The proposed technique is highly
               accurate, reliable and more robust than other sensor-based
               techniques presented in the literature. \copyright{} 2020 IAgrE",
  journal   = "Biosystems Eng.",
  publisher = "Academic Press",
  volume    =  192,
  pages     = "257--274",
  year      =  2020,
  url       = "http://dx.doi.org/10.1016/j.biosystemseng.2020.02.002",
  keywords  = "Automatic weed control; crop signalling; machine vision;
               precision agriculture; robotics; Computer vision; Conformal
               mapping; Costs; Crops; Decision making; Intelligent robots;
               Precision agriculture; Robotics; Vegetables; Crop
               classification; Crop productivity; Detection accuracy; Developed
               countries; Food production; Intelligent robotic systems; Novel
               techniques; Spraying system; Weed control;reviewed",
  issn      = "1537-5110",
  doi       = "10.1016/j.biosystemseng.2020.02.002"
}

@ARTICLE{Gomathi2020-pj,
  title     = "Meta-heuristic based trained deep convolutional neural network
               for crop/weed classification",
  author    = "Gomathi, N and Jagtap, A M",
  abstract  = "Computer vision and camera sensors are hopeful technologies for
               capturing information and processing to facilitate autonomous
               cultivation with machine learning. Nowadays, field robots are
               widely utilized, which autonomously navigates in fields tasks
               for advanced developments. However, manual activities are also
               required for certain needs, for instance, controlling weed in
               organic carrot farming is carried out manually and it is
               essential to evade considerable crop yield loss. This paper
               makes an attempt to introduce a new automatic crop/weed
               classification under three major phases: (i) Pre-processing (ii)
               Feature Extraction and (iii) Classification. Initially, the
               images are converted to greyscale images under pre-processing
               stage. Further, from the pre-processed image, the features like
               ``Grey Level Co-occurrence Matrix (GLCM) and Grey Level
               Runlength Matrix (GLRM) based texture features'' are extracted.
               Finally, the classification is done by the hybrid classifiers,
               where both the Deep Convolutional Neural Network (DCNN) and
               Neural Network (NN) is incorporated. Finally, both the
               classified outputs are ORed to get the final classification
               output. Moreover, in order to enhance the performance of
               proposed work, it is planned to tune the hidden neurons of NN
               optimally via a new improved Moth Search Algorithm (MSA) and is
               named as Moth Search with new Step Length evaluation (MS-SL).
               Finally, the performance of proposed work is evaluated over
               other state-of-the-art models with respect to certain
               performance measures. \copyright{} 2020, World Academy of
               Research in Science and Engineering. All rights reserved.",
  journal   = "International Journal of Emerging Trends in Engineering Research",
  publisher = "World Academy of Research in Science and Engineering",
  volume    =  8,
  number    =  7,
  pages     = "3915--3926",
  year      =  2020,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090297627&doi=10.30534%2fijeter%2f2020%2f161872020&partnerID=40&md5=4d864dcdcaf85499c6f0714982e3410f",
  keywords  = "Crop classification; DCNN; GLCM; GLRM; Moth Search
               Algorithm;glcm;reviewed",
  issn      = "2347-3983",
  doi       = "10.30534/ijeter/2020/161872020"
}

@ARTICLE{Sabzi2020-af,
  title    = "An automatic visible-range video weed detection, segmentation and
              classification prototype in potato field",
  author   = "Sabzi, Sajad and Abbaspour-Gilandeh, Yousef and Arribas, Juan
              Ignacio",
  abstract = "Weeds might be defined as destructive plants that grow and
              compete with agricultural crops in order to achieve water and
              nutrients. Uniform spray of herbicides is nowadays a common cause
              in crops poisoning, environment pollution and high cost of
              herbicide consumption. Site-specific spraying is a possible
              solution for the problems that occur with uniform spray in
              fields. For this reason, a machine vision prototype is proposed
              in this study based on video processing and meta-heuristic
              classifiers for online identification and classification of
              Marfona potato plant (Solanum tuberosum) and 4299 samples from
              five weed plant varieties: Malva neglecta (mallow), Portulaca
              oleracea (purslane), Chenopodium album L (lamb's quarters),
              Secale cereale L (rye) and Xanthium strumarium (coklebur). In
              order to properly train the machine vision system, various videos
              taken from two Marfona potato fields within a surface of six
              hectares are used. After extraction of texture features based on
              the gray level co-occurrence matrix (GLCM), color features,
              spectral descriptors of texture, moment invariants and shape
              features, six effective discriminant features were selected: the
              standard deviation of saturation (S) component in HSV color
              space, difference of first and seventh moment invariants, mean
              value of hue component (H) in HSI color space, area to length
              ratio, average blue-difference chrominance (Cb) component in
              YCbCr color space and standard deviation of in-phase (I)
              component in YIQ color space. Classification results show a high
              accuracy of 98\% correct classification rate (CCR) over the test
              set, being able to properly identify potato plant from previously
              mentioned five different weed varieties. Finally, the machine
              vision prototype was tested in field under real conditions and
              was able to properly detect, segment and classify weed from
              potato plant at a speed of up to 0.15 m/s.",
  journal  = "Heliyon",
  volume   =  6,
  number   =  5,
  pages    = "e03685",
  month    =  may,
  year     =  2020,
  url      = "http://dx.doi.org/10.1016/j.heliyon.2020.e03685",
  keywords = "Agricultural engineering; Agricultural soil science; Agricultural
              technology; Agriculture; Classification; Computational
              intelligence; Computer engineering; Computer simulation; Food
              engineering; Food science; Horticulture; Machine vision;
              Meta-heuristic algorithms; Plant competition; Site-specific
              spraying; Video processing; Video processing.;glcm",
  language = "en",
  issn     = "2405-8440",
  pmid     = "32490222",
  doi      = "10.1016/j.heliyon.2020.e03685",
  pmc      = "PMC7260593"
}

@MISC{Wirth2004-li,
  title        = "Shape Analysis and Measurement",
  author       = "Wirth, Michael A",
  year         =  2004,
  url          = "http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth10.pdf",
  howpublished = "Microscopy II: Image Analysis and 3D Reconstruction",
  keywords     = "reviewed;image processing",
  copyright    = "2004"
}

@PROCEEDINGS{Andujar2013-ot,
  title      = "Weed-crop discrimination using {LiDAR} measurements",
  author     = "And{\'u}jar, D and Moreno, H and Valero, C and Gerhards, R and
                Griepentrog, H W",
  abstract   = "In this paper, we propose a new approach for discriminating
                maize and weed plants from soil surface, evaluating the
                accuracy and performance of a LiDAR sensor for vegetation
                detection using distance and reflection values. Field
                measurements were conducted in a maize field at growth stage
                BBCH 12-14. Static measurements were taken at different
                sampling areas with different weed densities. Regression
                analyses were carried out to assess the capabilities of the
                system for vegetation and soil measurement. A high relationship
                between LiDAR measured distance (LiDAR heights) and actual
                height was found. A binary logistic regression was used to
                predict the presence or absence of vegetation. The results
                permitted the discrimination of vegetation from the soil with
                accuracy up to 95\%. This technique offers significant promise
                for the development of real-time spatially selective weed
                control techniques, either as the sole weed detection system or
                in combination with other detection tools.",
  year       =  2013,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893416174&partnerID=40&md5=2fd8355844d3f81a6dace79420c0da04",
  keywords   = "Laser scanning; Plant height; Reflection; Weed detection;
                Binary logistic regression; Control techniques; Field
                measurement; Laser scanning; Lidar measurements; Plant height;
                Static measurements; Weed detection; Agriculture; Optical
                radar; Reflection; Regression analysis; Vegetation; Weed
                control",
  conference = "9th European Conference on Precision Agriculture, ECPA 2013",
  location   = "Lleida, Catalonia",
  isbn       = "9789086862245"
}

@ARTICLE{Shahbazi2021-kr,
  title     = "Comparison of crop and weed height, for potential
               differentiation of weed patches at harvest",
  author    = "Shahbazi, N and Flower, K C and Callow, J N and Mian, A and
               Ashworth, M B and Beckie, H J",
  abstract  = "Weeds and weed control are major production costs in global
               agriculture, with increasing challenges associated with
               herbicide-based management because of concerns with chemical
               residue and herbicide resistance. Non-chemical weed management
               may address these challenges but requires the ability to
               differentiate weeds from crops. Harvest is an ideal opportunity
               for the differentiation of weeds that grow taller than the crop,
               however, the ability to differentiate late-season weeds from the
               crop is unknown. Weed mapping enables farmers to locate weed
               patches, evaluate the success of previous weed management
               strategies, and assist with planning for future herbicide
               applications. The aim of this study was to determine whether
               weed patches could be differentiated from the crop plants, based
               on height differences. Field surveys were carried out before
               crop harvest in 2018 and 2019, where a total of 86 and 105 weedy
               patches were manually assessed respectively. The results of this
               study demonstrated that across the 191 assessed weedy patches,
               in 97\% of patches with Avena fatua (wild oat) plants, 86\% with
               Raphanus raphanistrum (wild radish) plants and 92\% with Sonchus
               oleraceus L. (sow thistles) plants it was possible to
               distinguish the weeds taller than the 95\% of the crop plants.
               Future work should be dedicated to the assessment of the ability
               of remote sensing methods such as Light Detection and Ranging to
               detect and map late-season weed species based on the results
               from this study on crop and weed height differences.
               \copyright{} 2020 European Weed Research Society",
  journal   = "Weed Res.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  61,
  number    =  1,
  pages     = "25--34",
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097017406&doi=10.1111%2fwre.12450&partnerID=40&md5=9100bbbcdff8dafcf5ee83f8c55d1674",
  keywords  = "height; late-season weed control; site-specific weed management;
               weed-crop differentiation; cereal; comparative study; crop
               plant; differentiation; field survey; harvesting; height;
               herbicide; lidar; mapping method; remote sensing; weed; weed
               control; Avena fatua; Raphanus raphanistrum; Raphanus sativus;
               Sonchus oleraceus",
  issn      = "0043-1737",
  doi       = "10.1111/wre.12450"
}

@BOOK{Gonzalez2016-xu,
  title     = "Digital Image Processing",
  author    = "Gonzalez, Rafael C and Woods, Richard E",
  publisher = "Addison-Wesley Pub (Sd)",
  year      =  2016,
  isbn      = "9780133356724"
}

@ARTICLE{Barati2011-oj,
  title    = "Comparison the accuracies of different spectral indices for
              estimation of vegetation cover fraction in sparse vegetated areas",
  author   = "Barati, Susan and Rayegani, Behzad and Saati, Mehdi and Sharifi,
              Alireza and Nasri, Masoud",
  abstract = "Quantitative estimation of canopy biophysical variables are very
              important in different studies such as meteorology, agriculture
              and ecology, so knowledge of the spatial and temporal
              distribution of these variables would be highly beneficial.
              Meanwhile, remote sensing is known as an important source of
              information to estimate fractional vegetation cover in large
              areas. Today spectral indices have been very popular in the
              remote sensing of vegetation features. But often reflections of
              soil and rocks are much more than reflections of sparse
              vegetation in these areas, that makes separation of plant signals
              difficult. So in this study measured fractional vegetation cover
              of a desert area were evaluated with 20 vegetation indices in
              five different categories as the most appropriate category, or
              indicator for desert vegetation to be identified. The five
              categories were including: (1) conventional ratio and
              differential indices such as NDVI; (2) indices corrected and
              derived from the traditional indicators such as NDVIc and GNDVI;
              (3) soil reflectance adjusted indices such as SAVI; (4) triangle
              indices based on three discreet bands in their equation (Green,
              Red and NIR) like TVI; and (5) non-conventional ratio and
              differential indices such as CI. According to the results of this
              research, DVI index with 0.668 the coefficient of determination
              (R2) showed the best fractional vegetation cover estimation. But
              according to the sparse vegetation in desert areas and the
              results of this research it seems none of these indicators alone
              can accurately estimate the percentage of vegetation cover,
              however, to do a proper estimation it is possible to enter data
              of these indices in a multivariate regression model. Using this
              method enabled us to increase the coefficient of determination of
              fractional vegetation cover estimation model up to 0.797.",
  journal  = "Egypt. J. Remote Sens. Space Sci.",
  volume   =  14,
  number   =  1,
  pages    = "49--56",
  month    =  jun,
  year     =  2011,
  url      = "https://www.sciencedirect.com/science/article/pii/S1110982311000147",
  keywords = "Vegetation cover fraction; Remote sensing; LISS III; Vegetation
              indices",
  issn     = "1110-9823",
  doi      = "10.1016/j.ejrs.2011.06.001"
}

@MISC{Various_undated-lg,
  title        = "{CyVerse} Documentation",
  author       = "{Various}",
  url          = "https://learning.cyverse.org/_/downloads/foss/en/foss-2020-spring/pdf/",
  howpublished = "\url{https://learning.cyverse.org/_/downloads/foss/en/foss-2020-spring/pdf/}"
}

@MISC{Various_undated-oj,
  title        = "{CircleCI} Documentation",
  author       = "{Various}",
  url          = "https://circleci.com/docs/2.0/configuration-reference/",
  howpublished = "\url{https://circleci.com/docs/2.0/configuration-reference/}"
}

@MISC{Various_undated-yv,
  title        = "{HSL} and {HSV}",
  booktitle    = "Wikipedia",
  author       = "{Various}",
  url          = "https://en.wikipedia.org/wiki/HSL_and_HSV",
  howpublished = "\url{https://en.wikipedia.org/wiki/HSL_and_HSV}"
}

@INCOLLECTION{Sheeba2021-pd,
  title     = "A Comparison Study on Various Continuous Integration Tools in
               Software Development",
  booktitle = "4th International Conference on Information and Communication
               Technology for Intelligent {Systems,ICTIS} 2020",
  author    = "{Sheeba} and Shidaganti, G and Gosar, A P",
  editor    = "{Joshi A.} and {Khosravy M.} and {Gupta N.}",
  abstract  = "Continuous integration (CI) frameworks are currently the basis
               for a few software development projects that reduce the time and
               cost of the software. Most of the companies provide CI tools
               with similar facilities and some companies build their own CI
               tool. However, it does not mean that the costliest product is
               better than a low-cost or open-source device. Generally, all of
               these tools are intended to make the Product Building Software
               Process easy and automated by quality assurance and time
               reduction. In spite of this fact, ongoing integration tools have
               their own merits and demerits, so it is very important and
               sometimes difficult to choose the right CI tool for a project.
               The wrong choice of tools can reduce the overall flexibility and
               quality of the software. A few continuous integration tools,
               such as Jenkins, TravisCI, Codeship, Bamboo, TeamCity and
               CircleCI, which implement CI practices, are generally adopted by
               programmers, developers. The main objective of this paper is to
               demonstrate the analysis, usefulness and comparison of selected
               tools and to help in selecting the most compatible CI
               implementation tool that meets project requirements.
               \copyright{} 2021, The Editor(s) (if applicable) and The
               Author(s), under exclusive license to Springer Nature Singapore
               Pte Ltd.",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  volume    =  141,
  pages     = "65--76",
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096450028&doi=10.1007%2f978-981-15-7106-0_7&partnerID=40&md5=2c7c1f0fbfe2113290377747fc59fedf",
  keywords  = "CI tool; Continuous integration; Continuous integration
               framework; Software development projects; Software project",
  isbn      = "9789811571053",
  doi       = "10.1007/978-981-15-7106-0\_7"
}

@PROCEEDINGS{Liao2020-xv,
  title      = "Modelling {CI/CD} Pipeline through {Agent-Based} Simulation",
  author     = "Liao, Q",
  editor     = "{Vieira M.} and {Madeira H.} and {Antunes N.} and {Zheng Z.}",
  abstract   = "The need for rapid and efficient software development pushes
                the demand for automation in the phases of build, test, and
                release. Thereby, the methodology of Continuous Integration and
                Continuous Deployment (CI/CD) emerges, which then gives birth
                to a set of CI/CD enabling services, such as Travis CI and
                Jenkins. Those services facilitate the automatic compilation,
                connection tracking, and packaging of new features. They not
                only incorporate playgrounds for testing and functionality
                verification but also enable the final delivery.Poor
                understanding and execution in CI/CD operations can result in
                slowing and even halting the pace of a software project. Many
                bottlenecks of CI/CD pipeline might occur due to its incorrect
                configurations, i.e. the inadequate level of automation, the
                unsuitable load capacity and the suboptimal queueing strategy.
                However, understanding the actual CI/CD pipeline is hard since
                its performance varies significantly with different hosting
                machines, technologies and plugins. On the other hand, finding
                a way to analyse and improve the settings of CI/CD pipeline
                brings great managerial and economic benefits since an optimal
                configuration implies the eventual high efficiency. To that
                end, this study attempts to design a model that can not only
                capture the abstraction of the pipeline but also provides a
                testing environment for the impersonal influencers of CI/CD
                performance. The current study, therefore, aims to contribute
                (1) a pipeline model based on the logic of the queueing system
                and enabled by agent-based simulation, and (2) an experimental
                environment which allows the testing of different settings and
                operation scenarios. \copyright{} 2020 IEEE.",
  publisher  = "Institute of Electrical and Electronics Engineers Inc.",
  year       =  2020,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099854360&doi=10.1109%2fISSREW51248.2020.00059&partnerID=40&md5=a6801c3b2d534a7dc4cd1016223e47d5",
  keywords   = "Agent-based Modelling; CI/CD; Multi-agent Simulation; Pipeline
                Simulation; Queueing System",
  conference = "31st IEEE International Symposium on Software Reliability
                Engineering Workshops, ISSREW 2020",
  isbn       = "9781728198705",
  doi        = "10.1109/ISSREW51248.2020.00059"
}

@ARTICLE{Msibi2021-to,
  title    = "High pesticide inhalation exposure from multiple spraying sources
              amongst applicators in Eswatini, Southern Africa",
  author   = "Msibi, Sithembiso S and Chen, Chung-Yu and Chang, Cheng-Ping and
              Chen, Chiou-Jong and Chiang, Su-Yin and Wu, Kuen-Yuh",
  abstract = "BACKGROUND: Serious concerns surround the potential risks
              resulting from inhalation exposure to pesticides amongst
              agricultural workers when mixing and applying these compounds. In
              Eswatini (formerly known as Swaziland), Southern Africa,
              pesticides are widely used to improve the yield and quality of
              sugar cane production, the largest contributor to the country's
              economy. We assessed applicators' inhalation exposures from
              multiple spraying sources to four commonly used herbicides in
              Eswatini. RESULTS: Analysis of 76 personal air samples by liquid
              chromatography with tandem mass spectrometry (LC-MS/MS) revealed
              four pesticides: ametryn, atrazine, pendimethalin and
              2,4-dichlorophenoxyacetic acid, with mean concentrations of
              36.91, 21.57, 31.05 and 0.89 $\mu$g m-3 , respectively. These
              inhalation exposures are much higher than those recorded in
              previous similar studies. CONCLUSION: Although all applicators in
              this study used personal protective equipment (PPE), they
              nevertheless recorded high levels of inhalation exposure to
              commonly used pesticides. Our findings suggest that in addition
              to observing mandated regular changing and cleaning practices
              with PPE for ultimate personal protection, pesticide applicators
              should distance themselves from each other when spraying to
              effectively reduce their exposure to pesticides from multiple
              spraying sources. Further studies are needed to determine the
              optimal spraying distance between pesticide applicators.
              \copyright{} 2021 Society of Chemical Industry.",
  journal  = "Pest Manag. Sci.",
  volume   =  77,
  number   =  10,
  pages    = "4303--4312",
  month    =  oct,
  year     =  2021,
  url      = "http://dx.doi.org/10.1002/ps.6459",
  keywords = "Africa; Eswatini; applicators; inhalation exposure; multiple
              spraying sources; pesticides;herbicides and health",
  language = "en",
  issn     = "1526-498X, 1526-4998",
  pmid     = "33942970",
  doi      = "10.1002/ps.6459"
}

@ARTICLE{Rydz2021-ar,
  title    = "Estimating Exposure to Three Commonly Used, Potentially
              Carcinogenic Pesticides (Chlorolathonil, 2,4-D, and Glyphosate)
              Among Agricultural Workers in Canada",
  author   = "Rydz, Ela and Larsen, Kristian and Peters, Cheryl E",
  abstract = "OBJECTIVES: Certain pesticides have been associated with adverse
              health outcomes including cancer and reproductive harms. However,
              little is known about the prevalence of occupational pesticide
              exposure among agricultural workers in Canada. The purpose of
              this study was to estimate the prevalence and likelihood of
              occupational exposure to pesticides in Canada's agricultural
              industry, using three commonly used, potentially carcinogenic
              pesticides [chlorothalonil, 2,4-dichlorophenoxyacetic acid
              (2,4-D), and glyphosate] as an example. METHODS: Estimates were
              calculated using the Canadian Census of Population and the Census
              of Agriculture. The number of workers and the proportion of farms
              applying 'herbicides' or 'fungicides' by farm type was estimated
              using survey data from the Census of Agriculture. These values
              were multiplied to yield the potential number of workers at risk
              of exposure. Likelihood of exposure (i.e. exposed, probably
              exposed, and possibly exposed) was then qualitatively assigned
              using information on crop type, primary expected tasks, crop
              production practices, and residue transfer data. Additional
              agricultural workers who are at risk of exposure but not captured
              by the Census of Agriculture were identified using the 2016
              Census of Population. RESULTS: An estimated range of 37 700-55
              800 workers (11-13\% of agricultural workers) were exposed to
              glyphosate in Canada while 30 800-43 600 workers (9-11\%) and
              9000-14 100 (2.9-3.2\%) were exposed to 2,4-D and chlorothalonil,
              respectively. Approximately 70-75\% of workers at risk of
              exposure were considered probably or possibly exposed to any of
              the pesticides. Glyphosate exposure was most common among workers
              in oilseed (29\% of oilseed farm workers exposed) and dry
              pea/bean farms (28\%), along with those providing support
              activities for farms (31\%). 2,4-D exposure was most common in
              corn (28\%), other grain (28\%), and soybean farms (27\%), while
              chlorothalonil exposure was more likely among greenhouse,
              nursery, and floriculture workers (42\%), workers on farms (28\%,
              for occupations not captured by the Census of Agriculture,
              specifically), and those providing support activities for farms
              (20\%). Regional variations broadly reflected differences in farm
              types by province. CONCLUSIONS: This study estimated the
              prevalence of occupational exposure to three pesticides in
              Canada. Seasonal and temporary agricultural workers, which were
              captured by the Census of Agriculture, contributed to many
              additionally exposed workers. A large percent of the workers who
              were considered at risk of exposure were considered probably or
              possibly exposed, indicating a need for enhanced data collection
              and availability on pesticide use data in Canada. The study's
              methods can be applied to estimate workers' exposures to other
              pesticides within the agricultural industry.",
  journal  = "Ann Work Expo Health",
  volume   =  65,
  number   =  4,
  pages    = "377--389",
  month    =  may,
  year     =  2021,
  url      = "http://dx.doi.org/10.1093/annweh/wxaa109",
  keywords = "2, 4-D; agriculture; chlorothalonil; glyphosate; occupational
              exposure; pesticides;herbicides and health",
  language = "en",
  issn     = "2398-7316, 2398-7308",
  pmid     = "33336237",
  doi      = "10.1093/annweh/wxaa109"
}

@ARTICLE{Silva-Madera2021-cx,
  title     = "Pesticide Contamination in Drinking and Surface Water in the
               Cienega, Jalisco, Mexico",
  author    = "Silva-Madera, R J and Salazar-Flores, J and Peregrina-Lucano, A
               A and Mendoza-Michel, J and Ceja-G{\'a}lvez, H R and
               Rojas-Bravo, D and Reyna-Villela, M Z and Torres-S{\'a}nchez, E
               D",
  abstract  = "Sixty percent of global agricultural production depends on the
               use of pesticides, despite their adverse effects on human health
               and the ecosystem. In Mexico, the application of these products
               has been exacerbated, including pesticides already banned in
               other countries. The objective of this study was to determine
               pesticide concentrations in samples of water purification plants
               and surface water from the Cienega area of Jalisco, Mexico. A
               survey of 119 farmers with occupational exposure to pesticides
               was carried out in order to obtain information on the most
               frequently used pesticides. Subsequently, 51 samples taken at 7
               different sites were analyzed using liquid chromatography and
               mass-mass spectrometry. The most frequently used pesticides were
               organophosphates (28.87\%), pyrethroids (12.89\%), and the
               herbicide paraquat (31.95\%). In surface water, the prevalent
               pesticides were glyphosate (56.96--510.46 ppb) and malathion
               (311.76--863.49 ppb). Glyphosate levels were higher than the
               limits acceptable in daily water intake in Cumuato. Malathion
               levels exceeded the limits permissible by EPA in water
               purification plants in urban public establishments (100 ppb for
               children, and 200 ppb for adults). In addition, a
               multidimensional scaling analysis showed that the sampled sites
               could be grouped into 2 different bodies of water, based on
               similarities in their glyphosate concentrations (stress =
               0.005), while the concentrations of malathion were heterogeneous
               (stress = 0.001). \copyright{} 2021, The Author(s).",
  journal   = "Water Air Soil Pollut.",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  volume    =  232,
  number    =  2,
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099950709&doi=10.1007%2fs11270-021-04990-y&partnerID=40&md5=321232d9101012746bb00cd7019ec677",
  keywords  = "Cienega-Jalisco; HPLC-MS/MS; Pesticides; Surface water; Water
               purification plants; Agricultural robots; Agriculture;
               Herbicides; Liquid chromatography; Mass spectrometry; Potable
               water; Purification; Water treatment plants; Adverse effect;
               Agricultural productions; Herbicide paraquat; Multidimensional
               scaling analysis; Occupational exposure; Pesticide
               concentrations; Pesticide contaminations; Water purification
               plants; Surface waters; drinking water; glyphosate; malathion;
               organophosphate; paraquat; pesticide; pyrethroid; concentration
               (composition); drinking water; glyphosate; liquid
               chromatography; malathion; mass spectrometry; occupational
               exposure; surface water; water pollution; water treatment plant;
               agricultural worker; Article; concentration (parameter); fluid
               intake; human; Jalisco; liquid chromatography-mass spectrometry;
               occupational exposure; surface water hydrology; urban area;
               water contamination; water management; water sampling; Jalisco;
               Mexico [North America];herbicides and health",
  issn      = "0049-6979",
  doi       = "10.1007/s11270-021-04990-y"
}

@ARTICLE{Pedroso2021-bk,
  title    = "Cancer and occupational exposure to pesticides: a bibliometric
              study of the past 10 years",
  author   = "Pedroso, Thays Millena Alves and Benvindo-Souza, Marcelino and de
              Ara{\'u}jo Nascimento, Felipe and Woch, J{\'u}lia and Dos Reis,
              Fabiana Gon{\c c}alves and de Melo E Silva, Daniela",
  abstract = "Occupational exposure to pesticides has been identified as a
              major trigger of the development of cancer. Pesticides can cause
              intoxication in the individuals who manipulate them through
              either inhalation, ingestion, or dermal contact. Given this, we
              investigated the association between the incidence of cancer and
              occupational exposure to pesticides through a bibliometric
              analysis of the studies published between 2011 and 2020, based on
              62 papers selected from the Scopus database. The results
              indicated an exponential increase in the number of studies
              published over the past decade, with most of the research being
              conducted in the USA, France, India, and Brazil, although a
              further 17 nations were also involved in the research on the
              association between cancer and pesticides. The principal classes
              of pesticides investigated in relation to their role in
              intoxication and cancer were insecticides, herbicides, and
              fungicides. The types of cancer reported most frequently were
              multiple myeloma, bladder cancer, non-Hodgkin's lymphoma,
              prostate cancer, leukemia, and breast cancer. Despite the known
              association between pesticides and cancer, studies are still
              relatively scarce in comparison with the global scale of the use
              of these xenobiotic substances, which is related to the
              increasing demand for agricultural products throughout the world.",
  journal  = "Environ. Sci. Pollut. Res. Int.",
  month    =  oct,
  year     =  2021,
  url      = "http://dx.doi.org/10.1007/s11356-021-17031-2",
  keywords = "Agriculture; Cancer; DNA; Diseases; Farm workers; Health;
              Pesticides;herbicides and health",
  language = "en",
  issn     = "0944-1344, 1614-7499",
  pmid     = "34668133",
  doi      = "10.1007/s11356-021-17031-2",
  pmc      = "PMC8525621"
}

@ARTICLE{Macfarlane2008-am,
  title    = "Training and other predictors of personal protective equipment
              use in Australian grain farmers using pesticides",
  author   = "Macfarlane, E and Chapman, A and Benke, G and Meaklim, J and Sim,
              M and McNeil, J",
  abstract = "OBJECTIVES: To investigate patterns of use of personal protective
              equipment (PPE) to reduce pesticide exposure in a sample of
              Australian farmers and also to assess the influence of possible
              predictive factors. METHODS: A cross-sectional survey of 1102
              farmers recruited through the Victorian Farmers Federation (VFF)
              was conducted. A written questionnaire was filled out by
              participants at VFF meetings attended by a visiting research
              assistant. Participants answered questions about frequency of
              pesticide use and PPE items they usually used when doing two
              different pesticide-related tasks, mixing and application, of
              each of four classes of pesticides. They also answered questions
              about personal characteristics, farm characteristics, farming
              activities, career and health. RESULTS: Nearly all surveyed
              farmers had ever used pesticides, and over 87\% had used
              Herbicides or Animal Health Products in the previous 12 months.
              Non-use of PPE was frequently reported, with up to 10-40\% of
              farmers routinely using no PPE at all when using pesticides.
              Across all pesticide classes, PPE use was higher for pesticide
              mixing than for application. In multivariate analyses PPE use
              appeared to be most strongly associated with younger age and farm
              chemical training. CONCLUSIONS: PPE use across all pesticide
              classes was poor, indicating the possibility of clinically
              significant pesticide exposure in many farmers. Given that PPE
              use was found to be associated with farm chemical training, the
              authors suggest that training is likely to be an important
              intervention for reducing farmers' pesticide exposure. Poor
              uptake of farm chemical training by farmers and the aging farming
              workforce are causes for concern in the light of these findings.",
  journal  = "Occup. Environ. Med.",
  volume   =  65,
  number   =  2,
  pages    = "141--146",
  month    =  feb,
  year     =  2008,
  url      = "http://dx.doi.org/10.1136/oem.2007.034843",
  language = "en",
  issn     = "1351-0711, 1470-7926",
  pmid     = "17704194",
  doi      = "10.1136/oem.2007.034843"
}

@MISC{US_Bureau_of_Labor_Statistics2021-yi,
  title        = "Occupational Employment and Wages in Yuma --- May 2020 :
                  Western Information Office : {U.S}. Bureau of Labor
                  Statistics",
  booktitle    = "{U.S}. Bureau of Labor Statistics",
  author       = "{US Bureau of Labor Statistics}",
  abstract     = "Workers in the Yuma, AZ Metropolitan Statistical Area had an
                  average (mean) hourly wage of $20.75 in May 2020, 23 percent
                  below the nationwide average of $27.07.",
  month        =  jun,
  year         =  2021,
  url          = "https://www.bls.gov/regions/west/news-release/occupationalemploymentandwages_yuma.htm",
  howpublished = "\url{https://www.bls.gov/regions/west/news-release/occupationalemploymentandwages_yuma.htm}",
  note         = "Accessed: 2022-1-28",
  keywords     = "herbicides and health;field labor",
  language     = "en"
}

@MISC{noauthor_undated-tt,
  title       = "pypylon: The official python wrapper for the pylon Camera
                 Software Suite",
  abstract    = "The official python wrapper for the pylon Camera Software
                 Suite - basler/pypylon: The official python wrapper for the
                 pylon Camera Software Suite",
  institution = "Github",
  url         = "https://github.com/basler/pypylon",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Wikipedia_contributors2022-qd,
  title        = "{YCbCr}",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "YCbCr, Y′CbCr, or Y Pb/Cb Pr/Cr, also written as YCBCR or
                  Y′CBCR, is a family of color spaces used as a part of the
                  color image pipeline in video and digital photography
                  systems. Y′ is the luma component and CB and CR are the
                  blue-difference and red-difference chroma components. Y′
                  (with prime) is distinguished from Y, which is luminance,
                  meaning that light intensity is nonlinearly encoded based on
                  gamma corrected RGB primaries.",
  month        =  jan,
  year         =  2022,
  url          = "https://en.wikipedia.org/w/index.php?title=YCbCr&oldid=1069029896",
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=YCbCr&oldid=1069029896}",
  note         = "Accessed: NA-NA-NA"
}

@MISC{Siemens2021-vn,
  author       = "Siemens, Mark",
  editor       = "McGinnis, Evan",
  abstract     = "Labor costs in Yuma, AZ",
  month        =  aug,
  year         =  2021,
  howpublished = "personal communication"
}

@ARTICLE{Siemens2020-ds,
  title    = "High Speed Centimeter Scale Resolution Sprayers for Precision
              Weed Control in Vegetable Crops",
  author   = "Siemens, M C",
  journal  = "AZ Veg IPM Update",
  volume   =  11,
  number   =  8,
  month    =  apr,
  year     =  2020,
  url      = "https://acis.cals.arizona.edu/agricultural-ipm/vegetables/vegetable-ipm-updates/high-speed-centimeter-scale-resolution-sprayers-for-precision-weed-control-in-vegetable-crops?Biometeorologypage=4&Insectpage=15&Weedspage=3&Trappingpage=7&PlantDiagnosticpage=6",
  keywords = "weeding systems"
}

@ARTICLE{Zhang2004-cm,
  title    = "Review of shape representation and description techniques",
  author   = "Zhang, Dengsheng and Lu, Guojun",
  abstract = "More and more images have been generated in digital form around
              the world. There is a growing interest in finding images in large
              collections or from remote databases. In order to find an image,
              the image has to be described or represented by certain features.
              Shape is an important visual feature of an image. Searching for
              images using shape features has attracted much attention. There
              are many shape representation and description techniques in the
              literature. In this paper, we classify and review these important
              techniques. We examine implementation procedures for each
              technique and discuss its advantages and disadvantages. Some
              recent research results are also included and discussed in this
              paper. Finally, we identify some promising techniques for image
              retrieval according to standard principles.",
  journal  = "Pattern Recognit.",
  volume   =  37,
  number   =  1,
  pages    = "1--19",
  month    =  jan,
  year     =  2004,
  url      = "https://www.sciencedirect.com/science/article/pii/S0031320303002759",
  keywords = "Shape; Image retrieval; CBIR; Review; Shape descriptor;image
              processing",
  issn     = "0031-3203",
  doi      = "10.1016/j.patcog.2003.07.008"
}

@MISC{Kerns1999-la,
  title        = "Guidelines for Head Lettuce Production in Arizona ({ACIS})",
  author       = "Kerns, D L and Matheron, M E and Palumbo, J C and Sanchez, C
                  A and Still, D W and Tickes, B R and Umeda, K and Wilcox, M A",
  abstract     = "Provided by the University of Arizona.",
  year         =  1999,
  url          = "https://cals.arizona.edu/crop/vegetables/cropmgt/az1099.html",
  howpublished = "\url{https://cals.arizona.edu/crop/vegetables/cropmgt/az1099.html}",
  note         = "Accessed: 2022-3-30"
}

@INCOLLECTION{Anguita2013-df,
  title     = "Training computationally efficient smartphone--based human
               activity recognition models",
  booktitle = "Artificial Neural Networks and Machine Learning -- {ICANN} 2013",
  author    = "Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra,
               Xavier and Reyes-Ortiz, Jorge Luis",
  abstract  = "This work presents a HAR system which incorporates
               smartphone-embedded inertial sensors and uses Support Vector
               Machines (SVM) for the classification of Activities of Daily
               Living (ADL) and explores two feature selection mechanisms for
               allowing a radically faster recognition. The exploitation of
               smartphones for Human Activity Recognition (HAR) has been an
               active research area in which the development of fast and
               efficient Machine Learning approaches is crucial for preserving
               battery life and reducing computational requirements. In this
               work, we present a HAR system which incorporates
               smartphone-embedded inertial sensors and uses Support Vector
               Machines (SVM) for the classification of Activities of Daily
               Living (ADL). By exploiting a publicly available benchmark HAR
               dataset, we show the benefits of adding smartphones gyroscope
               signals into the recognition system against the traditional
               accelerometer-based approach, and explore two feature selection
               mechanisms for allowing a radically faster recognition: the
               utilization of exclusively time domain features and the
               adaptation of the L1 SVM model which performs comparably to
               non-linear approaches while neglecting a large number of
               non-informative features.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "426--433",
  series    = "Lecture notes in computer science",
  year      =  2013,
  url       = "http://link.springer.com/10.1007/978-3-642-40728-4_54",
  address   = "Berlin, Heidelberg",
  keywords  = "INFO523",
  language  = "en",
  issn      = "0302-9743, 1611-3349",
  isbn      = "9783642407277, 9783642407284",
  doi       = "10.1007/978-3-642-40728-4\_54"
}

@ARTICLE{Upendar2021-mq,
  title    = "Greenness identification using visible spectral colour indices
              for site specific weed management",
  author   = "Upendar, K and Agrawal, K N and Chandel, N S and Singh, K",
  abstract = "In the present study an attempt has been made to identify the
              green vegetation based on colour using visible spectral colour
              indices such as excess green index (ExG), excess red index (ExR)
              and excess green minus excess red index (ExGR). At first stage,
              the performance of colour indices were tested at four
              illumination intensities using the standard colour patches. The
              results indicated a clear separation between the ExG, ExR and
              ExGR values of green colour patches (foliage, yellow green \&
              green) and soil colour patches (dark skin, moderate red \&
              magenta) at illumination intensity of 89.04 $\pm$ 8.12 lux than
              188.8 $\pm$ 6.36, 259.25 $\pm$ 12.73 and 359.28 $\pm$ 10.10 lux
              illumination intensities. This observation suggested that the
              colour indices might perform better at low lighting condition. In
              the second stage, the images of original plants and soil were
              captured at an illumination intensity of 89.04 $\pm$ 8.12 lux and
              classification rate at different threshold were studied. The
              average correct classification rate of ExGR and ExG colour
              indices were found to be 93.03\% and 86.03\% at threshold values
              0 and 10, respectively. This indicates that the colour index ExGR
              could be successfully employed for image based classification of
              plant and non-plant material.",
  journal  = "Plant Physiology Reports",
  volume   =  26,
  number   =  1,
  pages    = "179--187",
  month    =  mar,
  year     =  2021,
  url      = "https://doi.org/10.1007/s40502-020-00562-0",
  keywords = "image processing",
  issn     = "2662-2548",
  doi      = "10.1007/s40502-020-00562-0"
}

@ARTICLE{Gu2022-xo,
  title     = "A Survey on Deep Learning for Human Activity Recognition",
  author    = "Gu, F and Chung, M-H and Chignell, M and Valaee, S and Zhou, B
               and Liu, X",
  abstract  = "Human activity recognition is a key to a lot of applications
               such as healthcare and smart home. In this study, we provide a
               comprehensive survey on recent advances and challenges in human
               activity recognition (HAR) with deep learning. Although there
               are many surveys on HAR, they focused mainly on the taxonomy of
               HAR and reviewed the state-of-the-art HAR systems implemented
               with conventional machine learning methods. Recently, several
               works have also been done on reviewing studies that use deep
               models for HAR, whereas these works cover few deep models and
               their variants. There is still a need for a comprehensive and
               in-depth survey on HAR with recently developed deep learning
               methods. \copyright{} 2021 Association for Computing Machinery.",
  journal   = "ACM Computing Surveys",
  publisher = "Association for Computing Machinery",
  volume    =  54,
  number    =  8,
  year      =  2022,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116615099&doi=10.1145%2f3472290&partnerID=40&md5=2517311222a3af6809c9de660ab4d03c",
  keywords  = "activity recognition; Additional Key Words and PhrasesMachine
               learning; deep learning; deep models; mobile sensing;
               Automation; Deep learning; Intelligent buildings; Pattern
               recognition; Activity recognition; Additional key word and
               phrasesmachine learning; Deep learning; Deep model; Human
               activity recognition; Human activity recognition systems; Key
               words; Mobile sensing; Smart homes; State of the art;
               Surveys;INFO523",
  issn      = "0360-0300",
  doi       = "10.1145/3472290"
}

@PROCEEDINGS{Anguita2013-ts,
  title      = "A public domain dataset for human activity recognition using
                smartphones",
  author     = "Anguita, D and Ghio, A and Oneto, L and Parra, X and
                Reyes-Ortiz, J L",
  abstract   = "Human-centered computing is an emerging research field that
                aims to understand human behavior and integrate users and their
                social context with computer systems. One of the most recent,
                challenging and appealing applications in this framework
                consists in sensing human body motion using smartphones to
                gather context information about people actions. In this
                context, we describe in this work an Activity Recognition
                database, built from the recordings of 30 subjects doing
                Activities of Daily Living (ADL) while carrying a waist-mounted
                smartphone with embedded inertial sensors, which is released to
                public domain on a well-known on-line repository. Results,
                obtained on the dataset by exploiting a multiclass Support
                Vector Machine (SVM), are also acknowledged.",
  year       =  2013,
  url        = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887123513&partnerID=40&md5=8f6328ee377708d1b485b885d929cb00",
  keywords   = "Activities of Daily Living; Activity recognition; Context
                information; Human activity recognition; Human body motion;
                Human-centered computing; Multiclass support vector machines;
                Research fields; Neural networks; Pattern recognition; Support
                vector machines; Smartphones",
  conference = "21st European Symposium on Artificial Neural Networks,
                Computational Intelligence and Machine Learning, ESANN 2013",
  location   = "Bruges",
  isbn       = "9782874190810"
}

@UNPUBLISHED{McGinnis2021-bo,
  title       = "Parameter Selection in {Weed/Crop} Discrimination",
  author      = "McGinnis, Evan",
  journal     = "INFO 521: Introduction to Machine Learning",
  institution = "University of Arizona",
  year        =  2021
}

@ARTICLE{Nguyen2005-zm,
  title    = "Shape Analysis of Breast Masses in Mammograms via the Fractal
              Dimension",
  author   = "Nguyen, Thanh and Rangayyan, Rangaraj",
  abstract = "Masses due to benign breast diseases and tumors due to breast
              cancer present significantly different shapes on mammograms. In
              general, malignant tumors appear with rough and complex
              boundaries or contours, whereas benign masses present smooth,
              round, or oval contours. Fractal analysis may be used to derive
              shape features to perform pattern classification of breast masses
              and tumors. Several procedures have been proposed to compute the
              fractal dimension of various types of objects or regions of
              interest in biomedical images, among which the box-counting and
              ruler methods are popular. In this study, we applied the two
              methods mentioned above to compute the fractal dimension of both
              the two-dimensional (2D) contours of breast masses and tumors, as
              well as their one-dimensional (1D) signatures. A comparative
              analysis was performed to assess the performance of the two
              methods of computing the fractal dimension and the two methods of
              representing the boundaries of masses. It was observed that
              analysis of the 2D contour representation with the ruler method
              resulted in the highest classification accuracy of up to 0.946,
              as indicated by the area under the receiver operating
              characteristics (ROC) curve. The results indicate that the
              fractal dimension can serve as a good shape feature for the
              benign-versus-malignant classification of breast masses in
              mammograms.",
  journal  = "Conf. Proc. IEEE Eng. Med. Biol. Soc.",
  volume   =  2005,
  pages    = "3210--3213",
  year     =  2005,
  url      = "http://dx.doi.org/10.1109/IEMBS.2005.1617159",
  keywords = "shape analysis",
  language = "en",
  issn     = "1557-170X",
  pmid     = "17282928",
  doi      = "10.1109/IEMBS.2005.1617159"
}

@ARTICLE{Shen1994-eg,
  title    = "Application of shape analysis to mammographic calcifications",
  author   = "Shen, L and Rangayyan, R M and Desautels, J L",
  abstract = "The authors have developed a set of shape factors to measure the
              roughness of contours of calcifications in mammograms and for use
              in their classification as malignant or benign. The analysis of
              mammograms is performed in three stages. First, a region growing
              technique is used to obtain the contours of calcifications. Then,
              three measures of shape features, including compactness, moments,
              and Fourier descriptors are computed for each region. Finally,
              their applicability for classification is studied by using the
              three shape measures to form feature vectors. Classification of
              143 calcifications from 18 biopsy-proven cases as benign or
              malignant using the three measures with the nearest-neighbor
              method was 100\% accurate.",
  journal  = "IEEE Trans. Med. Imaging",
  volume   =  13,
  number   =  2,
  pages    = "263--274",
  year     =  1994,
  url      = "http://dx.doi.org/10.1109/42.293919",
  keywords = "shape analysis",
  language = "en",
  issn     = "0278-0062",
  pmid     = "18218503",
  doi      = "10.1109/42.293919"
}

@MISC{noauthor_undated-gj,
  title        = "Color Spaces",
  url          = "https://culorijs.org/color-spaces/",
  howpublished = "\url{https://culorijs.org/color-spaces/}",
  note         = "Accessed: 2023-4-20"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Berrar2019-ol,
  title     = "Performance Measures for Binary Classification",
  booktitle = "Encyclopedia of Bioinformatics and Computational Biology",
  author    = "Berrar, Daniel",
  editor    = "Ranganathan, Shoba and Gribskov, Michael and Nakai, Kenta and
               Sch{\"o}nbach, Christian",
  abstract  = "This article is an introduction to some of the most commonly
               used performance measures for the evaluation of binary
               classifiers. These measures are categorized into three broad
               families: measures based on a single classification threshold,
               measures based on a probabilistic interpretation of error, and
               ranking measures. Graphical methods, such as ROC curves,
               precision-recall curves, TPR-FPR plots, gain charts, and lift
               charts, are also discussed. Using a simple example, we
               illustrate how to calculate the various performance measures and
               show how they are related. The article also explains how to
               assess the statistical significance of an obtained performance
               value, how to calculate approximate and exact parametric
               confidence intervals, and how to derive percentile bootstrap
               confidence intervals for a performance measure.",
  publisher = "Academic Press",
  pages     = "546--560",
  month     =  jan,
  year      =  2019,
  url       = "https://www.sciencedirect.com/science/article/pii/B9780128096338203518",
  address   = "Oxford",
  keywords  = "Accuracy; AUC; AUCH; AUCPR; Average precision; Balanced
               accuracy; Bootstrap percentile confidence interval; Brier score;
               Clopper-Pearson confidence interval; Cohen׳s kappa; Confusion
               matrix; Cross-entropy; Error rate; -measure; False discovery
               rate; -measure; Gain chart; Gini index; -measure; Information
               score; Kolmogorv-Smirnov statistic; Lift; Lift chart; Logloss;
               MAP; Matthew׳s correlation coefficient; Mean absolute error;
               Mean squared error; Negative likelihood ratio; Negative
               predictive value; Positive likelihood ratio; Positive predictive
               value; Precision; Precision-recall plot; Recall; ROC; Scoring
               function; Sensitivity; Specificity; TaKS; TPR-FPR plot;
               Trapezoidal estimators; True negative rate; True positive rate;
               Type I error; Type II error; Wald confidence interval; Youden
               index",
  isbn      = "9780128114322",
  doi       = "10.1016/B978-0-12-809633-8.20351-8"
}

@INCOLLECTION{Forsyth2012-tv,
  title     = "Chapter 3 - Color",
  booktitle = "Computer Vision -- A Modern Approach",
  author    = "Forsyth, David A and Ponce, Jean",
  publisher = "Pearson Education",
  pages     = "68--103",
  year      =  2012,
  isbn      = "9780136085928"
}

@INCOLLECTION{Titus2022-yl,
  title     = "Comparative Analysis of Local Binary Descriptors for Plant
               Discrimination",
  booktitle = "International Conference on Ubiquitous Intelligent Systems,
               {ICUIS} 2021",
  author    = "Titus, R M and Stephen, R and Vimina, E R",
  editor    = "{Karuppusamy P.} and {Perikos I.} and {Garcia Marquez F.P.}",
  abstract  = "Weed management is one of the prime obstacles faced by most of
               farmers nowadays. Efficient weed detection methods will cut back
               the price of weed management. Feature extractors have an
               important role in the domain of computer vision. The feature
               extracting algorithm takes the image as its input, and then it
               gives back the feature descriptors of the image that can be used
               to discriminate one feature from another. In software systems,
               there are various binary descriptors that are widely used for
               face recognition, plant discrimination, fingerprint detection,
               etc. This paper shows the performance comparison of different
               binary descriptors like local directional relation pattern
               (LDRP), local directional order pattern (LDOP), and local binary
               pattern (LBP) with support vector machine (SVM) for the image
               set classification. The results indicate that the sequence of
               LBP and SVM together produce a better accuracy of 84.51\% in
               ``bccr-segset'' plant leaf database when compared to LDOP which
               produced an accuracy of 75\% and LDRP with an accuracy of
               75.56\%. \copyright{} 2022, The Author(s), under exclusive
               license to Springer Nature Singapore Pte Ltd.",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  volume    =  243,
  pages     = "295--305",
  year      =  2022,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118133941&doi=10.1007%2f978-981-16-3675-2_22&partnerID=40&md5=da8801270ebb062f6deddfbf8a0f3c86",
  keywords  = "Classification; Feature extraction; LBP; LDOP; LDRP; Plant
               discrimination; SVM; Classification (of information); Face
               recognition; Comparative analyzes; Descriptors; Features
               extraction; Local binary patterns; Local directional order
               pattern; Local directional relation pattern; Order patterns;
               Plant discrimination; Support vectors machine; Weed management;
               Support vector machines;LBP;texture",
  isbn      = "9789811636745",
  doi       = "10.1007/978-981-16-3675-2\_22"
}

@ARTICLE{Kupidura2019-kq,
  title     = "The comparison of different methods of texture analysis for
               their efficacy for land use classification in satellite imagery",
  author    = "Kupidura, P",
  abstract  = "The paper presents a comparison of the efficacy of several
               texture analysis methods as tools for improving land use/cover
               classification in satellite imagery. The tested methods were:
               gray level co-occurrence matrix (GLCM) features, Laplace filters
               and granulometric analysis, based on mathematical morphology.
               The performed tests included an assessment of the classification
               accuracy performed based on spectro-textural datasets: spectral
               images with the addition of images generated using different
               texture analysis methods. The class nomenclature was based on
               spectral and textural differences and included the following
               classes: water, low vegetation, bare soil, urban, and two
               (coniferous and deciduous) forest classes. The classification
               accuracy was assessed using the overall accuracy and kappa index
               of agreement, based on the reference data generated using visual
               interpretation of the images. The analysis was performed using
               very high-resolution imagery (Pleiades, WorldView-2) and
               high-resolution imagery (Sentinel-2). The results show the
               efficacy of selected GLCM features and granulometric analysis as
               tools for providing textural data, which could be used in the
               process of land use/cover classification. It is also clear that
               texture analysis is generally a more important and effective
               component of classification for images of higher resolution. In
               addition, for classification using GLCM results, the Random
               Forest variable importance analysis was performed. \copyright{}
               2019 by the authors.",
  journal   = "Remote Sensing",
  publisher = "MDPI AG",
  volume    =  11,
  number    =  10,
  year      =  2019,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066761119&doi=10.3390%2frs11101233&partnerID=40&md5=db85c5e0c3b247c45397f74696b2ff49",
  keywords  = "Classification; GLCM; Granulometric analysis; Laplace filter;
               Mathematical morphology; Satellite imagery; Texture analysis;
               Classification (of information); Decision trees; Image analysis;
               Image enhancement; Image texture; Land use; Laplace transforms;
               Mathematical morphology; Morphology; Satellite imagery;
               Spectroscopy; Textures; Classification accuracy; GLCM;
               Granulometric analysis; Gray level co occurrence matrix(GLCM);
               High resolution imagery; Kappa index of agreements; Laplace
               filter; Texture analysis; Image classification;texture",
  issn      = "2072-4292",
  doi       = "10.3390/rs11101233"
}

@BOOK{Forsyth2012-hy,
  title     = "Computer Vision: A Modern Approach",
  author    = "Forsyth, David and Ponce, Jean",
  abstract  = "Computer Vision: A Modern Approach, 2e, is appropriate for
               upper-division undergraduate- and graduate-level courses in
               computer vision found in departments of Computer Science,
               Computer Engineering and Electrical Engineering. This textbook
               provides the most complete treatment of modern computer vision
               methods by two of the leading authorities in the field. This
               accessible presentation gives both a general view of the entire
               computer vision enterprise and also offers sufficient detail for
               students to be able to build useful applications. Students will
               learn techniques that have proven to be useful by first-hand
               experience and a wide range of mathematical methods",
  publisher = "Pearson",
  year      =  2012,
  url       = "https://play.google.com/store/books/details?id=gM63QQAACAAJ",
  language  = "en",
  isbn      = "9780136085928"
}

@MISC{MathWorks_undated-jg,
  title        = "Conversion of {RGB} colorspace",
  booktitle    = "Image Processing Toolbox Documentation. {MathWorks}.",
  author       = "{MathWorks}",
  abstract     = "This MATLAB function converts the red, green, and blue values
                  of an RGB image to luminance (Y) and chrominance (I and Q)
                  values of an NTSC image.",
  url          = "https://www.mathworks.com/help/images/ref/rgb2ntsc.html",
  howpublished = "\url{https://www.mathworks.com/help/images/ref/rgb2ntsc.html}",
  note         = "Accessed: 2023-4-20",
  keywords     = "color",
  language     = "en"
}

@ARTICLE{Hall-Beyer2017-fn,
  title     = "{GLCM} Texture: A Tutorial v. 3.0 March 2017",
  author    = "Hall-Beyer, Mryka",
  publisher = "Previously available at
               http://www.ucalgary.ca/UofC/nasdev/mhallbey/research.htm",
  month     =  mar,
  year      =  2017,
  url       = "http://hdl.handle.net/1880/51900",
  keywords  = "remote sensing; spatial descriptors; spatial statistics;
               texture; GLCM; educational resource",
  language  = "en"
}

@ARTICLE{Haralick1979-yh,
  title    = "Statistical and structural approaches to texture",
  author   = "Haralick, R M",
  abstract = "In this survey we review the image processing literature on the
              various approaches and models investigators have used for
              texture. These include statistical approaches of autocorrelation
              function, optical transforms, digital transforms, textural
              edgeness, structural element, gray tone cooccurrence, run
              lengths, and autoregressive models. We discuss and generalize
              some structural approaches to texture based on more complex
              primitives than gray tone. We conclude with some
              structural-statistical generalizations which apply the
              statistical techniques to the structural primitives.",
  journal  = "Proc. IEEE",
  volume   =  67,
  number   =  5,
  pages    = "786--804",
  month    =  may,
  year     =  1979,
  url      = "http://dx.doi.org/10.1109/PROC.1979.11328",
  keywords = "Image texture;Image analysis;Biomedical optical imaging;Optical
              sensors;Satellites;Remote sensing;Microscopy;Data
              mining;Biomedical measurements;Shape;glcm",
  issn     = "1558-2256",
  doi      = "10.1109/PROC.1979.11328"
}

@ARTICLE{Fragassa2023-cj,
  title     = "A New Procedure for Combining {UAV-Based} Imagery and Machine
               Learning in Precision Agriculture",
  author    = "Fragassa, C and Vitali, G and Emmi, L and Arru, M",
  abstract  = "Drone images from an experimental field cropped with sugar beet
               with a high diffusion of weeds taken from different flying
               altitudes were used to develop and test a machine learning
               method for vegetation patch identification. Georeferenced images
               were combined with a hue-based preprocessing analysis, digital
               transformation by an image embedder, and evaluation by
               supervised learning. Specifically, six of the most common
               machine learning algorithms were applied (i.e., logistic
               regression, k-nearest neighbors, decision tree, random forest,
               neural network, and support-vector machine). The proposed method
               was able to precisely recognize crops and weeds throughout a
               wide cultivation field, training from single partial images. The
               information has been designed to be easily integrated into
               autonomous weed management systems with the aim of reducing the
               use of water, nutrients, and herbicides for precision
               agriculture. \copyright{} 2023 by the authors.",
  journal   = "Sustainability (Switzerland)",
  publisher = "MDPI",
  volume    =  15,
  number    =  2,
  year      =  2023,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146608421&doi=10.3390%2fsu15020998&partnerID=40&md5=e713df2c96e5f65c3c681331b39da059",
  keywords  = "agricultural robotics; environmental sustainability; image
               analysis; machine learning; precision agriculture; sugar beet;
               unmanned aerial vehicle (UAV); weeding; image analysis; machine
               learning; precision agriculture; robotics; satellite imagery;
               sugar beet; sustainability; unmanned vehicle; weed control;drone",
  issn      = "2071-1050",
  doi       = "10.3390/su15020998"
}

@UNPUBLISHED{Hall-Beyer2017-nx,
  title    = "{GLCM} {TEXTURE}: A {TUTORIAL}",
  author   = "Hall-Beyer, Mryka",
  month    =  mar,
  year     =  2017,
  url      = "http://lapi.fi-p.unam.mx/wp-content/uploads/Tutorial-GLCM.pdf",
  keywords = "glcm"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Saile2022-vu,
  title     = "Evaluating {Sensor-Based} Mechanical Weeding Combined with
               Pre-and {Post-Emergence} Herbicides for Integrated Weed
               Management in Cereals",
  author    = "Saile, M and Spaeth, M and Gerhards, R",
  abstract  = "Due to the increasing number of herbicide-resistant weed
               populations and the resulting yield losses, weed control must be
               given high priority to ensure food security. Integrated weed
               management (IWM) strategies, including reduced herbicide
               application, sensor-guided mechanical weed control and
               combinations thereof are indispensable to achieve this goal.
               Therefore, this study examined combinations of pre-and
               post-emergence herbicide applications with sensor-based
               harrowing and hoeing in cereals by conducting five field
               experiments at two locations in Southwestern Germany from 2019
               to 2021. Each experiment contained an untreated control and a
               single post-emergence herbicide treatment as a comparison to
               these IWM treatments. The effects of the different IWM
               approaches on weed control efficacy (WCE), crop density, and
               grain yield were recorded. All experiments were set up in a
               randomized complete block design with four repetitions.
               Pre-emergence herbicide application combined with one-time
               harrowing and subsequent hoeing (Pre-Herb + Harr + Hoe) achieved
               the highest WCE (100\%), followed by an approach of WCE (95\%)
               for two-times hoeing. In contrast, a single pre-emergence
               herbicide application achieved the worst result with an average
               WCE of 25\%. Grain yield was equal between all treatments in
               between 6 t ha−1 and 10 t ha−1, except for a single
               pre-emergence herbicide application, which achieved a 2.5 t ha−1
               higher grain yield in winter wheat in 2021 that averaged 11 t
               ha−1, compared to the combination of Pre-Herb + Harr + Hoe that
               averaged 8.5 t ha−1. The results showed that it is possible to
               reduce and replace herbicides while achieving equivalent yield
               and WCE. \copyright{} 2022 by the authors. Licensee MDPI, Basel,
               Switzerland.",
  journal   = "Agronomy",
  publisher = "MDPI",
  volume    =  12,
  number    =  6,
  year      =  2022,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132769920&doi=10.3390%2fagronomy12061465&partnerID=40&md5=842a2f6f47233d0f4b0c742836062f59",
  keywords  = "camera-guided; digital farming; herbicide reduction; integrated
               weed management; precision farming; site-specific;treatment",
  issn      = "2073-4395",
  doi       = "10.3390/agronomy12061465"
}

@ARTICLE{Mwitta2022-yt,
  title     = "Evaluation of Diode Laser Treatments to Manage Weeds in Row
               Crops",
  author    = "Mwitta, C and Rains, G C and Prostko, E",
  abstract  = "Herbicides have been the primary weed management practice in
               agriculture for decades. However, due to their effects on the
               environment in addition to weeds becoming resistant, alternative
               approaches to weed control are critical. One approach is using
               lasers, particularly diode lasers because of their portability,
               low power demand, and cost effectiveness. In this research,
               weeds' response to diode laser treatments was investigated.
               Three experiments were conducted. The first experiment involved
               treating two species of weeds with four different laser powers
               to determine the time it takes to sever the weed stem. The
               second experiment involved monitoring the status of two species
               of weeds for a week after treating them with two lasers at
               constant application times of 1 s, 2 s, and 3 s. The third
               experiment was a repeat of the second with higher laser powers
               and shorter treatment times. The results showed diode lasers
               have a potential to be an effective weed controlling tool. Weed
               stem diameter, laser power, treatment duration, and distance
               between laser and weed were all statistically significant in
               weed mortality, with weed species having no significance.
               Furthermore, it was found that weed management is possible by
               exposing the stem of the two weed species between 0.8 and 2.65
               mm diameter to a laser beam dosage without necessarily severing
               it, with 80\% effectiveness at 0.5 s treatment time, and 100\%
               effectiveness using a 6.1 W laser for 1.5 s. \copyright{} 2022
               by the authors.",
  journal   = "Agronomy",
  publisher = "MDPI",
  volume    =  12,
  number    =  11,
  year      =  2022,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141582446&doi=10.3390%2fagronomy12112681&partnerID=40&md5=73754c5cfdaa68b457a599abb3adf2c4",
  keywords  = "laser weeding; non-chemical weed elimination; precision
               weeding;treatment",
  issn      = "2073-4395",
  doi       = "10.3390/agronomy12112681"
}

@ARTICLE{Gai2020-zh,
  title     = "Automated crop plant detection based on the fusion of color and
               depth images for robotic weed control",
  author    = "Gai, J and Tang, L and Steward, B L",
  abstract  = "Robotic weeding enables weed control near or within crop rows
               automatically, precisely and effectively. A computer-vision
               system was developed for detecting crop plants at different
               growth stages for robotic weed control. Fusion of color images
               and depth images was investigated as a means of enhancing the
               detection accuracy of crop plants under conditions of high weed
               population. In-field images of broccoli and lettuce were
               acquired 3--27 days after transplanting with a Kinect v2 sensor.
               The image processing pipeline included data preprocessing,
               vegetation pixel segmentation, plant extraction, feature
               extraction, feature-based localization refinement, and crop
               plant classification. For the detection of broccoli and lettuce,
               the color-depth fusion algorithm produced high true-positive
               detection rates (91.7\% and 90.8\%, respectively) and low
               average false discovery rates (1.1\% and 4.0\%, respectively).
               Mean absolute localization errors of the crop plant stems were
               26.8 and 7.4 mm for broccoli and lettuce, respectively. The
               fusion of color and depth was proved beneficial to the
               segmentation of crop plants from background, which improved the
               average segmentation success rates from 87.2\% (depth-based) and
               76.4\% (color-based) to 96.6\% for broccoli, and from 74.2\%
               (depth-based) and 81.2\% (color-based) to 92.4\% for lettuce,
               respectively. The fusion-based algorithm had reduced performance
               in detecting crop plants at early growth stages. \copyright{}
               2019 Wiley Periodicals, Inc.",
  journal   = "Journal of Field Robotics",
  publisher = "John Wiley and Sons Inc.",
  volume    =  37,
  number    =  1,
  pages     = "35--52",
  year      =  2020,
  url       = "http://dx.doi.org/10.1002/rob.21897",
  keywords  = "computer vision; crop detection; robotic weeding; sensor fusion;
               Color; Computer control systems; Computer vision; Data handling;
               Extraction; Image enhancement; Image fusion; Image segmentation;
               Robotics; Weed control; Color and depth images; Computer vision
               system; Different growth stages; False discovery rate; Image
               processing pipeline; Localization errors; Robotic weeding;
               Sensor fusion; Crops",
  issn      = "1556-4959",
  doi       = "10.1002/rob.21897"
}

@ARTICLE{Hasan2021-gx,
  title     = "A survey of deep learning techniques for weed detection from
               images",
  author    = "Hasan, A S M M and Sohel, F and Diepeveen, D and Laga, H and
               Jones, M G K",
  abstract  = "The rapid advances in Deep Learning (DL) techniques have enabled
               rapid detection, localisation, and recognition of objects from
               images or videos. DL techniques are now being used in many
               applications related to agriculture and farming. Automatic
               detection and classification of weeds can play an important role
               in weed management and so contribute to higher yields. Weed
               detection in crops from imagery is inherently a challenging
               problem because both weeds and crops have similar colours
               (`green-on-green'), and their shapes and texture can be very
               similar at the growth phase. Also, a crop in one setting can be
               considered a weed in another. In addition to their detection,
               the recognition of specific weed species is essential so that
               targeted controlling mechanisms (e.g. appropriate herbicides and
               correct doses) can be applied. In this paper, we review existing
               deep learning-based weed detection and classification
               techniques. We cover the detailed literature on four main
               procedures, i.e., data acquisition, dataset preparation, DL
               techniques employed for detection, location and classification
               of weeds in crops, and evaluation metrics approaches. We found
               that most studies applied supervised learning techniques, they
               achieved high classification accuracy by fine-tuning pre-trained
               models on any plant dataset, and past experiments have already
               achieved high accuracy when a large amount of labelled data is
               available. \copyright{} 2021 Elsevier B.V.",
  journal   = "Comput. Electron. Agric.",
  publisher = "Elsevier B.V.",
  volume    =  184,
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102421820&doi=10.1016%2fj.compag.2021.106067&partnerID=40&md5=0c28404b95433664c39285bac24c8976",
  keywords  = "Deep learning; Digital agriculture; Machine learning; Weed
               classification; Weed detection; Crops; Data acquisition; Deep
               learning; Large dataset; Learning algorithms; Object detection;
               Supervised learning; Textures; Weed control; Automatic
               classification; Deep learning; Digital agriculture; Learning
               techniques; Localisation; Machine-learning; Rapid detection;
               Recognition of objects; Weed classification; Weed detection;
               Classification (of information); accuracy assessment; data
               acquisition; data set; image classification; numerical model;
               satellite imagery; supervised learning",
  issn      = "0168-1699",
  doi       = "10.1016/j.compag.2021.106067"
}

@ARTICLE{Ong2023-lm,
  title     = "{UAV-based} weed detection in Chinese cabbage using deep
               learning",
  author    = "Ong, P and Teo, K S and Sia, C K",
  abstract  = "Weeds are unwanted plants on agricultural soil. They always
               competing for sunlight, nutrient, space and water with economic
               crops. Uncontrolled weed growth can cause both significant
               economic and ecological loss. Hence, weeds should be efficiently
               differentiated from the crops for the smart spraying solution.
               In this study, the Convolutional Neural Network (CNN) was used
               to perform weed detection amongst the commercial crop of Chinese
               cabbage, using the acquired images by Unmanned Aerial Vehicles.
               The acquired images were pre-processed and subsequently
               segmented into the crop, soil, and weed classes using the Simple
               Linear Iterative Clustering Superpixel algorithm. The segmented
               images were then used to construct the CNN-based classifier. The
               Random Forest (RF) was applied to compare with the performance
               of CNN. The results showed that the CNN achieved a higher
               overall accuracy of 92.41\% than the 86.18\% attained by RF.
               \copyright{} 2023 The Author(s)",
  journal   = "Smart Agricultural Technology",
  publisher = "Elsevier B.V.",
  volume    =  4,
  year      =  2023,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146181058&doi=10.1016%2fj.atech.2023.100181&partnerID=40&md5=fba5675f6430837fd200af789f5109dd",
  keywords  = "Chinese cabbage; Convolutional neural network; Deep learning;
               Random forest; Weed detection;drone;uav",
  issn      = "2772-3755",
  doi       = "10.1016/j.atech.2023.100181"
}

@ARTICLE{Tekin2020-ww,
  title     = "New local binary pattern approaches based on color channels in
               texture classification",
  author    = "Tekin, Ramazan and Ertu{\u g}rul, {\"O}mer Faruk and Kaya,
               Y{\i}lmaz",
  abstract  = "In this paper, four novel, simple and robust approaches, which
               are left to right local binary patterns ( LBPL L2R ), top to
               down local binary patterns ( LBP T2D ), cube surface local
               binary pattern ( LBP Surfaces ), and cube diagonal local binary
               pattern ( LBP Diagonal ), were proposed in order to exact
               texture features in color images. These approaches were based on
               the local binary pattern (LBP), which is an effective
               statistical texture descriptor and can be employed in gray
               images. Proposed approaches were evaluated and validated in four
               datasets, which are Outex, KTH\_TIPS, KTH\_TIPS2, and USPtex
               datasets. The images in these datasets are in RGB, HSV, YIQ, and
               YCbCr color formats. Achieved results by these approaches were
               compared with the obtained results by the classical LBP and
               literature findings. As a result, the proposed approaches
               performed better than the traditional LBP method and they found
               effective in the classification of color texture images,
               especially in images, which are in RGB and HSV formats.
               Furthermore, noise robustness and time complexity of the
               proposed approaches were validated.",
  journal   = "Multimed. Tools Appl.",
  publisher = "Springer US",
  volume    =  79,
  number    = "43-44",
  pages     = "32541--32561",
  month     =  nov,
  year      =  2020,
  url       = "http://dx.doi.org/10.1007/s11042-020-09698-5",
  address   = "New York",
  keywords  = "Computer Communication Networks;Computer Science;Data Structures
               and Information Theory;Multimedia Information Systems;Special
               Purpose and Application-Based Systems",
  issn      = "1380-7501, 1573-7721",
  doi       = "10.1007/s11042-020-09698-5"
}

@ARTICLE{Tekin2020-ie,
  title    = "New local binary pattern approaches based on color channels in
              texture classification",
  author   = "Tekin, Ramazan and Ertu{\u g}rul, {\"O}mer Faruk and Kaya,
              Y{\i}lmaz",
  abstract = "In this paper, four novel, simple and robust approaches, which
              are left to right local binary patterns (LBPLL2R), top to down
              local binary patterns (LBPT2D), cube surface local binary pattern
              (LBPSurfaces), and cube diagonal local binary pattern
              (LBPDiagonal), were proposed in order to exact texture features
              in color images. These approaches were based on the local binary
              pattern (LBP), which is an effective statistical texture
              descriptor and can be employed in gray images. Proposed
              approaches were evaluated and validated in four datasets, which
              are Outex, KTH\_TIPS, KTH\_TIPS2, and USPtex datasets. The images
              in these datasets are in RGB, HSV, YIQ, and YCbCr color formats.
              Achieved results by these approaches were compared with the
              obtained results by the classical LBP and literature findings. As
              a result, the proposed approaches performed better than the
              traditional LBP method and they found effective in the
              classification of color texture images, especially in images,
              which are in RGB and HSV formats. Furthermore, noise robustness
              and time complexity of the proposed approaches were validated.",
  journal  = "Multimed. Tools Appl.",
  volume   =  79,
  number   =  43,
  pages    = "32541--32561",
  month    =  nov,
  year     =  2020,
  url      = "https://doi.org/10.1007/s11042-020-09698-5",
  keywords = "LBP",
  issn     = "1380-7501, 1573-7721",
  doi      = "10.1007/s11042-020-09698-5"
}

@UNPUBLISHED{Clynch2002-lc,
  title  = "Coordinates",
  author = "Clynch, James R",
  year   =  2002,
  url    = "https://www.oc.nps.edu/oc2902w/coord/coord.pdf"
}

@ARTICLE{Abouzahir2021-sd,
  title     = "Bag-of-visual-words-augmented Histogram of Oriented Gradients
               for efficient weed detection",
  author    = "Abouzahir, S and Sadik, M and Sabir, E",
  abstract  = "As season-long weeds competition produces important yield
               losses, early detection of these plants is essential to sustain
               productivity. Machine vision as a non-destructive surveying
               technique requires features that can describe weeds in a real
               field case. Colours and shapes provide good results in
               controlled conditions. However, when different crops or weeds
               appear in clusters, such solutions fail to meet satisfactory
               performance. Therefore, considering features that are less
               specific to field conditions is crucial for integrated weed
               management. In this study, we provide effective use of the
               Histogram of Oriented Gradients (HOG) to improve its performance
               for weed detection. The concept is based on the
               Bag-of-Visual-Words (BOVW) approach. We use the HOG blocks as
               keypoints to generate the visual-words, and the features vectors
               are the histograms of these visual-words. Next, we use the
               Backpropagation Neural Network to detect weeds and classify
               plants for three different crop fields. Namely, we consider
               sugar-beet, soybean, and carrot as target crops. Results
               demonstrate that the proposed weed detection system can locate
               weeds for site-specific treatment and selective spraying of
               herbicides. The proposed BOVW-based HOG can discriminate between
               weeds and crops with an accuracy of 97.7\%, 93\%, and 96.6\% in
               sugar-beet, carrot and soybean fields respectively. For plant
               classification, our method can classify plants with an accuracy
               of 90.4\%, 92.4\%, and 94.1\% in sugar-beet, carrot and soybean
               fields respectively. Our results turn out 37.6\% better than the
               classical HOG that produces an accuracy ranging from 71.2\% to
               83.3\% in weed detection and 49.1\%--82.1\% in plant
               classification. \copyright{} 2020 IAgrE",
  journal   = "Biosystems Eng.",
  publisher = "Academic Press",
  volume    =  202,
  pages     = "179--194",
  year      =  2021,
  url       = "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099185136&doi=10.1016%2fj.biosystemseng.2020.11.005&partnerID=40&md5=ea5a92351214e43e8e2517ee32913145",
  keywords  = "Bag of visual words; Computer vision; Histogram of oriented
               gradients; Neural Network; Weed detection; Backpropagation;
               Crops; Graphic methods; Sugar beets; Back propagation neural
               networks; Bag-of-visual-words; Controlled conditions; Field
               conditions; Histogram of oriented gradients; Histogram of
               oriented gradients (HOG); Non destructive; Plant classification;
               Weed control;hog",
  issn      = "1537-5110",
  doi       = "10.1016/j.biosystemseng.2020.11.005"
}

@ARTICLE{Bossu2009-yf,
  title    = "Wavelet transform to discriminate between crop and weed in
              perspective agronomic images",
  author   = "Bossu, J and G{\'e}e, Ch and Jones, G and Truchetet, F",
  abstract = "We proposed testing and validating the accuracy of four image
              processing algorithms (wavelet transforms and Gabor filtering)
              for crop/weed discrimination in synthetic and real images. A
              large panel of wavelet bases (33) was tested and the two best
              wavelets and the worst one were selected for detailed study.
              Based on a confusion matrix the crop/weed classification results
              of wavelet transforms were compared to the results of Gabor
              filtering that was initially chosen to develop a machine vision
              system for a real-time precision sprayer. The accuracy of these
              algorithms was compared and showed that wavelets were well
              adapted for perspective images: the best results were with
              Daubechies 25 and discrete approximation Meyer wavelets. They
              provided better results than Gabor filtering not only for
              crop/weed classification but also in processing time.",
  journal  = "Comput. Electron. Agric.",
  volume   =  65,
  number   =  1,
  pages    = "133--143",
  month    =  jan,
  year     =  2009,
  url      = "https://www.sciencedirect.com/science/article/pii/S0168169908001981",
  keywords = "Image processing; Wavelet transform; Gabor filtering; Weed
              infestation rate; Synthetic images;gabor;weed discrimination",
  issn     = "0168-1699",
  doi      = "10.1016/j.compag.2008.08.004"
}

@ARTICLE{Haryanto2020-gw,
  title     = "{Multipatch-GLCM} for Texture Feature Extraction on
               Classification of the Colon Histopathology Images using Deep
               Neural Network with {GPU} Acceleration",
  author    = "Haryanto, Toto and Pratama, Adib and Suhartanto, Heru and Murni,
               Aniati and Pidanic, Jan",
  abstract  = "Cancer is one of the leading causes of death in the world. It is
               the main reason why research in this field becomes challenging.
               Not only for the pathologist but also from the view of a
               computer scientist. Hematoxylin and Eosin (H\&E) images are the
               most common modalities used by the pathologist for cancer
               detection. The status of cancer with histopathology images can
               be classified based on the shape, morphology, intensity, and
               texture of the image. The use of full high-resolution
               histopathology images will take a longer time for the extraction
               of all information due to the huge amount of data. This study
               proposed advance texture extraction by multi-patch images pixel
               method with sliding windows that minimize loss of information in
               each pixel patch. We use texture feature Gray Level
               Co-Occurrence Matrix (GLCM) with a meanshift filter as the data
               pre-processing of the images. The mean-shift filter is a
               low-pass filter technique that considers the surrounding pixels
               of the images. The proposed GLCM method is then trained using
               Deep Neural Networks (DNN) and compared to other classification
               techniques for benchmarking. For training, we use two hardware:
               NVIDIA GPU GTX-980 and TESLA K40c. According to the study, Deep
               Neural Network outperforms other classifiers with the highest
               accuracy and deviation standard 96.72$\pm$0.48 for four
               cross-validations. The additional information is that training
               using Theano framework is faster than Tensorflow for both in
               GTX-980 and Tesla K40c",
  journal   = "Journal of Computer Science",
  publisher = "Science Publications",
  volume    =  16,
  number    =  3,
  pages     = "280--294",
  month     =  mar,
  year      =  2020,
  url       = "https://www.researchgate.net/publication/340484256_Multipatch-GLCM_for_Texture_Feature_Extraction_on_Classification_of_the_Colon_Histopathology_Images_using_Deep_Neural_Network_with_GPU_Acceleration",
  keywords  = "glcm",
  issn      = "1549-3636",
  doi       = "10.3844/jcssp.2020.280.294"
}
