%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%     Declarations (skip to Begin Document, line 88, for parts you fill in)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[10pt]{article}
%%\documentclass[10pt]{report}
\documentclass[letterpaper]{article}
\usepackage{geometry}
%\usepackage{xcolor}
\usepackage[table]{xcolor}
\usepackage{amsmath}
\usepackage[some]{background}
%\usepackage{lipsum}
%\usepackage{natbib}
\usepackage{siunitx}

% C I T A T I O N S
\usepackage[backend=biber, style=apa]{biblatex}  
%\usepackage{biblatex} 
\addbibresource{paperpile.bib}

\usepackage[hidelinks]{hyperref}

% C A P T I O N S
%\usepackage{caption, copyrightbox}
%\captionsetup{justification=centering, labelfont=sc, labelsep=endash}

% Tables
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{longtable}

\usepackage{diagbox} %table split headers
\usepackage{longtable}
\usepackage{array}
\usepackage{rotating}
\usepackage{eqparbox}
\usepackage{makecell, caption, booktabs}
\usepackage{wrapfig}
\usepackage{colortbl}

% Stuff needed to get table to span pages
\usepackage{enumitem}
%\usepackage{array, booktabs, longtable}
\newcolumntype{x}[1]{>{\raggedright}p{#1}}
% Highlight a column
\definecolor{Highlight}{HTML}{FFF59C}
\newcolumntype{h}{>{\columncolor{Highlight}}c}


\usepackage{etoolbox}
\AtBeginEnvironment{longtable}{%
    \setlist[itemize]{nosep,     % <-- new list setup
                      topsep     = 0pt       ,
                      partopsep  = 0pt       ,
                      leftmargin = *         ,
                      label      = $\bullet$ ,
                      before     = \vspace{-\baselineskip},
                      after      = \vspace{-0.5\baselineskip}
                        }
                           }% end of AtBeginEnvironment
% End table span
% Listings
\usepackage{listings}

\usepackage{geometry}  % Lots of layout options.  See http://en.wikibooks.org/wiki/LaTeX/Page_Layout
\geometry{letterpaper}  % ... or a4paper or a5paper or ... 
\usepackage{fullpage}  % somewhat standardized smaller margins (around an inch)
\usepackage{setspace}  % control line spacing in latex documents
\usepackage[parfill]{parskip}  % Activate to begin paragraphs with an empty line rather than an indent

\usepackage{amsmath,amssymb}  % latex math
\usepackage{empheq} % http://www.ctan.org/pkg/empheq
\usepackage{bm,upgreek}  % allows you to write bold greek letters (upper & lower case)

% for typsetting algorithm pseudocode see http://en.wikibooks.org/wiki/LaTeX/Algorithms_and_Pseudocode
\usepackage{algorithmic,algorithm}  

\usepackage{graphicx}  % inclusion of graphics; see: http://en.wikibooks.org/wiki/LaTeX/Importing_Graphics
% allow easy inclusion of .tif, .png graphics
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% \usepackage{subfigure}  % allows subfigures in figure
\usepackage{caption}
\usepackage{subcaption}

\usepackage{xspace}
\newcommand{\latex}{\LaTeX\xspace}

\usepackage{color}  % http://en.wikibooks.org/wiki/LaTeX/Colors

\long\def\todo#1{{\color{red}{\bf TODO: #1}}}

\long\def\ans#1{{\color{blue}{\em #1}}}
\long\def\ansnem#1{{\color{blue}#1}}
\long\def\boldred#1{{\color{red}{\bf #1}}}
\long\def\boldred#1{\textcolor{red}{\bf #1}}
\long\def\boldblue#1{\textcolor{blue}{\bf #1}}

% Useful package for syntax highlighting of specific code (such as python) -- see below
\usepackage{listings}  % http://en.wikibooks.org/wiki/LaTeX/Packages/Listings
\usepackage{textcomp}


%%% The following lines set up using the listings package
\renewcommand{\lstlistlistingname}{Code Listings}
\renewcommand{\lstlistingname}{Code Listing}

%%% Specific for python listings
\definecolor{gray}{gray}{0.5}
\definecolor{green}{rgb}{0,0.5,0}

\lstnewenvironment{python}[1][]{
\lstset{
language=python,
basicstyle=\footnotesize,  % could also use this -- a little larger \ttfamily\small\setstretch{1},
stringstyle=\color{red},
showstringspaces=false,
alsoletter={1234567890},
otherkeywords={\ , \}, \{},
keywordstyle=\color{blue},
emph={access,and,break,class,continue,def,del,elif ,else,%
except,exec,finally,for,from,global,if,import,in,i s,%
lambda,not,or,pass,print,raise,return,try,while},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{green},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
upquote=true,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray}\slshape,
emph={[4]1, 2, 3, 4, 5, 6, 7, 8, 9, 0},
emphstyle=[4]\color{blue},
literate=*{:}{{\textcolor{blue}:}}{1}%
{=}{{\textcolor{blue}=}}{1}%
{-}{{\textcolor{blue}-}}{1}%
{+}{{\textcolor{blue}+}}{1}%
{*}{{\textcolor{blue}*}}{1}%
{!}{{\textcolor{blue}!}}{1}%
{(}{{\textcolor{blue}(}}{1}%
{)}{{\textcolor{blue})}}{1}%
{[}{{\textcolor{blue}[}}{1}%
{]}{{\textcolor{blue}]}}{1}%
{<}{{\textcolor{blue}<}}{1}%
{>}{{\textcolor{blue}>}}{1},%
%framexleftmargin=1mm, framextopmargin=1mm, frame=shadowbox, rulesepcolor=\color{blue},#1
framexleftmargin=1mm, framextopmargin=1mm, frame=single,#1
}}{}
%%% End python code listing definitions

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\cov}{cov}


%\bibliography{./paperpile.bib}
\author{Evan McGinnis}
\title{Automated Weeding}



\definecolor{titlepagecolor}{cmyk}{1,.60,0,.40}

\DeclareFixedFont{\bigsf}{T1}{phv}{b}{n}{1.5cm}

\backgroundsetup{
scale=1,
angle=0,
opacity=1,
contents={\begin{tikzpicture}[remember picture,overlay]
 \path [fill=titlepagecolor] (-0.5\paperwidth,5) rectangle (0.5\paperwidth,10);  
\end{tikzpicture}}
}
\makeatletter                       
\def\printauthor{%                  
    {\large \@author}}              
\makeatother
\author{%
    Evan McGinnis \\
    Graduate Student \\
    Biosystems Analytics \\
    \texttt{evanmc@arizona.edu}\vspace{40pt} \\
    }
\begin{document}
\begin{titlepage}
\BgThispage
\newgeometry{left=1cm,right=4cm}
\vspace*{1cm}
\noindent
%%\vspace*{0.4\textheight}
\textcolor{white}{\Huge\textbf{\textsf{Weed Classification in Row Crops}}}
\vspace*{3.5cm}\par
\noindent
\begin{minipage}{0.35\linewidth}
    \begin{flushright}
        \printauthor
    \end{flushright}
\end{minipage} \hspace{15pt}
%
\begin{minipage}{0.02\linewidth}
    \rule{1pt}{175pt}
\end{minipage} \hspace{-10pt}
%
\begin{minipage}{0.70\linewidth}
\vspace{5pt}
    \begin{abstract} 
The precision treatment of weeds within a crop depends first on accurate classification of plants into two classes: desired plants and undesired plants. Once classified, both positive treatments or assessment of the crop and negative treatment of the weeds is possible, but both of these depend on making this classification. This paper presents the findings that weeds and crop can be distinguished from each other using shape, color, texture, and placement, all factors that can be extracted from RGB images. The shape factors found to be significant were the length to width ratio, and the shape index. The color factor found to be significant was the mean Y value in the YIQ color space. The placement factor found to be significant was the distance the plant was from the crop-line. Using these factors in classification resulted in the correct classification rate of weeds at 100\%, but an incorrect classification rate of crop as weeds of 6.9\% using logistic regression.
    \end{abstract}
\end{minipage}
\end{titlepage}
\restoregeometry
%
% F R O N T  M A T T E R
%
\tableofcontents
\listoffigures
\listoftables
%\listofequations
\newpage

%
% O V E R V I E W 
%



\section{Introduction}
While precision negative treatment of unwanted vegetation or positive treatment of desired vegetation may be the end-product, classifying vegetation in the acquired images is the first major step in that workflow. This paper concentrates on the binary classification of vegetation into two discrete classes: desired vegetation (cantaloupe) and undesired vegetation (weeds and crop occupying an undesired position). Once classified, vegetation can be treated using a variety of mechanical, chemical~(\cite{Saile2022-vu}), or no-touch~(\cite{Saile2022-vu,Mwitta2022-yt}) methods. The specifics of and integration with the treatment system are beyond the scope of this document, but treatment considerations will be discussed within the context of establishing a buffer zone around desired vegetation to minimize crop damage.

While it is tempting to use the terms \textit{weed} and \textit{crop} (and this document will often use those terms), this proposal will more often reference vegetation as \textit{desired} (crop in the position wanted) and \textit{undesired} (weeds and crop not in the position wanted).  That is, classification more often answers the question ``do we want this plant'' than ``is this plant a weed''. These two questions are often conflated, but are subtly distinct. As will be discussed in a later section, a plant may be desirable in one growing season, and undesired in another, complicating attempts at classification.

 Images from UAVs offer several advantages over those obtained with manned aircraft and satellite, such as the economics or flexibility of acquisition, but this study will leverage the relative ease with which UAVs are able to gather imagery from various distances close to the ground. While many studies use images gathered close to the ground from a single distance and report classification accuracy and others use images gathered at multiple distances, reporting (the inevitably declining) accuracy seen with increasing distance, the former tends to suffer when the same attributes are applied at higher distances and the latter from holding the set of attributes constant as altitudes increase. \citeauthor{Ong2023-lm}, in a study of classification of Chinese Cabbage, classifies vegetation acquired at an average of 2m AGL using a single set of attributes, Local Binary Pattern (LBP, a texture descriptor), and a single approach (Random Forest) leveraging those attributes, validating the use of texture in classification \parencite{Ong2023-lm}. \citeauthor{Etienne2021-ik}, in a study of Corn and Soybean plots, used imagery gathered from both 10m and 30m AGL. but found that the best training set for selected algorithm (YOLOv3) was taken from the set of images acquired from 10m \parencite{Etienne2021-ik}. The parameters selected as significant to classification fall into three basic categories: shape, color, and texture. Variants of each of these categories will be used in this study. 
In a study of weed mapping in sunflower crop, \citeauthor{Perez-Ortiz2015-yk} present findings from images taken from three distances AGL, 30m, 60m, and 100m, classifying each pixel as crop, weed, or soil using SVM, KNN, and k-means (and variants of those) approaches \parencite{Perez-Ortiz2015-yk}. While this approach does indeed yield weed maps that lend themselves to later study, the approach is a bit different in terms of what is being classified. In that study, individual pixels are being classified, in contrast to this study that seeks to classify individual plants.
In a study of weeds in corn, \citeauthor{Lin2017-xq} used shape metrics as part of the classification workflow, stressing the importance of expressing shape metrics that are not rotationally variant. More importantly, the study details a shape metric possessing the desired qualities for use in classification. Using shape and texture (GLCM), the study demonstrated an overall accuracy exceeding 95\%.\footnote{\citeauthor{Lin2017-xq}'s study did not report some items that are of interest: the distances of the imagery to the camera, and the false positive rate of weed identification. While the high overall rate sounds is certainly impressive, that metric will not be used in this study. Additionally, the images of vegetation were acquired under controlled conditions, something that is not reproducible in the field an at higher distances AGL.} \citeauthor{Sabzi2020-af}, in a study of a potato field, used various channels of color spaces (YIQ, YCbCr, HSI) as well as texture (Grey-Level Co-occurrence Matrix, GLCM) to achieve a high correct classification rate, but the use of GLCM is not applied  to channels of the color conversion, only to RGB images that have been converted to greyscale \parencite{Sabzi2020-af}. While this is a conventional use of the texture identification technique, it does not leverage the information content available in other color spaces.

Pragmatic concerns must also be considered. UAVs can acquire images in situations where ground acquisition (towed, self-propelled, or even a phone camera on a stick) are infeasible due to field conditions (after an irrigation or precipitation).

\section{Goals and Expectations}
The goal of this study is to classify vegetation with high accuracy (exceeding 90\%) for classification using color, texture, or shape, with higher accuracy achieved using a combination of those attributes. Color is likely affected by ambient light, and shape by height. Both of these will be characterized. In the case of color, it is anticipated that correction will improve accuracy.


\section{Data Preparation}
Data for this study comes in two forms: static images obtained with visible light cameras and prepared images of manually manipulated scenarios.  Feature analysis and machine learning use unmanipulated images exclusively, while tests will be conducted on a mixture of the two. Manipulated images are used throughout this document to illustrate concepts, and use will be noted when employed.

\subsection{Discrete Static Images}
Discrete image sets are those acquired in the field with a hand-held RGB camera. The image set used in this analysis was acquired using three sources: a Samsung SM-G930V phone, an iPhone 14 Pro, and a DJI Air 2s. These images are not without problems, of course. As images often have vegetation at the edges of the image, and frequently of only a single plant, attempts to classify vegetation considering shape and position within the crop-line can lead to poor results. As Figure~\ref{fig:problem-cropline} shows, a crop-line cannot be determined or surmised using only a single crop image. In this instance, the crop-line is assumed to be along the center-line of the image. Figure~\ref{fig:problem-cutoff} demonstrates the problem encountered where the shape of an object cannot be used as a factor in classification, as the shape is distorted by having a regular, straight edge.

% Side by side subfigures 
\begin{figure}[H]
	\begin{subfigure}[h]{0.48\linewidth}
		\includegraphics[width=1\linewidth]{./figures/problem-cropline.jpg}
		\caption{The crop-line cannot be determined.}
		\label{fig:problem-cropline}
		
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.48\linewidth}
		\includegraphics[width=1\linewidth]{./figures/problem-cutoff.jpg}
		\caption{The vegetation is cut off.}
		\label{fig:problem-cutoff}		
	\end{subfigure}%
	\caption[Common problems in field images]{These cases illustrate common problems encountered in image sets where the crop-line cannot be conclusively determined (\ref{fig:problem-cropline}) and the cutoff of vegetation (\ref{fig:problem-cutoff}). In the case where images contain a single image of crop the intended crop-line cannot be automatically determined. In the case where vegetation is cut off along the edges of the image shape analysis cannot be used, as otherwise the shape of the vegetation is distorted by the clean, straight edge. \\ \textit{Images source: Dr.~Mark Siemens, University of Arizona}}
\end{figure}



\subsection{Prepared Images}
The images acquired in the field are prepared using Adobe Photoshop\textsuperscript{\textregistered} to create test data for various scenarios:
\begin{itemize}
	\item{The presence of a specific weed in the image.}
	\item{While somewhat tangential to the task of classification, the presence of unwanted vegetation too close to wanted vegetation may lead to the same classification, but a different treatment. This is the case where vegetation is identified as \textit{undesired}, but is left untreated due to the risk of treatment would pose to the crop.} This is achieved in two ways -- duplicating or moving an existing weed within the image and inserting a new weed image obtained elsewhere.
	\item{Debris in the image -- this is the case where non-vegetated matter appears in the image. While the image segmentation discussed in section \ref{section:segmentation} should result in non-vegetated matter being removed, inserting both expected (irrigation equipment) and unexpected (human litter) into the images is a mechanism to test various scenarios.}
\end{itemize}

\begin{figure*}[h]
	\begin{subfigure}[t]{0.48\linewidth}
		\includegraphics[width=1\linewidth]{./figures/with-purslane.jpg}
		\caption{Purslane (\textit Portulaca oleracea) inserted.}
		\label{fig:prepared-weed}
	\end{subfigure}
	\hfill
	%
	% TODO: Replace this image with one where irrigation equipment is inserted into image
	%
	\begin{subfigure}[t]{0.48\linewidth}
		\includegraphics[width=1\linewidth]{./figures/cantaloupe-with-lid.jpg}
		\caption{Identifying lid beside plant.}
		\label{fig:prepared-lid}		
	\end{subfigure}%
	\caption[Prepared images]{Images acquired in the field are subsequently manipulated by either inserting images of weeds not already present (\ref{fig:prepared-weed}), or portions of the same image (\ref{fig:prepared-lid}). Both cases are intended to present situations that should be handled by the classification algorithms. \\ \textit{Manipulated images: (\ref{fig:prepared-weed}) Dr. Mark Siemens, University of Arizona}}
\end{figure*}


Here, vegetation is placed within separate layers to create a composite image that shows the desired result.  Figure~\ref{fig:prepared-weed} illustrates the basic technique that is used to form a single image of both crop and a weed. This figure shows several differences that are immediately obvious: scale differences, as the weed image was obtained at a different distance, and lighting difference, as the weed image was obtained at a different location. Notice the obvious seam in the lettuce plant on the right hand side of the image. As these pixels will be discarded early in image processing, they will have no effect on the current process as a whole, but may be a consideration if future processing considers factors such as the vegetation’s shadow. Inserting a weed into an image solely for the purposes of classification has little value, of course, as classification of separate images is equivalent. The intent of this is to test the effect of the weed's proximity to the crop. Images are also manipulated to test certain scenarios, as Figure \ref{fig:prepared-lid} shows. A bucket lid with dots used to identify a specific plant is inserted into the image two test two things: the exclusion of the bucket lid itself, and the identification of the lid and plant it is closest to.



\section{Approach To Classification}
The goal of this processing flow is to determine what each item within the image is and classify it as desired or undesired. Weeds are, of course, undesired, but crop plants can also be classified as \textit{undesired} in the case of thinning operations, so this document will use the term \textit{undesired} and \textit{weed} interchangeably. Vegetation in an image falls into one of these classifications:
\begin{itemize}
	\item{Desired, the crop}
	\item{Undesired, weeds that can be treated without crop damage}
	\item{Unknown, vegetation that cannot be confidently identified}
	\item{Ignored, vegetation that is too close to desired vegetation to be treated without crop damage}
\end{itemize}

Additionally, this work makes a few simplifying assumptions about the images:
\begin{itemize}
	\item{Crop is typically in roughly alignment (the centers will be with a few degrees of each other).}
	\item{Weed centers are typically not in horizontal alignment.}
	\item{Crop is typically much larger than weeds.}
\end{itemize}

These are not without exceptions. For instance, a weed may be in perfect alignment within a crop line and two other plants. Each of these observations, however, are used in classification. Take, for instance, the observation that crop tends to be much larger than weeds. If the size of an item is three times that of another, the smaller item is -- more likely than not -- a weed.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.48\linewidth]{./figures/crop-alignment}
	\caption{Crop may not be in perfect horizontal or vertical alignment in images acquired autonomously. In this drone image, the crop is not in an alignment that would lend itself to crop identification in either orientation. }
	\label{fig:alignment}	
\end{figure}

For each identified plant, color, shape, and texture features are `

\section {Study Area}
This study will be carried out at the University of Arizona Maricopa Agricultural Center (MAC) on a cantaloupe crop (spring 2024) and broccoli crop (fall 2024) overseen by Drs. Attalah and Elshikha (University of Arizona), as part of another study.\footnote{The field in question is located at N 33.061805$^{\circ}$ W 111.966162$^{\circ}$} That study will involve three separate water treatments: center-pivot, flood irrigation, and drip. As the drip treatment study area is unlikely to produce a heavy weed load, it will not be included in this study, as differentiating between crop and weeds is an initial goal of this study. The study site is relatively free of obstacles that would impact low level flight with one notable exception: the center pivot irrigation system. Allowances will be made to avoid that system, but doing so is not expected to affect flight plans, as images in only subset of the field is planned to be captured.
 

\section{Software Environment \& Source code}
Both commercial and open-source software were used in the development an operational aspects of this study. While the majority of the software (the operational portions responsible for image processing and classification) was written in Python, commercial packages were used for pre-processing. Additionally, R was used in visualization and analysis of classification results. Some of these components play a large enough role that they warrant special mention in various places throughout this document, but in-depth descriptions of them will not be made here. Likewise, this list is not exhaustive, as numerous libraries were used for a few functions, but included here. The complete source code for this project is available in this GitHub repository: \href{https://github.com/evan-mcginnis/weeds}{\textit {weeds}}. Images, both raw and processed, are far too large to be hosted with GitHub, and are available on request from the author.

All development was done on Windows 10 and Ubuntu 18.04.06 with analysis on the University of Arizona's \textit{High Performance Compute Cluster}.

{
% This avoids the document line spacing affecting the contents of the table
\setstretch{1.0}
\begin{longtable}{x{\dimexpr.20\columnwidth-2\tabcolsep}
                  x{\dimexpr.20\columnwidth-2\tabcolsep}
                  x{\dimexpr.5\columnwidth-2\tabcolsep}}
%\begin{hyphenrules}{nohyphenation}
    \caption{Software Used}\label{tab:software}  \\
\toprule
{\textbf{Component}} & {\textbf{Use}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endfirsthead
%%%%
    \caption{Software Used (cont.)}\label{tab:software}  \\
\toprule
{\textbf{Component}} & {\textbf{Use}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endhead
%%%%
\midrule[\heavyrulewidth]
\multicolumn{3}{r}{\footnotesize\itshape
                   Continued on the next page}
    \endfoot
%%%%
\bottomrule
    \endlastfoot
%%%%
		PyCharm 
		& Python IDE     
		& Commercial software for Python development
\tabularnewline\addlinespace
		Python 3.9     
		& Python runtime                    
		& Most software was written in python
\tabularnewline\addlinespace
		scikit-learn
		& Machine Learning     
		& Implementations for various ML techniques 
\tabularnewline\addlinespace
		OpenCV2 
		& Image Processing     
		& Machine Vision and Image Processing framework
\tabularnewline\addlinespace
		Numpy
		& Numeric Processing   
		& Python library
\tabularnewline\addlinespace
		Pandas 
		& Numeric Processing     
		& Python library
\tabularnewline\addlinespace
		PyQT5 
		& UI     
		& User Interface Framework for Python
\tabularnewline\addlinespace
		R 
		& Post Processing     
		& Statistical Software
\tabularnewline\addlinespace
		Plotly
		& Visualization     
		& Graphing libraries
\tabularnewline\addlinespace
		Adobe Lightroom
		& Color Correction     
		& Commercial software used for pre-processing
\tabularnewline\addlinespace
		Dronelink
		& UAV Control     
		& Commercial software used for planning \& control
\tabularnewline\addlinespace
		MongoDB
		& Image organization     
		& No-SQL database 
\tabularnewline\addlinespace
		Docker Container
		& Virtualization     
		& Virtual containers host database and web server
\tabularnewline\addlinespace
		Slurm
		& Workload     
		& Supercomputer workload management
\tabularnewline\addlinespace
		NGINX
		& Web Server     
		& Host HTTP API requests
\label{table:software}
\end{longtable}
}

\section{Workflow}
Images are processed following the workflow shown in Figure~\ref{fig:workflow}. Each step in this workflow will be discussed in subsequent sections.\footnote{The example images and analysis within the next few sections are taken from a set of images supplied by Dr. Mark Siemens, University of Arizona, and as such are not expected to provide an exact match to those acquired under field conditions. These images, however, are useful in the demonstration of key techniques that will be applied to images gathered under field conditions. This image set of 39 images was taken in 2019 with a Samsung SM-G930V phone.} The workflow illustrated supports the notion of supervised learning, where a human classifies plants that are then used in a model for predicting the plant's class.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{./figures/workflow.png}
	\caption{Image processing workflow. There are two workflows followed, the creation of the model (where a human classifies the vegetation in the image) and use of that model to predict the plant.}
	\label{fig:workflow}	
\end{figure}


\section{Acquire Image}
While the the processing step consists only of reading images from disk, actual acquisition was carried out in this study using a \href{https://www.dji.com/air-2s/specs} {DJI Mavic Air 2 SE} drone with a 20 MP camera, and an \href{https://support.apple.com/en-us/111849} {Apple iPhone 14 Pro} with a 48 MP camera, followed by color correction of acquired images. Each image acquisition was accompanied by an image of a \href{https://www.datacolor.com/spyder/products/spyder-checkr-photo/} {SpyderChecker 24} color calibration chart (under the same ambient lighting conditions) and color correction was subsequently applied using Adobe Photoshop Lightroom Classic and custom calibration profiles created using the Spyder Checkr software. Raw sensor data was stored in Adobe Digital Negative (DNG) format before color correction and conversion to a compressed JPG format for the resulting images. Images were color corrected against images taken before each flight of a Spyder CHECKR24 calibration target. The digital negative of each image (DNG, a standard format for raw sensor data) was corrected and converted to JPG format for subsequent processing.\footnote{The merits of raw, uncompressed, and unprocessed sensor data versus compressed, processed formats like JPG are beyond the scope of this document, but it is fair to say taking raw sensor data, processing, and then producing a compressed JPG is precisely what consumer products like phone cameras do. So that the color correction step can be inserted, the process of producing an image that can be viewed with most software is altered a bit, but essentially unchanged. A picture stored as a DNG is not higher resolution or higher quality than a JPG equivalent, just more easily manipulated without the introduction of errors. That is, JPG is simply a commonly encountered format for images that is considerably smaller than what would be expected of raw sensor data.} The sensor data for each image was quite large, requiring 38.5 MB of storage, resulting in these requirements for the altitudes studied:

Images were color corrected against images taken before each flight of a Spyder CHECKR24 calibration target. The digital negative of each image (DNG, a standard format for raw sensor data) was corrected and converted to JPG format for subsequent processing.\footnote{The merits of raw, uncompressed, and unprocessed sensor data versus compressed, processed formats like JPG are beyond the scope of this document, but it is fair to say taking raw sensor data, processing, and then producing a compressed JPG is precisely what consumer products like phone cameras do. So that the color correction step can be inserted, the process of producing an image that can be viewed with most software is altered a bit, but essentially unchanged.}
Where feasible, missions were conducted in the same ambient light conditions, but an exact match of ambient conditions using uncontrolled lighting is not always possible.


{\renewcommand{\arraystretch}{2}%

{
% This avoids the document line spacing affecting the contents of the table
\setstretch{1.0}
\begin{longtable}{x{\dimexpr.25\columnwidth-2\tabcolsep}
                  x{\dimexpr.35\columnwidth-2\tabcolsep}
                  x{\dimexpr.4\columnwidth-2\tabcolsep}}
%\begin{hyphenrules}{nohyphenation}
    \caption{Storage Requirements}\label{tab:storage}  \\
\toprule
{\textbf{AGL (meters)}} & {\textbf{DNG (Raw Sensor Data, MB)}} & {\textbf{Compressed (JPG, MB)}}
\tabularnewline
\midrule
    \endfirsthead
%%%%
    \caption{Storage Requirements (cont.)}\label{tab:storage}  \\
\toprule
{\textbf{AGL (meters)}} & {\textbf{DNG (Raw Sensor Data, MB)}} & {\textbf{Compressed (JPG, MB)}}
\tabularnewline
\midrule
    \endhead
%%%%
\midrule[\heavyrulewidth]
\multicolumn{3}{r}{\footnotesize\itshape
                   Continued on the next page}
    \endfoot
%%%%
\bottomrule
    \endlastfoot
%%%%
		2
		& XXX     
		& XXX
\tabularnewline\addlinespace
		5     
		& XXX                    
		& XXX
%\tabularnewline\addlinespace
\label{table:segmentation}
\end{longtable}
}

%
% I M A G E  S E G M E N T A T I O N
%
\section{Segment Image}
\label{section:segmentation}
Perhaps the least compute-intensive portion of this workflow is simply to discard to portion of an image that does not contribute meaningfully to classification, and for this problem space those pixels that do not contribute useful information are, most notably, pixels representing the ground. There are three segmentation approaches considered for this task: index-based, threshold based, and deep-learning based. The result of each of these aproaches is the same: the portions of the images that did not contain pixels with vegetation present were the discarded by applying a visible light vegetation index to both isolate the vegetation and reduce the information content. This discards both the ground and debris while retaining the vegetation. 

\subsection{RGB Index Based}
These images were segmented using various visible light indices (\cite{Hunt2013-ih}, \cite{Hamuda2016-dw}). As this process is not the primary subject of this paper, it will be given only superficial mention here.  Various approaches to image segmentation are  summarized in Table \ref{table:segmentation}.  The intent of this step of the workflow is not to classify the individual plants, but to preprocess the image before further analysis is done.

{\renewcommand{\arraystretch}{2}%

{
% This avoids the document line spacing affecting the contents of the table
\setstretch{1.0}
% Example to span two pages
\begin{longtable}{x{\dimexpr.25\columnwidth-2\tabcolsep}
                  x{\dimexpr.35\columnwidth-2\tabcolsep}
                  x{\dimexpr.4\columnwidth-2\tabcolsep}}
%\begin{hyphenrules}{nohyphenation}
    \caption{Visible light indices}\label{tab:example}  \\
\toprule
{\textbf{Index}} & {\textbf{Formula}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endfirsthead
%%%%
    \caption{Visible light indices (cont.)}\label{tab:example}  \\
\toprule
{\textbf{Index}} & {\textbf{Formula}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endhead
%%%%
\midrule[\heavyrulewidth]
\multicolumn{3}{r}{\footnotesize\itshape
                   Continued on the next page}
    \endfoot
%%%%
\bottomrule
    \endlastfoot
%%%%
		Triangular Greenness
		& \begin{minipage}[t]{0.3\textwidth}
			$R_{green} - \alpha R_{red} - \beta R_{blue}\\ \alpha = \frac {2(\lambda_{blue} - \lambda_{green})} {(\lambda_{blue} - \lambda_{red})}\\ 
		    	\beta = \frac {2(\lambda_{green} - \lambda_{red})} {(\lambda_{blue} - \lambda_{red})} $
		   \end{minipage}     
		& Corrects for camera calibration using the peak sensitivity
\tabularnewline\addlinespace

		Normalized Difference     
		& $128 * \left( \left( \frac {(G - R)} {(G + R)} \right) + 1 \right) $                    
		& The NDI index produces a near-binary image. 
\tabularnewline\addlinespace

		Excess Green      
		& \begin{minipage}[t]{0.3\textwidth}
			$R = \frac {R} {R_{max}}\\ G = \frac {G} {G_{max}}\\ B = \frac {B} {B_{max}}$ 
		   \end{minipage}
		& ExG provided a clear contrast between plants and soil 
\tabularnewline\addlinespace

		Excess Red      
		& $1.3 R - G$ 
		& inspired by the fact that there are 4\% blue, and 32\% green, compared with 64\% red cones in the retina of the human eye
\tabularnewline\addlinespace

		Color Index of Vegetation Extraction      
		& $0.441 R - 0.811 G + 0.385 B + 18.78745$
		& This method was proposed to separate green plants from soil background in order to evaluate the crop growing status.
\tabularnewline\addlinespace

		Excess Green - Excess Red   
		& $ExG - ExR$ 
		& ExG used to extract the plant region and ExR used to eliminate the background noise (soil and residue) where green–red material (stems, branches, or petioles) may exist
\tabularnewline\addlinespace

		Normalized Green-Red Difference    
		& $\frac {(G - R)} {(G + R)}$ 
		& The method of NGRDI was used to overcome the differences in exposure settings selected by the digital camera when acquiring aerial photography of the field. 
\tabularnewline\addlinespace

		Vegetative Index      
		& $\frac {G} {R^aB^{(1-a)}}, a = 0.667$ 
		& VEG has a significant advantage because it is robust to lighting change.
\tabularnewline\addlinespace

		Com1   
		& $ExG + CIVE + ExGR + VEG$ 
		& High computational cost --- does not perform well in high or low light levels
\tabularnewline\addlinespace

		Modified Excess Green      
		& $1.262G - 0.884R = 0.311B$ 
		& Does not perform well in high or low light levels. 
\tabularnewline\addlinespace

		Combined Indices 2      
		& $0.36ExG + 0.47CIVE + 0.17VEG$ 
		& Uses weighting factors to emphasize strengths of various approaches
%\tabularnewline\addlinespace
\label{table:indices}
\end{longtable}
}

These indices are used to create a mask that is then applied to the original source image to permit vegetation to show while masking details that are not relevant (ground pixels, stones, and other items that may appear in field conditions) The intent here is to remove all pixels that are not relevant to the task of distinguishing between crop and weed while leaving the vegetated pixels unmanipulated.

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{.30\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{figures/original.jpg}
	  \caption{Field view of lettuce and weed}
	  \label{fig:original}
	\end{subfigure}
	\begin{subfigure}[h]{.30\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{figures/original-mask.jpg}
	  \caption{Mask produced with NDI}
	  \label{fig:mask}
	\end{subfigure}
	\begin{subfigure}[h]{.30\textwidth}
	  \centering
	  \includegraphics[width=1\linewidth]{figures/original-masked.jpg}
	  \caption{After applying mask}
	  \label{fig:original-masked}
	\end{subfigure}
	\caption[Before and after segmentation]{Before and after segmentation. Note the absence of the stems seen in the weed in the lower portion of~\ref{fig:original-masked} -- this is made a bit more obvious with a close examination of the mask shown in~\ref{fig:mask}. The lack of stems will lead to a single plant being identified as multiple plants, but has no effect on the classification using the features identified as significant.}
	\label{fig:segmentation}
\end{figure}
The segmented image has discarded ground pixels while retaining most of the pixels that will be used, but a close examination reveals that pixels in the stems of the weed are also eliminated, as they are less green than the rest of the plant. This effect is even more pronounced when segmenting images of weed that do not contain green stems as would be seen in the red stems seen in \textit{Portulaca oleracea} (Purslane). While they are not eliminated, pixels in the area of the deep shadows of the vegetation may affect attempts to classify objects based on color attributes. Unfortunately, the band of color featured in the stems (red) is frequently found in the background (soil), so attempts to make the stems appear in the masked image are problematic, as this solution tends to bring allow unwanted ground pixels in the final image that contain hues found within the stems. Likewise, immature vegetation where stems are not sufficiently green will not appear in the final image. Fortunately, both of these cases do not appear to have an appreciable effect on the classification. For the purposes of this paper, images will use the \textit{Normalized Difference Index} (NDI) segmentation approach (see Table~\ref{table:segmentation} for this formula). This process results in image data with only two sets of values: RGB values of zero where there is no vegetation and the original RGB values for pixels containing vegetation. Figure~\ref{fig:ndi-segmentation} illustrates the values used in producing a mask, but here we see a problem introduced.

The creation of an index has produced data that often exaggerates portions of the image with vegetated pixels, but deciding what portions of the data to discard and which to keep can be automated instead of using a single threshold for all images. The manual selection of a threshold can be shown to work relatively well, and can be fine-tuned by examining the results ensure that the number of vegetated pixels are maximized, while the number of non-vegetated pixels are minimized (in other words, we do not want to have significant portions of the ground in the image, but we do want as much of the plant as is possible). Unfortunately, images can be acquired under different lighting conditions, leading to the case where there is often not a threshold that works well for all images. An alternative, first proposed by \citeauthor{Otsu1979-io}  \cite{Otsu1979-io}, is to select this threshold automatically for each image, or the image can be selected using the \textit{Triangle} algorithm. Otsu's algorithm arrives at a threshold by maximizing the variance between foreground and background pixels. The triangle method arrives at a thresholdh as the point where a line connecting the histogram’s peak to the maximum intensity is maximized. Both of these techniques operate on greyscale images, considering only pixel intensity. It is also worth considering the distribution of pixel values: typically bi-modal (two distinct peaks) and unimodal (a single peak).

%
% Histogram produced with this command:'
% python plot-threshold.py -i d:\maricopa-test\flood-2-2m\DJI_0610.jpg

\begin{figure*}[h]
	\centering
	  \subfloat[Intensity Histogram\centering]{\includegraphics[height=0.19\textheight]{figures/pixel-values.png}\label{fig:intensity-histogram}}
	\hfill
	   \subfloat[Source Image\centering]{\includegraphics[height=0.19\textheight]{figures/pixel-values-image.jpg}
	  \label{fig:source-image}}
	\caption[Bimodal distribution of pixel intensity]{A example of a bimodal distribution of pixel intensity values. Note that there are two peaks in the histogram above (\ref{fig:intensity-histogram}), corresponding to what can be observed in \ref{fig:source-image}, where there are only two classes of pixels visible: crop and ground.}
	\label{fig:intensity}
\end{figure*}

Otsu's method operates on the assumption that there are two classes of pixels in the object (interesting and uninteresting things). The algorithm minimizes intra-class variance. This method performs well when presented with images that have two distinct peaks
% Equation from https://infoaryan.com/blog/opencv-python-otsu-and-triangle-thresholding-full-mathematics-code-explained-important/
\begin{equation}
\sigma^2_B (t) \times p(t) \times p(\overline{t}) \\
\end{equation}
where $\sigma^2_B (t)$ is the variance between the two classes, $p(t)$ is the probability of class $t$, and $p(\overline{t})$ is the probability of the complement.

For the triangle method uses this equation:
\begin{equation}
T_{triangle} = argmax \left[ h(T) \times \left( 1 - \left| \frac{T - P}{R - P} \right| \right) \right]
\end{equation}
Where $h(T)$ is the value at $T$, $R$ is the maximum of the intensity, and $P$ is the peak value.

The details of the implentation of these two approaches are beyond the scope of this document, and while there are certainly others, these are, perhaps the most frequently encountered approaches. Both of these approaches, however, are based on some assumptions about the intensity histogram, and may not work well in all situations, particularly if these assumptions are not valid. Otsu's algorithm, for example, tends to yield satisfactory results when operating on images posessing a bimodal distribution, but may not yield good results on images with a single peak or multiple peaks.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.48\textwidth}
	  \centering
	  \includegraphics[height=5cm]{./figures/ndi-1-of-2.png}
	  \caption{Image segmented using NDI}
	  \label{fig:ndi-1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{.48\textwidth}
	  \centering
	  \includegraphics[height=5cm]{./figures/triangle-algorithm}
	  \caption{Triangle method for threshold selection.}
	  \label{fig:ndi-2}
	\end{subfigure}
	\caption[NDI segmentation and threshold selection]{An example of NDI segmentation and threshold selection. A threshold for the production of a mask can be done manually, perhaps by selecting all values below 140 to form the mask that will be applied to the image, illustrated by the semi-opaque plane. While that manual selection may be sufficient to perform the segmentation of an image set acquired under the same ambient conditions, it may lead to the exclusion of portions of the plant in images taken under different conditions. Using the Triangle algorithm \parencite{Brink1996-xy,Zack1977-yl} allows this selection to be made automatically. The point at which the line between the lowest and highest points in the histogram is selected as the threshold, selecting a threshold applicable to each image in a set rather than a global value used for all images in a set.}
	\label{fig:ndi-segmentation}
\end{figure}

% Put the graphics in a longtable

% Longtable end

\begin{figure}[H]
\centering
\captionsetup[subfloat]{labelfont=scriptsize,textfont=scriptsize}
\begin{tabular}{cccccc}
\subfloat[NDI]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-NDI.jpg} \label{fig:ndi}} &
\subfloat[EXG]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-ExG.jpg} \label{fig:exg}} &
\subfloat[EXR]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-ExR.jpg} \label{fig:exr}} &
\subfloat[CIVE]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-CIVE.jpg} \label{fig:cive}} &
\subfloat[EXG-EXR]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-ExGR.jpg} \label{fig:exgexr}} &
\subfloat[NGRDI]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-NGRDI.jpg} \label{fig:nrgdi}} \\
\subfloat[Com1]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-COM1.jpg} \label{fig:com1}} &
\subfloat[Com2]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-COM2.jpg} \label{fig:com2}}&
\subfloat[MEXG]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-MexG.jpg} \label{fig:mexg}} &
\subfloat[TGI]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-TGI.jpg} \label{fig:tgi}} &
\subfloat[VEG]{\includegraphics[height=0.10\textheight]{figures/20201117_112624-VEG.jpg} \label{fig:veg}} &
\subfloat[Original]{\includegraphics[scale=0.0215,angle=90]{figures/20201117_112624.jpg} \label{fig:original}} \\

\end{tabular}
\caption[Results of various visible-light image segmentation techniques]{Various visible-light image segmentation techniques (See Table \ref{table:indices} for details). At first glance, many of the segmentation algorithms produce similar results, but some of the methods are not sensitive to less green portions of the plants -- slender stems. Contrast the results seen in \ref{fig:exr} and \ref{fig:mexg}, with the slender stems of the plant in the middle of the image not present in the result of the former.}
\label{figure:results}
\end{figure}

There are two sorts of mistakes encountered in segmentation, as there are only two sorts of things in the image: things that are vegetation (crop and weeds) and things that aren't (typically, the ground). Omission of a crop leaf is apparent in Figure \ref{fig:ndi}, showing the result of using NDI.  The center plant is only partially visible. Contrast that with the result shown in Figures \ref{fig:hi} and  \ref{fig:hv}, where the ground is featured. This case may be due to the shadow of the plant containing too much green as sunlight passes through the leaf. Figure \ref{fig:overlay} illustrates the effects of the index and errors -- of special note here is the error that is shown in the upper right-hand portion of Figure \ref{fig:overlay-cive}. Applying the NDI mask produces the errors discussed previously -- the leaf shadow is identified as vegetation. The evaluation of errors in index-based approaches is illustrated 

\begin{figure}[H]
	\centering
	\subfloat[CIVE mask overlay\centering]{\includegraphics[width=0.45\textwidth]{figures/overlay-cive.jpg}\label{fig:overlay-cive}}
	\hfill
	\subfloat[NDI mask overlay\centering]{\includegraphics[width=0.45\textwidth]{figures/overlay-ndi.jpg}\label{fig:overlay-ndi}}
	\caption[Masks overlayed on original images]{Masks produced with CIVE and NDI overlayed on the original image. In these images, the masks used are colored red and blended with the original image. Areas that are tinted red will be shown, so a `correct' mask will completely cover vegetation. Note the mask shown in \ref{fig:overlay-ndi} does not cover the plant completely, and segments out shadow portions of the image. What seems to be going on here is that sunlight is shining through the leaf, making the ground green enough to be interpreted as vegetation. Contrast this with the errors seen in \ref{fig:overlay-cive}, where portions of the ground are incorrectly segmented as vegetation in cases not attributable to color contamination. (The upper right-hand portion of \ref{fig:overlay-cive} is a good example of this error.) Photo: Dr. Mark Seimens, University of Arizona.}
	\label{fig:overlay}
\end{figure}

\subsubsection{HSI and HSV}
The Hue, Saturation and Intensity (HSI) and Hue Saturation and Value (HSV) color spaces present an opportunity to consider how saturated the colors are instead of just emphasizing the green found it an image. While that is a bit of an over-simplification of basing a technique on the values found within the RGB bands, it is essentially what is happening. Hue based approaches take much the same approach in one respect -- the hue frequently present in vegetation (green) is represented as peaking around $120^o$ in both models. This is convenient, certainly, as simply considering points as vegetation of they contain hue values around the peak. How wide this range is, of course, will determine what is captured. And while the topic of color is revisited in Section ~\ref{section:problems-color}, consider that leaves contain much that is not green, just as the ground contains much that is not brown. A leaf may contain hue values in the range of 40-70, an indication that the hue is toward the red end of the representation. Likewise, a patch of dirt may contain hue values in the range of 15-20, an indication that the hue is much more red than anything else. HSV is a bit more complicated, in that the V band should be normalized between $(0..1)$ and mutiplied by the H value. Lacking more creative terms, these indices will be referred to as HI and HV. 

\begin{equation}
	\label{equation:hsi}
	HSI
    \begin{split}
		discard~pixels~with~H~channel &\geq 190~or \leq 55\\
		discard~pixels~with~S~channel &\leq 0.45 \\
		discard~pixels~with~I~band &\geq 50
    \end{split}
\end{equation}

\begin{equation}
	\label{equation:hsv}
	HSV
	\begin{split}
		H * norm(V) \\
	\end{split}
\end{equation}

\subsubsection{CIELAB}
The CIELAB color space, often referred to with L*A*B* in the name, expresses color along three lines: Lightness (L), Green-Red (A), and Blue-Yellow (B). These values seen in these bands have different ranges, L ranging from 0 (black) to 100 (white), A ranging from negative (green) to positive (red), and B ranging from negative (blue) to positive (yellow). The A and B bands are not technically limited to a specific range, as this space was designed to exceed human perception. Human perception of these colors can be covered with a $\pm 150$ range, and implementations tend to limit the values reported in these bands for pragmatic reasons.  For the purposes of indexing vegetation the bands of  most interest are the A and B bands. Taking the difference between B and A and discarding pixels with a difference is below 30 yields acceptable results. This index will be referred to as CI throughout this document.

\begin{equation}
		\label{equation:cielab}
		B - A, discard~results~\leq~30
\end{equation}

\subsubsection{YCbCr}
This color space (sometimes termed YCC) is frequently encountered in video transmission, as it is designed to address the redundancies seen in the RGB color space. It uses the $Y$ channel to carry Luma information (black and white) and two color channels expressing the blue difference (Cb) and red difference (Cr). Separating the brightness from the color components allows the color representations to match human perception -- as our eyes are more sensitive to changes in brightness than they are to color. As with other indices, the objective is to exaggerate the band vegetation occupies and ignoring components that are not. Using equation \ref{equation:ycbcr} achieves that goal.
\begin{equation}
	\label{equation:ycbcr}
	\ln(Cr) * Cb, discard\ results\geq 890
\end{equation}
This index will be referred to as YCbCRI throughout this document.
\subsubsection{YIQ}
The YIQ model of color was used by the (analog) NTSC color TV system. Later, digital transmission schemes used different color spaces, notably the YCbCr that will be addressed in more detail in a separate section. These schemes have a common approach: \textit{chrominance} (a color component) is added to a black and white image. In the YIQ model, $Y$ represents the luma information (black and white), with $I$ (in-phase, representing red-cyan contrast) and $Q$ (quadature, representing magenta-green contrast) the chrominance information. 
The processing employed here is to convert the image to the YIQ color space and take the mean value for the $I$, or in-phase component for the blob's pixels. (\cite{MathWorks_undated-jg}) For segmentation purposes, the information in the Q band is helpful, primarily in discarding things that are not vegetation.
\begin{equation}
	\label{equation:yiq}
	discard\ any\ pixel\ with\ Q\ band\leq 0.04
\end{equation}
This index will be referred to as YI throughout this document.

\begin{figure}[H]
\centering
\begin{tabular}{ccc}
	\subfloat[CIElab]{\includegraphics[width = 1.25in]{figures/20201117_112624-CI.jpg} \label{fig:ci}} &
	\subfloat[HSI]{\includegraphics[width = 1.25in]{figures/20201117_112624-HI.jpg} \label{fig:hi}} &
	\subfloat[HSV]{\includegraphics[width = 1.25in]{figures/20201117_112624-HV.jpg} \label{fig:hv}} \\
	\subfloat[YCbCr]{\includegraphics[width = 1.25in]{figures/20201117_112624-YCbCrI.jpg} \label{fig:ycbcr}} &
	\subfloat[YIQ]{\includegraphics[width = 1.25in]{figures/20201117_112624-YI.jpg} \label{fig:yiq}} &
	\subfloat[Original]{\includegraphics[scale=0.0415,angle=90]{figures/20201117_112624.jpg} \label{fig:original}} \\
	\end{tabular}
	\caption[Segmentation results from various colorspaces outside RGB]{Segmentation results from various colorspaces outside RGB. The \ref{fig:ci} and \ref{fig:yiq} segmentation approaches are relatively clean, providing vegetation images that are free of ground clutter. The other three approaches, while providing good vegetation images, show too much of the ground, particularly in areas near to a plant, likely due to color contamination in the shadow area}
	\label{figure:results-colorspaces}
\end{figure}


% Bar chart produced with this command
%  python evaluate-masks.py -i ../lib/mask.jpg -t ../lib/testing/20201117_112624
\begin{figure}[h]
	\centering
	\includegraphics[width=.75\linewidth]{figures/segmentation-error-rates.png}
	\caption[Error rates of segmentation algorithms]{Average error rates for the images. Type I (Vegetation identified as ground) and Type II (Ground identified as vegetation) error are summarized by this figure. Of special note here are the errors encountered with NRGDI and HI approaches, as they represent clear differences in errors. The majority of the errors shown by the HI index mistakenly shows the ground as vegetation, with NDGRI  doing the opposite, hiding vegetation.}
	\label{fig:segmentation-errors}
\end{figure}

\subsubsection{Problem: Reflections and Shadows}
Reflections within vegetation provide a challenge not easily surmounted.  In some cases, the area in reflection is not merely brighter than the surrounding pixels, but is completely devoid of usable pixels, as it is completely white (sometimes referenced as \textit{clipped}). This leads to the situation where portions of the vegetation are not present in the final segmented image, as the pixels do not contain the values associated with vegetation.  Deep shadows suffer from a problem similar to reflections in that they may be seen as nearly pure black. While mild shadows or reflections do not present much of a challenge, as vegetation in these areas typically have pixel values that are closely associated with their class. Reflections and shadows can be partially mitigated by a technique that improves the overall contrast of the image: \textit{histogram equalization}. Histogram equalization, for lack of a more precise description, stretches out the contrast over a broad range. While histogram equalization cannot reconstruct pixels than have been registered as pure white or pure black, this approach yields images with a more uniform intensity distribution.


% This produces figures that have aligned captions -- the [t] bit does the trick
\begin{figure}[H]
	\centering
	\subfloat[Leaf with reflection]{\includegraphics[width=.22\linewidth]{figures/reflection.jpg}\label{fig:leaf-with-reflection}}
	\hfill
	\subfloat[Before]{\includegraphics[width=.22\textwidth]{figures/reflection-histogram.jpg}\label{fig:reflection-histogram}}
	\hfill
	\subfloat[After equalization]{\includegraphics[width=.22\textwidth]{figures/reflection.equalized.jpg}\label{fig:leaf-equalized}}
	\hfill
	\subfloat[After equalization]{\includegraphics[width=.22\textwidth]{figures/reflection-histogram-equalized.jpg}\label{fig:leaf-equalized-histogram}}
	\caption[Reflection problems in segmented vegetation]{Reflections may be clipped (completely white) containing no usable information. While histogram equalization improves the image somewhat, applying that technique cannot reconstruct pixels that have been recorded as pure white. Figure \ref{fig:leaf-with-reflection} shows a portion of an image with a reflection strong enough to render a significant portion of the image unusable. Note the pixel count for pure or nearly pure white pixels in both \ref{fig:reflection-histogram} and \ref{fig:leaf-equalized-histogram} remain constant.  Those pixels should contain values indicative of vegetation. Aside from being a bit more pleasant to look at, these images differ in way that may have an impact on threshold selection. Note that the histogram of \ref{fig:reflection-histogram} no longer has the same shape, as intensities have be redistributed, eliminating the small bump.}
	\label{fig:reflection}
\end{figure}

\subsubsection{Color problems}
\label{section:problems-color}
Many of the indexes suffer from the same problem: they are focused on vegetation being green. Green stems and leaves are easily detected, but stems and leaves of other colors are not. Consider commonly encountered red-leaf lettuce. Depending on the stage of development and the angle of view, the leaves can be partially or entirely red (or a deep purple color). This is not limited to crops, of course, weeds such as redroot pigweed (\textit{Amaranthus retroflexus}) have red at the base of the leaves. Likewise, some weeds exhibit red stems. Purslane (\textit{Portulaca oleracea}) has red stems, and as an added complication, red edges on the leaves. Attempts to reveal the stems are complicated by the ground often containing a strong red coloration. Unfortunately, the band of color featured in the stems (red) is frequently found in the background (soil), so attempts to make the stems appear in the masked image are problematic, as this solution tends to bring unwanted ground pixels in the final image that contain hues found within the stems. The result of this is that the non-green portions of the vegetation does not appear in the resulting segmentation.  Even predominately green vegetation has non-green portions. Flowers are often missed by a green-centric index. While the omission of flowers may be desirable in further processing, if the flower obscures green vegetation that would otherwise be identified as such, that complicates further processing, especially when the green portion is significantly obscured. Uncorrected colors also complicate segmentation, as the same plant's leaves may be captured differently with two different cameras or under different lighting conditions. Under controlled lighting conditions (such as would be encountered in an enclosed system) color calibration is not essential every time, but systems that use ambient lighting require calibration each time to achieve optimal results. While the loss of red portions of the plant may not be a significant factor in subsequent processing, this should be taken into account. It may be the case that a single plant appears to be many more, as seen in \ref{fig:ndi}.
Even green portions of the vegetation may be green ``enough''. The segmented images in Figure \ref{figure:results} have discarded ground pixels while retaining most of the vegetated pixels that will be used is subsequent processing, but a close examination reveals that pixels in the stems of the weed in the center are also eliminated, as they are less green than the rest of the plant. Likewise, immature vegetation where stems are not sufficiently green will not appear in the final image.


\begin{figure}[H]
	\centering
	\subfloat[Field view of Purslane]{\includegraphics[width=.3\linewidth]{figures/purslane.png}\label{fig:purslane-original}}
	\hfill
	\subfloat[Image processed with NDI]{\includegraphics[width=.3\linewidth]{figures/ndi-purslane.jpg}\label{fig:purslane-ndi}}
	\hfill
	% Bar chart produced with this command
	% python evaluate-masks.py -i c:\uofa\weeds\lib\testing\IMG_1133 -s c:\uofa\maricopa\corrected\2024-04-24\iphone-drip\IMG_1133.jpg -t d:\maricopa\masks\2024-04-24\IMG_1133-mask.jpg -l ..\jetson\logging.ini
	\subfloat[Error rates]{\includegraphics[width=.3\linewidth]{figures/segmentation-error-rates-color.png}\label{fig:error-rates}}
	\caption[Missing red portions of vegetation]{The red portions of the vegetation are missing after the index has been applied, as \ref{fig:purslane-ndi} shows. While the absence of stems may not affect further image processing using factors like leaf texture in classification, the absence of the red portions of features such as leaves may complicate attempts to use factors that are affected such as shape. As \ref{fig:error-rates} shows, the presence of red in a significant portion of the vegetation has a negative impact on the error rates of the algorithms examined}
	\label{fig:segmentation-color-problem}
\end{figure}

\subsection{Classification Based}
\label{section:classification}
A different approach is to classify each of the pixels in an image as belonging to one of two classes: vegetation or ground (This is often referred to as \textit{semantic segmentation}) Table \ref{table:ml-segmentation} shows the results of building models using the following approaches:  Support Vector Machine, Linear Discriminant Analysis, and Multi-layer Perceptron (MLP)\footnote{These implementations were provided in this \textit{scikit-learn} Python package.}. Each technique was applied to these color spaces: RGB, YIQ, YUV, HSI, HSV, YCbCr, and CIELab. A set of images taken from 2M AGL was selected and 20 samples taken from each of the 58 images, 10 ground points and 10 vegetation points. The term \textit{samples} in this context means that for each of the points sampled, the values for each of the color spaces identified in Section \ref{section:classification} were recorded. Based on these sample points, models were created and trained on 60\% of the data using the Scikit-learn Python library. While the RGB color space is commonly encountered, using it did not produce the best results for any of the techniques examined. That distinction tended to be -- but not always was -- the YIQ space. There are some important exceptions to keep in mind, particularly the performance of LDA, where identical scores for both RGB and YIQ spaces are seen. Using these classification approaches is not particularly pragmatic from a realtime perspective, as classifying every pixel in a modest 12MP image can take several hours on a modestly powerful CPU.
%\footnote{An approach optimized for a GPU would, no doubt, be much faster, but that is an exercise that is beyond the scope of this document}

%
% Begin Copied Table from this command
% python segment.py -i images -o segmented -t ../util/segmentation-training.csv  -b -s -l logging.ini -a all
% Minor manual edit: Remove leading space from header lines

Table \ref{table:ml-segmentation} shows a summary of the three classification approaches in various color spaces, giving the Area Under the Curve (AUC), Precision, Recall, and F1 scores. And while the the scores reported are encouraging, the resulting segmented images are not.

% Spacing between the rows
\renewcommand*{\arraystretch}{1.1}

%
% Begin Copied Table from this command
% python segment.py -i images -o segmented -t ../util/segmentation-training.csv  -b -s -l logging.ini -a all
% Minor manual edit: Remove leading space from header lines & data


\begin{longtable}{llrrrr}
\caption[Machine Learning Segmentation]{Machine Learning Segmentation}
\label{table:ml-segmentation}\\
\toprule
Technique &  Color &  AUC & Precision & Recall &   F1\\
\midrule
\endfirsthead
\caption[]{Machine Learning Segmentation} \\
\toprule
Technique &  Color &  AUC & Precision & Recall &   F1\\
\midrule
\endhead
\midrule
\multicolumn{6}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
MLP &RGB & 0.93 &      0.88 &   0.95 & 0.91 \\
MLP &YIQ & 0.94 &      0.88 &   0.96 & 0.92 \\
MLP &YUV & 0.94 &      0.88 &   0.96 & 0.92 \\
MLP &HSI & 0.91 &      0.86 &   0.94 & 0.90 \\
MLP &HSV & 0.90 &      0.87 &   0.94 & 0.91 \\
MLP &YCBCR & 0.94 &      0.88 &   0.96 & 0.92 \\
MLP &CIELAB & 0.94 &      0.88 &   0.96 & 0.92 \\
LDA &RGB & 0.94 &      0.88 &   0.96 & 0.92 \\
LDA &YIQ & 0.94 &      0.88 &   0.96 & 0.92 \\
LDA &YUV & 0.94 &      0.88 &   0.96 & 0.92 \\
LDA &HSI & 0.91 &      0.86 &   0.95 & 0.90 \\
LDA &HSV & 0.90 &      0.86 &   0.95 & 0.90 \\
LDA &YCBCR & 0.94 &      0.88 &   0.96 & 0.92 \\
LDA &CIELAB & 0.94 &      0.88 &   0.96 & 0.92 \\
SVM &RGB & 0.94 &      0.89 &   0.96 & 0.92 \\
SVM &YIQ & 0.92 &      0.88 &   0.82 & 0.85 \\
SVM &YUV & 0.93 &      0.86 &   0.93 & 0.89 \\
SVM &HSI & 0.91 &      0.87 &   0.94 & 0.90 \\
SVM &HSV & 0.90 &      0.87 &   0.94 & 0.90 \\
SVM &YCBCR & 0.92 &      0.85 &   0.91 & 0.88 \\
SVM &CIELAB & 0.91 &      0.86 &   0.96 & 0.90 \\
\end{longtable}

% 
% End copied table
%

\subsection{Semantic Segmentation}
Semantic segmentation is a convolutional neural network (CNN) using a trained model to classify each pixel in an image with a class label.  For the most part, agricultural images contain only two things: plants and the ground. While there are exceptions to this, of course, as images may include debris on the ground, irrigation equipment, etc., but from another viewpoint, images contain pixels that are plants and pixels that aren't.  The focus here is not to actually classify what each pixel represents (a brocolli plant, a segment of pipe, the ground, etc.), but to produce a mask that can be applied to the image to isolate vegetated pixels. This approach differs from the prior two approaches in that while index-based approaches consider no information about other images, and learning base approaches consider samples of a set of images to predict class membership, the deep-learning techniques considered in this section are trained on both images and the corresponding masks.

\subsubsection{U-Net}
The name of the U-Net architecture derives from the U-shaped arrangement of the downard decoder section and the upward encoder section. Developed initially for the segmentation of bio-medical images \cite{Ronneberger2015-ye}, this technique has been more widely adopted into various use cases.

\begin{figure}[H]
	\centering
	\includegraphics[height=5cm]{figures/u-net-architecture.png}
	\label{fig:u-net}
	\caption[U-Net architecture]{The name of U-Net architecture refers to the arrancement of decoders (often referred to as a contracting path) and encoders (the expanding path)}
\end{figure}
This architecture has a bit of a limitation that must be taken into acount in image sets: the length of each image's axis must be a mutiple of 32. That is, a 32x32 image will work, but a 32x90 image will not. The images and masked used for training and testing should be resized to fit this limitation. Using a model trained on images and associated masks from the MAC to a set of test images yields fairly low error rate masks when  those masks are evaluated against the ground truth: FPR of 0.0005 and FNR of 0.06, comparable to the lower rates achieved with some index-based approaches.

\begin{figure}[H]
	\centering
	\subfloat[Ground truth mask]{\includegraphics[width=.3\linewidth]{figures/IMG_1115-mask.jpg}}
	\label{fig:unet-original}
	\hfill
	\subfloat[U-Net mask]{\includegraphics[width=.3\linewidth]{figures/IMG_1115-mask-unet.png}}
	\label{fig:unet-mask}
	\hfill
	\subfloat[Apply U-Net mask]{\includegraphics[width=.3\linewidth]{figures/IMG_1115-original-masked.png}}
	\label{fig:unet-original-masked}
	\caption[U-Net segmentation vs ground truth]{The mask produced with U-Net shows some errors within the leaf, but is quite close to a match for the ground truth.}
	\label{fig:u-net-segmentation}
\end{figure}


\section{Blob Identification}
Segmented images yield images with only vegetated pixels. Single items within those images are often called (somewhat generically) \textit{blobs}, and as that is the term generally used by the OpenCV software libraries used for image processing, that is the term adopted for this document. Blobs are the largest area of contiguous pixels with non-zero values.  The mask produced in the previous section is quite close to being suitable for the purposes of masking out non-vegetated pixels while allowing vegetated pixels of the original image, but requires some further refinement with morphological operations \textit{dilation} and \textit{closing} before it can be used, as the mask produced using only the index tends to hide portions of the vegetation.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.48\textwidth}
	  \centering
	  \includegraphics[height=5cm]{./figures/mask-raw.jpg}
	  \caption{Unrefined mask}
	  \label{fig:mask-raw}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{.48\textwidth}
	  \centering
	  \includegraphics[height=5cm]{./figures/mask-processed.jpg}
	  \caption{Mask refined with pixel dilation and closing}
	  \label{fig:mask-processed}
	\end{subfigure}
	\caption[Mask before and after morphological refinements]{The unprocessed and refined mask. The unrefined mask in Figure~\ref{fig:mask-raw} would not allow for many of the details of the plant to be present in the final image. Note that large portions of some of the leaves are rendered as black, even as the vegetation edges are not. After the pixels are dilated with a 5x5 kernel of all 1s, the mask shown in Figure~\ref{fig:mask-processed} is produced, allowing all pixels within the leaf area to be present in the final image.}
	\label{fig:mask-before-and-after}
\end{figure}


Blobs are identified as an object requiring classification, but not yet classified. Figure~\ref{fig:original-masked} is an example of this concept. This image shows eight blobs that can be classified: three lettuce plants and a single weed plant that appears as five plants in the final image.

\section{Feature Extraction}
Once the vegetation is isolated and blobs identified, various features can be extracted for subsequent use in classification. These features include position-independent characteristics such as shape, hue, and textural characteristics as well as position-dependent features such as where a plant resides relative to the crop-line and size ratios between objects.  

\subsection{Shape Characteristics}
The shape of an object can be globally or locally described by various factors \parencite{Zhang2004-cm}. The problem quickly encountered in using shape in this instance is that a 3D object is projected on to a 2D plane, losing information in the process. The object in the resulting image is, of course, only a partial representation of the object as it exists in the real world. As section \ref{problem-overlap} shows in greater detail, even simple, but common occurrences such as object overlap complicate attempts to exploit shape. The classification examined in this paper will consider only the former, global descriptors that apply to an entire object. Shape parameters used in classification must exhibit three characteristics: scale invariance, rotation invariance, and translation invariance. Rotation invariance is, perhaps, the most relatable of these. While a camera mounted on a UAV or tractor will collect imagery in the same orientation, plants are not so obliging in their growth. Two plants might have leaves that may look extremely similar (and could be said to be of the same shape), but are rotated by 90\si{\degree}. Scale invariance is almost as easy to describe: two leaves have the same shape, but are simply of different size due to differences in the time of emergence. Translation invariance means that the same descriptor is derived without considering the position of the object within the image.  A fourth invariance is a bit more complex: viewpoint. While a camera capturing images as it passes overhead can always -- within a very small variation -- be considered to have the same viewpoint, the same does not hold true for the plants in the image. Two leaves can have the same physical shape, but differences in orientation with respect to the Z-axis (consider the case where the tip of one leaf is closer to the ground) produce significantly different descriptions of their shape. While a perfect shape descriptor would be invariant under all of these, it is typically not realistic to arrive at a shape descriptor that is invariant in all cases.
The shape parameters considered in this analysis are expressions describing the perimeter of the plant. The shape formulae used to extract the various shape features are given in Table~\ref{tab:shape-formulae}, but some concepts and formulae warrant further explanation.

\subsubsection{Shape Basics}
Before delving into some of the specifics of describing a shape, it may be necessary to discuss two topics that will appear is some of the details that follow: \textit{perimeter}, and \textit{major$/$minor axis}. The perimeter of an object, in this context, consists of the set of pixels that form the boundary of an object. An example perimeter is shown in \ref{fig:bounding-box}. The blue pixels following the edge of the plant defines the perimeter. While the perimeter is a concept that is probably most familiar to a wide audience, the major and minor axes are probably not. The major axis is the longest line that can be drawn through an object. If two objects of equal area are considered, one circular and one oval, the oval will have a longer major axis. The minor axis is considered to be the longest line that can be drawn perpendicular to the major axis.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{./figures/shape-major-minor.png}
	\caption[Major/Minor Axis]{The major/minor axis of an object are illustrated by this simple example using an oval. The major axis is the longest line that can be drawn though the oval, and the minor axis is the longest line that can be drawn perpendicularly though that axis.}
	\label{fig:major-minor}
\end{figure}


\subsubsection{Bounding Box and Convex Hull}
An object can be said to have rectangular box of minimum size that completely encloses the object, commonly termed the \textit{bounding box}. The box placed around the object without regard for orientation -- that is, the placement makes no attempt to align the edges of the box with the edges of the image, so it may appear tilted at normal viewing angles. The \textit{convex hull}, likewise, is a convex shape with straight lines and of minimum size that completely encloses the object.   
\begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.48\textwidth}
	  \centering
	  \includegraphics[height=5cm]{./figures/shape-bounding-box.jpg}
	  \caption{A bounding box}
	  \label{fig:bounding-box}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{.48\textwidth}
	  \centering
	  \includegraphics[height=5cm]{./figures/shape-convex-hull.jpg}
	  \caption{A convex hull}
	  \label{fig:convex-hull}
	\end{subfigure}
	\caption[Bounding box and convex hull]{A bounding box and a convex hull (both noted in green) surround an object, but a convex hull is a shape with the minimum size that can contain the object. A bounding box is the minimum sized rectangle that can contain the object.}
	\label{fig:bounding-and-hull}
\end{figure}
The convex hull is a simple polygon whose vertices are a subset of the points found in the edge of the original shape. A widely used algorithm for determining the convex hull, \textit{Graham's Scan} is fairly efficient, $\mathcal{O}(n\log n)$, as \citeauthor{Klette2004-qz} show in this excerpt (altered for clarity with assistance from lecture notes from \citeauthor{Prosser1998-mi}). \parencite{Klette2004-qz}
\begin{quote}
\begin{enumerate}
	\item{Find the bottom-most point by comparing \textit{y} coordinate of all points. If there are two points with the same \textit{y} value, then the point with smaller \textit{x} coordinate value is considered. Let the bottom-most point be $P0$. Put $P0$ at first position in output hull.}
	\item{Sort the remaining points $p_i$ of $S$ in order of increasing angles $\eta_i$; if the angle is the same for more than one point, keep only the point furthest from $p$. Let the resulting sorted sequence of points be $q_i,..., q_m$.}
	\item{Initialize $C(S)$ by the edge between $p$ and $q_1$.}
	\item{Scan through the sorted sequence. At each left turn, add a new edge to $C(S)$; skip the point if there is no turn (a collinear situation); backtrack at each right turn.}
\end{enumerate}
\end{quote}
The most salient problem with the convex hull, however, is one that will have an adverse affect on all shape descriptors: distortions introduced by the resolution of the same object at different altitudes. That is, the edges are described by fewer pixels as the distance between the camera and plant increases, resulting in increased pixelation. While this can be minimized to some extent by blurring the image prior to performing the hull calculations, the hull is still distorted by this lower resolution.

\subsubsection{Area \& Size Ratio}
The area of the plant in the segmented image is simply the number of pixels with non-zero values. The term \textit{area} in this context is considered only in terms of pixels, not how much the pixel captures of the ground (e.g., the $cm^2$ each pixel represents). The relative area of two plants can then be compared to predict membership in a class. For instance, if a plant is $\frac{1}{3}$ the size of the largest plant in the image it is more likely than not a weed. This ratio is highly dependent on the development phase of the crop, as the reverse could be true for crop just emerging compared to much larger weeds. For the classification purposes presented here, the larger item is presumed to be crop.

% Figure produced with this command
% make weeds-no-db INPUT=/cygdrive/d/dissertation/ OUTPUT=/cygdrive/d/dissertation/output/ TRAINING=/cygdrive/d/maricopa-test/imbalance/processed/final/corrected.csv THRESHOLD=TRIANGLE INDEX=com2 DECORATIONS=area SESSION=shape
\begin{figure}[h!]
	\centering
	\includegraphics[height=4cm]{./figures/shape-area.jpg}
	\caption[Shape attribute: Area]{The area of the object in terms of numbers of pixels. By itself, this is not a particularly useful metric, but it may be useful to consider the relative sizes of plants to each other. As can be seen in this example, the crop plant is several times the size of the weed. This is, of course, highly dependent on the phase of development and how long the weed has been allowed to grow.}
	\label{fig:shape-area}
\end{figure}

\subsubsection{Length/Width Ratio}
\label{sec:width-length-ratio}
The ratio of width to length is not -- as the name might imply -- a simple ratio of two measurements, but is expressed as:
\begin{equation}
S = 
	\begin{bmatrix}
	Var(X) & Cov(XY) \\[0.3em]
	Cov(XY) & Var(Y) \\[0.3em]
	\end{bmatrix},
\lambda = \frac {eig_{1}(S)} {eig_{2}(S)}
\end{equation}
Where $eig_{1}(S)$ and $eig_{2}(S)$ are the maximum eigenvalues of the matrix $S$, with $\lambda$ representing the ratio. (\cite{Lin2017-xq}) The X and Y positions refer to an object's perimeter -- the boundary between an object's edge and the background.

% Figure produced with this command
%  make weeds-no-db INPUT=/cygdrive/d/dissertation/ OUTPUT=/cygdrive/d/dissertation/output/ TRAINING=/cygdrive/d/maricopa-test/imbalance/processed/final/corrected.csv THRESHOLD=TRIANGLE INDEX=com2 DECORATIONS=lw_ratio SESSION=shape

\begin{figure}[h!]
	\centering
	\includegraphics[height=4cm]{./figures/shape-lw-ratio.jpg}
	\caption[Shape attribute: Length-Width ratio]{The length-width ratio of plants. Note that the length-width ratio of the plant on the left of the image cannot be determined, as it is cut off, a common occurence in images.}
	\label{fig:shape-area}
\end{figure}

\subsubsection{Boundary Descriptors}
Once again focusing on the edge of an object, a global descriptor of the shape is the \textit{k-curvature} of the edges, but arriving at this global descriptor involves a few preliminary steps. Changes in the $(x,y)$ position of edge pixels can be described by a \textit{chain code}. A chain code representation of a boundary describes unit length changes in the $(x,y)$ positions of the boundary,  where changes in the position are noted by their position in a matrix:

 \begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.4\textwidth}
	  \centering
	  \includegraphics[height=3cm]{./figures/chain-code-matrix.jpg}
	  \caption{Chain code matrix}
	  \label{fig:chain-code-matrix}
	\end{subfigure}
	%\hfill
	\begin{subfigure}[h]{.4\textwidth}
	  \centering
	  \includegraphics[height=3cm]{./figures/chain-code-cells.jpg}
	  \caption{Example edge}
	  \label{fig:chain-example}
	\end{subfigure}
%	\hfill
%		\begin{subfigure}[h]{.25\textwidth}
%	  \centering
%	  \includegraphics[height=2cm]{./figures/chain-code-result.jpg}
%	  \caption{Result}
%	  \label{fig:chain-result}
%	\end{subfigure}
	\caption[Boundary chain codes]{Chain codes of the transitions of an object's edge -- as seen in successive pairs of coordinates -- are described by positions within an 8 position matrix, with each position noting the change in the $(X, Y)$ coordinate (steps of length 1 along the $x,y$ axis lines and of length $\sqrt{2}$ for diagonal movements). In this example the edge has a chain code of 2,1,0,7,7,0,1,1. While this example shows an open curve with a distinct beginning and ending, the boundary of a plant is a closed curve with no distinct end or beginning. A 4 direction matrix is is also used, as is a general $N$-directional chain $(N > 8\ and\ N =2^k)$, although those variants are not addressed in this document. The 8 position matrix is, perhaps, the most relatable, as changes in adjacent boundary pixels always take one of the 8 possibilities present in the matrix. The matrix shown in \ref{fig:chain-code-matrix} is a $3x3$ matrix, but the middle position (implying no change) is not possible -- this is simply an illustration.}
	\label{fig:chain-codes}
\end{figure}

The \textit{$\kappa$-slope} of a given boundary point $(X_i, Y_i)$ is estimated from the slope of the line joining positions $(X_{i-k/2}, Y_{i-k/2})$ and $(X_{i+k/2}, Y_{i+k/2})$ with this equation:
\begin{equation}
\tan^{-1}\left(\frac{Y_{i+k/2} - Y_{i-k/2}}{X_{i+k/2} - X_{i-k/2}}\right)
\end{equation}
 
 The \textit{$\kappa$-curvature} of the boundary at location $(X_i,Y_i)$ can be estimated by this equation:
 \begin{equation}
 \tan^{-1}\left(\frac{Y_{i+k} - Y_{i}}{X_{i+k} - X_{i}}\right) - \tan^{-1}\left(\frac{Y_{i} - Y_{i-k}}{X_{i} - X_{i-k} }\right)
 \end{equation}
 The goal of this metric is to assess the concavity or convexity of points along the perimeter. Consider the weed in this image with the computed $\kappa$-curvature:
 
  \begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.4\textwidth}
	  \centering
	  \includegraphics[height=4cm]{./figures/for-curvature-blob-1.jpg}
	  \caption{Weed}
	  \label{fig:curvature-weed}
	\end{subfigure}
	%\hfill
	\begin{subfigure}[h]{.4\textwidth}
	  \centering
	  \includegraphics[height=4cm]{./figures/curvature-blob-1.jpg}
	  \caption{$\kappa$-curvature}
	  \label{fig:curvature-plot}
	\end{subfigure}
	\caption[Example of $\kappa$-curvature]{The $\kappa$-curvature of various points around the perimeter of the weed are illustrated here. Positive values are indicative of convex shapes. Likewise, negative values indicate concave shapes.  While it is tempting to relate the values shown in the graph to the image displayed (as the figure invites the user to do), exact correlation is not advised, as the points sampled may not match the finer details that can be seen in the leaf. This example uses a relatively coarse sampling. However, the image does show a pronounced curve at the lower edge that can be seen in the curvature plot.}
	\label{fig:curvature}
\end{figure}
This leads to a global descriptor used in classification: \textit{bending energy}. Bending energy is typically described as the energy required to bend a rod to the shape under consideration, and is expressed as the sum of squares of the $\kappa$-curvature over the boundary length $L$.
\begin{equation}
\frac{1}{L} \sum_{p=1}^{L}\kappa(p)^2
\end{equation}
While there will be no bending of rods required for classification, this global metric can be used to express the overall shape of the perimeter, exploiting the fact that weeds in a crop and the crop itself exhibit different shapes.

% Figure produced with this command
%  make weeds-no-db INPUT=/cygdrive/d/dissertation/ OUTPUT=/cygdrive/d/dissertation/output/ TRAINING=/cygdrive/d/maricopa-test/imbalance/processed/final/corrected.csv THRESHOLD=TRIANGLE INDEX=com2 DECORATIONS=bending SESSION=shape

\begin{figure}[h!]
	\centering
	\includegraphics[height=4cm]{./figures/shape-bending.jpg}
	\caption[Shape attribute: Bending Energy]{The bending energy}
	\label{fig:shape-bending}
\end{figure}

The aspects of the \textit{radial distance} can also be used to describe the shape. \citeauthor{Kilday1993-aq}, in a study of classification of lesions in mammogram images, describe techniques to exploit the radial distance of a shape \parencite{Kilday1993-aq}. Before stepping through those techniques, it may prove useful to step back a bit and look at radial distances of an object.
\begin{figure}[H]
	\centering
	\includegraphics[width=.25\linewidth]{./figures/radial-distance.jpg}
	\caption[Radial distance from object centroid]{The radial distance from object centroid is noted by \textit{d} in this figure. Scale invariance is achieved by expressing functions of the normalized distance (using the maximum radial distance of the object)}
	\label{fig:radial-distance}	
\end{figure}
As Figure \ref{fig:radial-distance} shows, the radial distance referred to is the distance from an object's centroid (geometric center) to its perimeter. To make this distance scale invariant, the normalized distance to the maximal distance is used. Using the normalized radial distance, the mean and standard deviation is given by these equations for the $i^{th}$ blob:
\begin{equation}%\labeleqn{Radial distance mean and standard deviation}
\bar{x}_i = \frac{1}{N}\sum_{k=1}^{N}r_k \qquad
\sigma_i = \sqrt{\frac{1}{N}\sum_{k=1}^{N}({r_k} - \bar{x}_i)^2}
\end{equation}
By themselves these metrics may not be particularly useful in predicting class membership, however these provide useful information for metrics that may be. The number of times the normalized radial distance crosses the mean, for instance, provides an estimate of the roughness of a perimeter.


% Begin table of shape equations
\begin{longtable}{x{\dimexpr.15\columnwidth-2\tabcolsep}
                  x{\dimexpr.425\columnwidth-2\tabcolsep}
                  x{\dimexpr.425\columnwidth-2\tabcolsep}}
%\begin{hyphenrules}{nohyphenation}
    \caption{Shape Features}\label{tab:shape-formulae}  \\
\toprule
{\textbf{Feature}} & {\textbf{Formula}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endfirsthead
%%%%
    \caption{Shape Features (cont.)}\label{tab:shape-formulae}  \\
\toprule
{\textbf{Feature}} & {\textbf{Formula}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endhead
%%%%
\midrule[\heavyrulewidth]
\multicolumn{3}{r}{\footnotesize\itshape
                   Continued on the next page}
    \endfoot
%%%%
\bottomrule
    \endlastfoot
%%%%
		Perimeter
		& \begin{minipage}[t]{0.3\textwidth}
			$\sum_{i=1} ^{N-1}\left|X_1 - X_{i+1}\right| $
		   \end{minipage}     
		& Count of pixels forming the boundary of an object
\tabularnewline\addlinespace

		Major/Minor Axis     
		& $\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $                    
		& Major axis is the longest line that can be drawn through the object. Minor axis is the longest line that can be drawn through the object such that the line remains perpindicular to the major axis.
\tabularnewline\addlinespace

		Compactness      
		& \begin{minipage}[t]{0.3\textwidth}
			$\frac{4\pi * area}{(perimeter)^2}$ 
		   \end{minipage}
		& Ratio of an object's area to the area of a circle having the same perimeter 
\tabularnewline\addlinespace

		Elongation      
		& $\frac{width_{bounding}}{length_{bounding}}$ 
		& Ratio of the width of the object's bounding box to its width
\tabularnewline\addlinespace

		Eccentricity      
		& $\frac{length_{minor}}{length_{major}}$
		& Ratio of the minor to major axis
\tabularnewline\addlinespace

		Convexity   
		& $\frac{convex~perimeter}{perimeter}$ 
		& Ratio of the convex perimeter to the perimeter.
\tabularnewline\addlinespace

		Solidity    
		& $\frac{area}{convex~area}$ 
		&  Ratio of an object's area to its convex area. 
\tabularnewline\addlinespace

		Circularity    
		& $\frac{4\pi * area}{convex~perimeter}$ 
		& Ratio of an object's area to its convex perimeter. 
\tabularnewline\addlinespace


		Shape Index    
		& $\alpha = \frac {e} {4 \sqrt{A}}$ 
		&  Relationship between an object's perimeter and its area
\tabularnewline\addlinespace

		Width/Length Ratio
		& \begin{minipage}[h]{0.10\textwidth}
			\begin{eqnarray*}
				S = 
				\begin{bmatrix}
					Var(X) & Cov(XY) \\[0.10em]
					Cov(XY) & Var(Y) \\[0.10em]
				\end{bmatrix}
			\lambda = \frac {eig_{1}(S)} {eig_{2}(S)}
			\end{eqnarray*}
		  \end{minipage}
		& See section~\ref{sec:width-length-ratio}
\tabularnewline\addlinespace

		Bending Energy    
		& $\frac{1}{L} \sum_{p=1}^{L}\kappa(p)^2$ 
		&  The energy required to bend a rod to the perimeter shape
\tabularnewline\addlinespace

\label{table:shape-formulae}
\end{longtable}
}

% End table of shape equations

%
% C O L O R
%
\subsection{Color}
Color presentations of an image are ubiquitous, and while the Red-Blue-Green (RGB) color space is widely familiar, an image can be represented in other color spaces. RGB representation is nothing more than three channels, each of which note (respectively) the levels of Red, Green, and Blue in the image.  Examining each of these channels in isolation usually results in an odd -- but fairly realistic -- view of an object. Other color spaces do not alter the image itself, just the representation of the colors found in the image.  Choosing to represent the amounts of red, green, and blue in an image is just as arbitrary as other mechanisms, however closely aligned it is with human color perception. Other schemes provide somewhat equivalent -- but typically superior -- encodings of the colors found in the image. The representations are, most often, superior in some way, perhaps either in the range of colors represented or the efficiency of that representation. Each of the representations may have an advantage over the RGB model, but those advantages are of no concern here. Classification will use both components of those representations (i.e., the mean of an object's \textit{hue} in the HSI space) as well as the data contained within those components being used in other techniques (GLCM, for instance). All of the color representation schemes that follow all follow the RGB model in one key respect: each represents an image using three channels. RGB represents the amount of green in the image in the G channel, while the HSV space represents the Saturation of the hue in the S channel. This characteristic is not something that will be exploited in classification, however. The representation itself may be superior in some aspect to other representations, but aspects of that representation that can be used for classification is the concern in this problem space. While viewing the vegetation in one of the components can be fairly seen as not accurately representing the same experience one would have in viewing the object in daylight or other presentations, these components often have information relevant to the task at hand. This section presents a brief overview of the color spaces used in classification.

\subsubsection{RGB}

% Figure generated with
% make plot-for-document X=1236 Y=1160 X2=1567 Y2=1141 SUBJECT=Weed BAND=rgb INPUT=/cygdrive/d/dissertation/20190118_124250.jpg  COLOR=rgb
% Change the SUBJECT to reflect the X,Y coordinates, as each figure requires two plots -- the two coordinates are just for the image
% python plot.py -x 1236 -y 1160 -x2 1567 -y2 1141 -i `cygpath -w /cygdrive/d/dissertation/20190118_124250.jpg` -s Weed -l logging.ini -c rgb -b 0 -b 1 -b 2 -t band --length 100
% This generates two files: transect-image.jpg and transect-plot.png -- rename the latter 

\begin{figure*}[h]
	\centering
	\subfloat[Original\centering]{\includegraphics[width=4cm]{./figures/transect-image.jpg}\label{fig:transect-original-rgb}}
	\hfill
	\subfloat[Weed\centering]{\includegraphics[width=6cm]{./figures/transect-plot-rgb-weed.png}\label{fig:transect-weed-rgb}}
	\hfill
	\subfloat[Crop\centering]{\includegraphics[width=6cm]{./figures/transect-plot-rgb-crop.png}\label{fig:transect-crop-rgb}}
	\caption[RGB Transects]{Transects of vegetation in the RGB color space. Figure \ref{fig:transect-original-rgb} shows the transect locations for the weed (\ref{fig:transect-weed-rgb}) and crop (\ref{fig:transect-crop-rgb}) Note the distinct change of the transect as it passes through the weed, where ground pixels are captured. Both of the transects capture a portion of the ground to the left of each plant.}
	\label{fig:transects-rgb}
\end{figure*}
	
\subsubsection{Hue-Centric colorspaces: HSI \& HSV}
The terms {\it color} and {\it hue} are often used interchangeably, and while this is mostly true, hue refers to the dominant color family. Black and white are colors, for instance, but they are almost never referred to as a hue. In this case, the image is converted to the Hue, Saturation, and Intensity {\it HSV} colorspace and the mean value for the hue of a blob's pixels is taken. The saturation of a color expressed by a component of the Hue, Saturation, and Intensity ({\it HSI}) colorspace and the mean value for the saturation of a blob's pixels is taken. (\cite{Forsyth2012-hy})

The Hue, Saturation, and Intensity (HSI) and the Hue, Saturation, and Value (HSV) color spaces are cylindrical-coordinate representations of the RGB color model.  These models, first developed in the 1970s, rearrange the RGB cube representation to be more perceptually relevant. Both models are closely related in that they share an common expression of hue and saturation. While it is a bit simplistic to state, and as the similarities in the name imply, the values for \textit{hue} and \textit{saturation} are identical for these two colorspaces, leaving focus on the \textit{intensity} and \textit{value} components as the unique values that will be analyzed. \parencite[p.~84]{Forsyth2012-hy} Somewhat confusingly, HSV is sometimes referred to as HSB (or Hue, Saturation, and Brightness); likewise, HSI is  is sometimes referred to as HSL (Hue, Saturation, and Lightness), but the two are identical for the most part. In this cylindrical representation, hue is expressed as the angular dimension, red (at 0\si{\degree}), passing through green (at 120\si{\degree}) and blue (at 240\si{\degree}) In each representation, the vertical axis comprises the neutral, achromatic, or gray colors ranging, from top to bottom, white at lightness 1 (value 1) to black at lightness 0 (value 0).

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{.45\textwidth}
		\centering
		\includegraphics[width=6cm]{./figures/HSV_color_solid_cylinder_saturation_gray.png}
		\caption{HSV/HSB Cylinder}
		\label{fig:hsv}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{.45\textwidth}
		\centering
		\includegraphics[width=6cm]{./figures/HSL_color_solid_cylinder_saturation_gray.png}
		\caption{HSL/HSI Cylinder}
		\label{fig:hsl}
	\end{subfigure}
	\caption[HSL and HSV color representations]{HSL and HSV color representations. (Creative Commons)}
	\label{fig:overlap}
\end{figure}

\begin{figure*}[h]
	\centering
	\subfloat[Original\centering]{\includegraphics[width=4cm]{./figures/transect-image.jpg}\label{fig:transect-original-rgb}}
	\hfill
	\subfloat[Weed\centering]{\includegraphics[width=6cm]{./figures/transect-plot-hsv-weed.png}\label{fig:transect-weed-hsv}}
	\hfill
	\subfloat[Crop\centering]{\includegraphics[width=6cm]{./figures/transect-plot-hsv-crop.png}\label{fig:transect-crop-hsv}}
	\caption[HSV Transects]{Transects of vegetation in the HSV color space. Figure \ref{fig:transect-original-rgb} shows the transect locations for the weed (\ref{fig:transect-weed-hsv}) and crop (\ref{fig:transect-crop-hsv}) Note the distinct change of the transect as it passes through the weed, where ground pixels are captured. Both of the transects capture a portion of the ground to the left of each plant.}
	\label{fig:transects-hsv}
\end{figure*}

\begin{figure*}[h]
	\centering
	\subfloat[Original\centering]{\includegraphics[width=4cm]{./figures/transect-image.jpg}\label{fig:transect-original-rgb}}
	\hfill
	\subfloat[Weed\centering]{\includegraphics[width=6cm]{./figures/transect-plot-hsi-weed.png}\label{fig:transect-weed-hsi}}
	\hfill
	\subfloat[Crop\centering]{\includegraphics[width=6cm]{./figures/transect-plot-hsi-crop.png}\label{fig:transect-crop-hsi}}
	\caption[HSI Transects]{Transects of vegetation in the HSI color space. Figure \ref{fig:transect-original-rgb} shows the transect locations for the weed (\ref{fig:transect-weed-hsi}) and crop (\ref{fig:transect-crop-hsi}) Note the distinct change of the transect as it passes through the weed, where ground pixels are captured. Both of the transects capture a portion of the ground to the left of each plant.}
	\label{fig:transects-hsv}
\end{figure*}


\subsubsection{YIQ}
The YIQ model of color was used by the (analog) NTSC color TV system. Later, digital transmission schemes used different color spaces, notably the YCbCr that will be addressed in more detail in a separate section. These schemes have a common approach: \textit{chrominance} (a color component) is added to a black and white image. In the YIQ model, $Y$ represents the luma information (black and white), with $I$ (in-phase) representing red-cyan contrast, and $Q$ (quadature) representing magenta-green contrast) the chrominance information.  The YIQ and YUV color spaces are variants of each other, as they follow the same basic representation, a luma representation with chrominance in two additional channels.  The U \& V values can be considered the X \& Y axes within the color space, while I \& Q are a second set of axes rotated by $33^o$. Different (and now obsolete) technologies used these encodings: YUV was used by PAL, and YIQ was used by NTSC.


The processing employed here is to convert the image to the YIQ color space and take the mean value for the $I$, or in-phase component for the blob's pixels. (\cite{MathWorks_undated-jg})
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{./figures/yiq.png}
	\caption[YIQ color space at $Y = 0.5$]{A view of the YIQ color space for $Y=0.5$. Of special note here is that the I and Q axes are normalized from -1 to +1. In the YIQ space, the Y component is used by black and white receivers (perhaps difficult to find nowadays), as it represents the Luma information. (Creative Commons)}
	\label{fig:yiq-space}
\end{figure}
The $I$ component is the feature of interest here, and conversion of RGB to YIQ is achieved with this transformation:
\begin{equation}
	\begin{bmatrix}
	Y \\[0.3em]
	I \\[0.3em]
	Q \\[0.3em]
	\end{bmatrix}
	\approx
	\begin{bmatrix}
	0.299 & 0.587 & 0.114 \\[0.3em]
	0.5959 & -0.2746 & -0.3213\\[0.3em]
	0.2115 & -0.5227 & 0.3112 \\[0.3em]
	\end{bmatrix}
	\begin{bmatrix}
	R \\[0.3em]
	G \\[0.3em]
	B \\[0.3em]
	\end{bmatrix}	
\end{equation}
Here we also see convenience and ubiquity of the RGB colorspace. More often than not, color spaces are expressed as transformation from the RGB values, a technique that will be used in this section.

As Figure \ref{fig:hue_vs_inphase} shows, the tight grouping of the mean hue of an object class (it is not particularly important, at this juncture, what class $0$ and $1$ actually are), while interesting, is not something that can be used to predict if a plant is a weed or crop.  The somewhat greater variation of the other class is, likewise, interesting, but not an aspect of the data that can be exploited in classification. The in-phase component of the YIQ space, however, demonstrates a fairly clean separation that can, as there is little overlap between the two classes.


\begin{figure}[H]
	\begin{subfigure}[h]{0.48\linewidth}
		\includegraphics[width=1\linewidth]{./figures/hsi-hue.jpg}
		\caption{Hue in HSI}
		\label{subfig:hue}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.48\linewidth}
		\includegraphics[width=1\linewidth]{./figures/yiq-in-phase.jpg}
		\caption{In-phase in YIQ}
		\label{subfig:in_phase}		
	\end{subfigure}%
	\caption[Prediction using hue in HSI and In-phase in YIQ]{While the grouping of hue values is certainly tighter in one group (Figure~\ref{subfig:hue}), the complete overlap between means that group assignments cannot be made with confidence. In contrast, the values of the in-phase component in the YIQ space (Figure~\ref{subfig:in_phase}) shows relatively clean separation between the two groups.}
	\label{fig:hue_vs_inphase}
\end{figure}

\begin{figure*}[h]
	\centering
	\subfloat[Original\centering]{\includegraphics[width=4cm]{./figures/transect-image.jpg}\label{fig:transect-original-rgb}}
	\hfill
	\subfloat[Weed\centering]{\includegraphics[width=6cm]{./figures/transect-plot-yiq-weed.png}\label{fig:transect-weed-yiq}}
	\hfill
	\subfloat[Crop\centering]{\includegraphics[width=6cm]{./figures/transect-plot-yiq-crop.png}\label{fig:transect-crop-yiq}}
	\caption[YIQ Transects]{Transects of vegetation in the YIQ color space. Figure \ref{fig:transect-original-rgb} shows the transect locations for the weed (\ref{fig:transect-weed-rgb}) and crop (\ref{fig:transect-crop-rgb}) Note the distinct change of the Luma line of the transect as it passes through the weed into the area where ground pixels are captured, but the Blue-difference and Red-Difference values remain relatively stable. Contrast this with what is shown in the transect of the crop, where the same sort of abrupt change is seen in the Luma, but the difference between the Blue-difference and Red-difference is more noticeable. Both of the transects capture a portion of the ground to the left of each plant.}
	\label{fig:transects-yiq}
\end{figure*}


\subsubsection{CIElab}
Often going by the somewhat awkward acronym L*a*b*, this colorspace was intended to approximate human vision, as the L component expresses  the lightness of an object, an attribute seen a more applicable to perception of an object's color than the RGB model of mixtures of the three underlying colors.  The A and B components contain data about green to red, and blue to yellow, respectively, the four unique colors humans can perceive. While the space is not completely uniform, it is often employed in describing small changes in color. That is, numeric changes correspond to perceptual changes. The CIElab color space was standardized in the mid-1970s but remains in widespread use today. \parencite{Wikipedia_contributors2023-xt}  The $A$ and $B$ axes are theoretically unbound, but are typically limited to some practical range, say +127 to -127. 

\begin{figure}[H]
	\centering
	\includegraphics[width=.35\linewidth]{./figures/cielab-colorspace.jpg}
	\caption[CIElab colorspace]{In the CIElab colorspace, positive values for $b$ indicate more yellow; likewise negative values for $b$ indicate more blue. The values for $a$ similarly indicate more red (positive) or more green (negative). This is often referred to as an \textit{opponent representation} (yellow-blue and red-green) (Illustration reproduced from Color Application Specialists)}
	\label{fig:cielab}	
\end{figure}

Perceptual uniformity and pragmatic implementation details aside, the use of this colorspace is motivated by some of its salient features, most notably the ability to capture small color changes. These small color changes are expected to be significant in distinguishing crop from weed.

\begin{figure*}[h]
	\centering
	\subfloat[Original\centering]{\includegraphics[width=4cm]{./figures/transect-image.jpg}\label{fig:transect-original-rgb}}
	\hfill
	\subfloat[Weed\centering]{\includegraphics[width=6cm]{./figures/transect-plot-cielab-weed.png}\label{fig:transect-weed-cielab}}
	\hfill
	\subfloat[Crop\centering]{\includegraphics[width=6cm]{./figures/transect-plot-cielab-crop.png}\label{fig:transect-crop-cielab}}
	\caption[CIELab Transects]{Transects of vegetation in the CIELab color space. Figure \ref{fig:transect-original-rgb} shows the transect locations for the weed (\ref{fig:transect-weed-rgb}) and crop (\ref{fig:transect-crop-rgb}) Note the distinct change of the Luma line of the transect as it passes through the weed into the area where ground pixels are captured, but the Blue-difference and Red-Difference values remain relatively stable. Contrast this with what is shown in the transect of the crop, where the same sort of abrupt change is seen in the Luma, but the difference between the Blue-difference and Red-difference is more noticeable. Both of the transects capture a portion of the ground to the left of each plant.}
	\label{fig:transects-cielab}
\end{figure*}

\subsubsection{YCbCr}
Often encountered for digital video, YCbCr encodes both luminance information ($Y$, black and white) and chrominance information ($Cb$, $Cr$). In that respect, of course, it is similar to other spaces separating luma from color encoding. By separating the component the human eye is most sensitive to, brightness ($Y$) from color ($Cb$ and $Cr$), the color components can be altered to achieve efficiencies in storage or transmissions that would not be possible where each and every pixel must be represented by the levels of Red, Green, and Blue.  The $Cb$ and $Cr$ components represent the difference between a reference value and the blue or red component, respectively. The JPEG image format exploits these efficiencies in creating compressed images. While the efficiencies of the representation are not of interest in classification, the color representation is. This color space is similar to another encoding commonly encountered in digital video, YUV. The two spaces are nearly identical; YCbCr is a scaled version of YUV (used in analog broadcasts) This color space is also referred to by the somewhat shortened abbreviation YCC, although this document will continue to use YCbCr.

The ECMA report on the JPG file format gives this transformation for RGB to YCbCr:
\nocite{Ecma2019-yo}
\begin{equation}
	\begin{bmatrix}
	Y \\[0.3em]
	C_b \\[0.3em]
	C_r \\[0.3em]
	\end{bmatrix}
	\approx
	\begin{bmatrix}
	0.2126 & 0.7152 & 0.0722 \\[0.3em]
	-0.1146 & -0.3854 & 0.5 \\[0.3em]
	0.5 & -04542. & -0.458 \\[0.3em]
	\end{bmatrix}
	\begin{bmatrix}
	R \\[0.3em]
	G \\[0.3em]
	B \\[0.3em]
	\end{bmatrix}	
\end{equation}

\begin{figure*}[h]
	\centering
	\subfloat[Original\centering]{\includegraphics[width=4cm]{./figures/transect-image.jpg}\label{fig:transect-original-rgb}}
	\hfill
	\subfloat[Weed\centering]{\includegraphics[width=6cm]{./figures/transect-plot-ycbcr-weed.png}\label{fig:transect-weed-ycbcr}}
	\hfill
	\subfloat[Crop\centering]{\includegraphics[width=6cm]{./figures/transect-plot-ycbcr-crop.png}\label{fig:transect-crop-ycbcr}}
	\caption[YCbCr Transects]{Transects of vegetation in the YCbCr color space. Figure \ref{fig:transect-original-rgb} shows the transect locations for the weed (\ref{fig:transect-weed-rgb}) and crop (\ref{fig:transect-crop-rgb}) Note the distinct change of the Luma line of the transect as it passes through the weed into the area where ground pixels are captured, but the Blue-difference and Red-Difference values remain relatively stable. Contrast this with what is shown in the transect of the crop, where the same sort of abrupt change is seen in the Luma, but the difference between the Blue-difference and Red-difference is more noticeable. Both of the transects capture a portion of the ground to the left of each plant.}
	\label{fig:transects-ycbcr}
\end{figure*}


\subsection{Textural}
Textural descriptors of a object are formal expressions of the relationship pixel values have with one another, reducing a tactile experience to a numerical one. Common texture descriptions, bumpy, rough, smooth, etc., refer to a quality that is useful for a human description, but tends not to be particularly exact. Each of these descriptors, however, has a few attributes in common that can then be used to quantify them relative to a fingertip: the difference between high and low points, and the spacing of these high and low points. A smooth surface could be said to have low difference between high and low points, with small spacing. A rough surface would exhibit fairly large differences between high and low points with relatively wide spacing (at least compared to a human fingertip). While this list of descriptors is not comprehensive, and the additional quantification incomplete, this gives a sense of what is meant by a more formal texture description. Textural descriptors quantify (at least) two items: the neighborhood where a change is seen, and the direction of that change. Consider, for instance, the coat of a smooth-haired dog. Most of the hairs encountered would be aligned in the same direction, and in that direction the same texture would be encountered.
Classifying the texture is -- like color -- an analysis of the interior of the plant, and for this analysis three techniques will be used:  \textit{Grey-level Co-occurrence Matrix} (GLCM), \textit{Local Binary Pattern} (LBP) and \textit{Histogram of Oriented Gradients} (HOG). These approaches consider a pixel's relationship with surrounding pixels.

\subsubsection{GLCM}
In \textit{Grey-level Co-occurrence Matrix} (GLCM) (\cite{Haralick1973-gr}, \cite{Hall-Beyer2017-nx}), Haralick describes the analysis of an image that has been converted to greyscale values to note the relationship. Before delving into the details of the use of GLCM, it might be helpful to address what it is.

\begin{figure}[H]
	\begin{subfigure}[h]{0.28\linewidth}
		\includegraphics[height=3cm]{./figures/glcm-example.jpg}
		\caption{Sample image}
		\label{subfig:glcm_sample}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.28\linewidth}
		\includegraphics[height=3cm]{./figures/glcm-numeric.jpg}
		\caption{Numeric equivalent}
		\label{subfig:glcm_numeric}		
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.28\linewidth}
		\includegraphics[height=3cm]{./figures/glcm-matrix.jpg}
		\caption{Co-occurence matrix}
		\label{subfig:glcm_matrix}		
	\end{subfigure}%
	\hfill
	\caption[GLCM matrix explained]{The GLCM matrix concept is based on the contents of adjacent pixels. \ref{subfig:glcm_sample} shows a simple image containing only three values: 0, 1, and 2 -- as seen in a matrix of the numeric values shown in \ref{subfig:glcm_numeric}. A co-occurrence matrix is the count of the number of times adjacent pixels are found.  For instance, cell (0,0) contains a count of the times $0$ is adjacent to another $0$, $4$.}
	\label{fig:glcm-explained}
\end{figure}
As Figure \ref{fig:glcm-explained} shows, a co-occurrence matrix is simply a count of a specific value appearing adjacent to another value. However, this brings up two additional factors of what is meant by \textit{adjacent}: how far away a neighbor is, and what angle is considered when forming the count. In the simplistic example, an adjacent neighbor is considered to be directly adjacent (1 away) at an angle of $90^o$ degrees. Alternatively, a neighbor might be considered directly adjacent, but $135^o$ away.  With this in mind the (0,0) count would be 3, instead of the 4 shown in \ref{subfig:glcm_matrix}. This study considers pixels immediately adjacent to be neighbors (neighbor distance of 1), but ignores an inconvenient  complication: in very high resolution images, the immediately adjacent neighbor is, quite often, nearly identical the pixel in question. The angle of the relationship is a bit more complicated. While images of crops are often gathered in an orderly manner along a row such that orientation is maintained (as can be expected from a camera aboard a UAV or mounted on the back of a tractor), the vegetation is not so orderly. Otherwise quite similar plants are encountered in the field have markedly different texture descriptors depending on orientation. That is, different GLCM values for the same plant or obtained simply by rotating it by 45$^{\circ}$. That is, the texture description of two lettuce plants might differ markedly based on their growing orientation. Expressing an orientation-independent feature (texture) as differing with rotation may work well for an analysis of a single, static image set, but may yield sub-optimal results when applied to a general case where rotational differences are expected. \citeauthor*{Haralick1973-gr} discuss this, suggesting that the mean of calculations be used instead of the individual angles (calculations performed at $0^o, 45^o, 90^o, 135^o$, and $180^o$ -- angles greater than $180^o$ are not considered. Computations are made for each of the angles (0$^{\circ}$, 45$^{\circ}$, 90$^{\circ}$, 135$^{\circ}$, 180$^{\circ}$). The angular computations are then averaged to address the orientation of the plant. That is, the computations made at one orientation differ when the plant is rotated by $45^o$. In instances where leaves can be laid out and assessed all in the same orientation, this it not a concern, but in field scenarios where plants can grow in unpredictable orientations, it is.

Table~\ref{tab:glcm-formulae} details the subset that will be used in this analysis. The term \textit{grey-level} implies that the use of the co-occurrence matrix will be limited to greyscale images. Indeed, that is what \citeauthor{Haralick1973-gr} discuss in their article. Instead, the co-occurrence matrix will also be computed for each of the channels in the YIQ, RGB, CIELab, HSI, HSV, YCbCr colorspaces using the equations detailed in Table~\ref{tab:glcm-formulae}. For instance, the ASM calculation will be performed on the Cb channel of the image in YCbCr space. While co-occurrence matrices are used for channels in various colorspaces, this document will continue to reference the entire class as GLCM. 
\begin{longtable}{x{\dimexpr.15\columnwidth-2\tabcolsep}
                  x{\dimexpr.225\columnwidth-2\tabcolsep}
                  x{\dimexpr.625\columnwidth-2\tabcolsep}}
%\begin{hyphenrules}{nohyphenation}
    \caption{GLCM Formulae}\label{tab:glcm-formulae}  \\
\toprule
{\textbf{Feature}} & {\textbf{Formula}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endfirsthead
%%%%
    \caption{GLCM Features (cont.)}\label{tab:glcm-formulae}  \\
\toprule
{\textbf{Feature}} & {\textbf{Formula}} & {\textbf{Comment}}
\tabularnewline
\midrule
    \endhead
%%%%
\midrule[\heavyrulewidth]
\multicolumn{3}{r}{\footnotesize\itshape
                   Continued on the next page}
    \endfoot
%%%%
\bottomrule
    \endlastfoot
%%%%
		Homogeneity
		& \begin{minipage}[t]{0.3\textwidth}
			$\sum_{i} \sum_{j}\frac{c}{1 + \left|i-j\right|} $
		   \end{minipage}     
		& An expression of how much a pixel is similar to its neighbor
\tabularnewline\addlinespace

		Entropy     
		& $-\sum_{ij}c_{ij}\log_{2}(c_{ij}) $                    
		& An expression of how orderly the image is
\tabularnewline\addlinespace

		Correlation      
		& \begin{minipage}[t]{0.3\textwidth}
			$-\sum_{ij}\frac{(i-\mu_{i})(j - \mu_{j}) c_{ij}}{\theta_{i}\theta_{j}}$ 
		   \end{minipage}
		& An expression of the linear relationship between neighboring pixels 
\tabularnewline\addlinespace

		Dissimilarity      
		& \begin{minipage}[t]{0.3\textwidth}
			$-\sum_{ij}\frac{(i-\mu_{i})(j - \mu_{j}) c_{ij}}{\theta_{i}\theta_{j}}$ 
		   \end{minipage}
		& An expression of how much neighboring pixels differ 
\tabularnewline\addlinespace

		Contrast      
		& $\sum_{i}\sum_{j}{(i - j)}^2 c_{ij}$ 
		& Expresses the contrast between a pixel and its neighbor
\tabularnewline\addlinespace

		ASM      
		& $\sum_{ij}P{ij}^2$
		& Angular Second Moment -- high values are seen when the cell is very orderly
\tabularnewline\addlinespace

		Energy   
		& $\sqrt{ASM}$ 
		& The opposite of entropy
\label{table:glcm-formulae}
\end{longtable}

Figure~\ref{fig:glcm} illustrates an example of GLCM calculations in weed and crop images. While the clusters seen are not neatly divided, two clusters can be seen with the points sampled. This example plots the ASM and Homogeneity of greyscale images to illustrate the textural differences between plants.

\begin{figure}[H]
	\begin{subfigure}[h]{0.28\linewidth}
		\includegraphics[height=5cm]{./figures/glcm-crop.png}
		\caption{Lettuce}
		\label{subfig:glcm_crop}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.28\linewidth}
		\includegraphics[height=5cm]{./figures/glcm-weed.png}
		\caption{Pigweed}
		\label{subfig:glcm_weed}		
	\end{subfigure}%
	\hfill
	\begin{subfigure}[h]{0.28\linewidth}
		\includegraphics[height=5cm]{./figures/glcm-plot.png}
		\caption{ASM \& Homogenity}
		\label{subfig:glcm_plot}		
	\end{subfigure}%
	\hfill
	\caption[An example of GLCM calculations for weed and crop]{GLCM calculations derived from crop (\ref{subfig:glcm_crop}) and weed (\ref{subfig:glcm_crop}) show that the sampled points in weeds (green dots) and crop (blue dots) fall into two clusters. While this does not account for differences in lighting conditions or resolution between the two images, this illustrates the expected textural differences between the two plant types that are more easily seen (``weeds just look like they would feel different than crops''), than quantified. Crop image source: Dr. Mark Siemens, University of Arizona. Weed image source: University of California Agriculture and Natural Resources}
	\label{fig:glcm}
\end{figure}

While the term \textit{Grey Level} is often applied to this technique (and will be used throughout this document with the GLCM abbreviation), the technique will be applied not just to images converted from the RGB colorspace to greyscale, but to the various colorspaces previously discussed. The calculations will be made on each component of those spaces.  That is, each of the seven calculations are made for each component of the various color spaces (YIQ, HSI, HSV, YCbCr, and CIELab). 

These computations, referenced in subsequent sections, take the form \textit{$<$colorspace$>$\_$<$component$>$\_$<$angle$>$}, i.e., \textit{yiq\_i\_energy\_135} to indicate the energy computation at 135$^{\circ}$ of the in-phase component of the blob in the YIQ colorspace.

%
% Produced with this script:
% post/for-figure-glcm.R
% This script produces two graphs, glcm-pairs.pdf and hog-pairs.pdf in the jetson directory.
%

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{./figures/glcm-pairs.pdf}
	\caption[GLCM parameter correlation assessment]{A breakdown of the GLCM parameter calculations detailed in Table~\ref{table:glcm-formulae} for the sample images (greyscale). In contrast to the samples within an image demonstrated by Figure~\ref{fig:glcm}, these calculations are carried out across the entire plant. To negate the distortions seen with specific orientations of vegetation, these are averages across the entire plant, not specific angles. Some of the relationships are interrelated, so the data may look much more interesting than it actually is. \textit{Energy}, for instance, is the square root of \textit{ASM}, so no special significance should be attributed to that curve presented above. For the purposes of exploring the data, it is not of particular concern what class 0 and 1 represent.}
\end{figure} 

\subsubsection{Local Binary Pattern}
Often shortened to LBP, \citeauthor{Ojala1996-ps} first described Local Binary Patterns, concentrating on a pixel's immediate neighbors, and treating the result as a value that can be thought of as describing that central pixel. While first proposed as a texture descriptor, its use in more general computer vision problems has expanded since it was first introduced. One of the more salient advantages of LBP is that they are tend not to differ with illumination changes, as the descriptors are based on relative changes from a central pixel, not absolute. Color, in this approach, is ignored, as images are first converted to greyscale before being analyzed. However useful this approach is, it does have a hard limitation: only a 3x3 matrix can be used to produce the final value. This is more of a limitation than it first appears, as this technique is not suitable for capturing details at varying scales.

The basic algorithm for LBP is reasonably uncomplicated:
\begin{enumerate}
	\item{Choose a pixel in the image and select its neighboring pixels in a circular or rectangular region around it.}
	\item{Take the threshold (intensity of the selected pixel).}
	\item{Go through every neighboring pixel and check whether its intensity is greater than or less than the threshold.}
	\item{Assign 1 to the neighboring pixel, if the intensity of the neighboring pixel is greater than the threshold.}
	\item{Assign 0 to the neighboring pixel, if the intensity of the neighboring pixel is less than the threshold.}
	\item{Combine the binary values for all neighboring pixels to obtain a binary code for the central pixel.}
	\item{Repeat steps 10-6 for each pixel in the image to obtain a binary code for each pixel.}
\end{enumerate}

\begin{figure}[H]
	\begin{subfigure}[h]{0.48\linewidth}
		\centering
		\includegraphics[height=2cm]{./figures/lbp_thresholding.jpg}
		\caption{LBP Thresholding}
		\label{subfig:lbp-thresholding}	
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.48\linewidth}
		\centering
		\includegraphics[height=2cm]{./figures/lbp_calculation.jpg}
		\caption{LBP Calculation}
		\label{subfig:lbp-calculation}		
	\end{subfigure}%
	\caption[Local Binary Pattern thresholding and calculation]{To determine the LBP value of the central pixel (marked in red in \ref{subfig:lbp-thresholding}), the value is thresholded to produce the second matrix: where the original matrix has a value $<$ 4 the original value is replaced with a 0. Values greater than 4 are replaced by a 1. This result matrix is then used as the binary values of a single integer, 23 in this example. Considering a 3x3 matrix, there are $2^8$ possibilities. (Images reproduced from PyImageSearch)}
	\label{fig:lbp}
\end{figure}



LBP suffers from a few other disadvantages that must be taken into account. LBP is sensitive to noise in the image under consideration, yielding incorrect descriptors than have been ``contaminated'' with noise. Effective de-noising the images is essential in combating this. A more substantial shortcoming is that the relatively small sample size of a pixels immediate neighbors means that large scale texture may not be effectively captured. This shortcoming is exacerbated, of course, by extremely high-resolution images, where immediate neighbors are often not very different than a pixel under consideration. In contrast to the rotational problems seen with GLCM, LBP is rotation invariate. It is probably correct to view techniques such as GLCM as effective at capturing \textit{global} texture, and LBP more effective at describing \textit{local} texture.

To make this technique more useful, a variant of this is used, where a circular neighborhood is considered instead of a square. The radius of the circle allows consideration of features at different scales.  Subsets of an image can then be described by moving the square or circular neighborhood across the image.  Sampling in a circular pattern will mean that some samples will not be be between two pixels. In this case, the intensity value used for the comparison can be determined by bilinear interpolation. This sampling discrepancy between a pixel's center and the actual sampling point can be seen in the middle neighborhood diagram of Figure ~\ref{fig:lbp-circular}, where pixel centers and sampling points are not aligned in many instances.

\begin{figure}[H]
	\centering
	\includegraphics[height=2cm]{./figures/lbp_circular.jpg}	
	\caption[Local Binary Pattern circular neighborhoods]{The size of a neighborhood can be varied depending on the size of the detail to be characterized. The neighborhood on the left roughly corresponds to the 8 position square neighborhood seen in Figure \ref{fig:lbp} (Image reproduced from PyImageSearch)}
	\label{fig:lbp-circular}
\end{figure}

An introduction to LBP concepts would not be complete without addressing the notion of \textit{LBP Uniformity}. An LBP is considered to be uniform if it has at most two $0 \rightarrow 1$ or $1 \rightarrow 0$ transitions in the binary representation. The LBP pattern $01000000$ shows one $0 \rightarrow 1$ transition and one $1 \rightarrow 0$ transition, making it uniform. A pattern such as $01010000$ would not be considered uniform, as there are more than two transformations. Uniform transitions are featureless areas often termed \textit{flat}.

%
% Figures generated with:
% python lbp-graph.py -l ../documents/figures/example-weed.jpg -r ../documents/figures/example-crop.jpg -o ../documents/figures -radius 5
% Using make
% make lbp-figure LEFT=../documents/figures/example-weed.jpg RIGHT=../documents/figures/example-crop.jpg OUTPUT=../documents/figures

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=.4]{./figures/lbp-left.jpg}
		\caption{LBP of weed}
		\label{subfig:lbp-weed}	
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[scale=.4]{./figures/lbp-right.jpg}
		\caption{LBP of crop}
		\label{subfig:lbp-crop}		
	\end{subfigure}%
	\caption[Local Binary Pattern of a weed and crop plant histogram]{Local Binary Pattern of a weed and crop plant histogram. While the counts (note that the Y axis is log scale) show similar shapes, there are important differences between the two. The counts at the left and right edges of the histograms represent \textit{flat} areas -- regions where neighboring pixels are all white or all black (not surprising, given that the images are segmented, with black substituted for the background). The center peak seen in both images can be thought of as representing the \textit{edges} found in the images. The two dips seen in both graphs are typically referred to as \textit{corner} areas. While the shapes of the dips are similar, there are clear differences between the two. Graphs taken of $radius=5$.}
	\label{fig:lbp}
\end{figure}

\subsubsection{Histogram of Oriented Gradients}
Often shortened to \textit{HOG}, this technique is a technique to formalize the gradient orientation and magnitude of local portions of an image \parencite[p.~155]{Forsyth2012-hy}. This is a structural analysis that gives insight to not only the overall shape of the object, but to the interior details as well, as this technique is geared toward the identification of high-contrast edges within the object. Contrast this with the shape analysis detailed earlier in the document where the concern was limited almost exclusively to the perimeter of the vegetation.  As shape analysis considers only the edge of the blob, the vegetation is effectively a cartooned image, providing only areas of black and white. This technique considers small cells within an image, lowering the risk that a gradient with low contrast will be missed, as it will be compared (and normalized) within that small cell. While much of the literature on this topic focuses on describing an entire scene, that is not the case here, as the background has been eliminated and the object in question is known to be vegetation. The concept underlying this technique is that the distribution of gradient intensities across an object is a descriptor of that object. An object is analyzed in terms of a set of small, overlapping regions called \textit{cells}.
The detail shown in Figure~\ref{fig:hog} is still recognizable as vegetation, but illustrates gradient orientation of 16x16 pixels\footnote{The cell size can be varied, of course. This size was selected somewhat arbitrarily} cells of the image, not pixel values. It is these orientations that will be exploited in classification.  For the classification approach described in this document, the gradients will be reduced to a few numbers to describe each plant. Specifically the mean, standard deviation, and the range of the magnitudes of the gradient descriptors for a plant. In lettuce the orientation of a gradient is visible even to the unaided eye, as the vein structure can be used to classify the image \parencite{Elhariri2014-eo}. While detailed analysis of structures will not be directly used in classification, the overall effect of those structures will be used. That is, to say that leaf is ``very veiny'' or ``quite rough'' can be quantified with a few numbers that are reflective of the overall texture of the plant.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{./figures/hog.jpg}
	\caption[HOG Representation of Lettuce]{HOG Representation of greyscale lettuce image. This image represents not the actual pixels of the image, but is a representation of the gradients found within each 16x16 cell. The image on the left in the image above shows a detail portion of the overall image. Relatively uniform areas expose little of interest, but areas where features such as veins, where high-contrast edges are found expose areas that can be of interest.}
	\label{fig:hog}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{./figures/hog-pairs.pdf}
	\caption[HOG features used in classification]{Instead of considering the values of small cells contained within the perimeter of the blob, the classification will use attributes of that portion of the image. That is, the measurements shown in this figure are for the blob in question, not for the image as a whole. For the purposes of exploring the data, it is not of particular concern what class 0 and 1 represent.}
	\label{fig:hog-pairs}
\end{figure}



\subsection{Distance from Cropline}
The cropline in a planting is, simply, the line along the bed where crop can be expected. Under field conditions where images are acquired from a constant position (such as would be obtain from a towed system) the cropline will appear in the same spot in each photo. For classification purposes, the idea is obvious to anyone who has had the pleasure of performing weeding along a cropline: things along the cropline are likely to be crop, and things not along the cropline are likely to be weeds. Things are a bit more complicated than that, of course, and it is probably more accurate to use the terms \textit{desired} and \textit{undesired} to classify vegetation. Weeds may appear along the cropline or within close proximity to crop\footnote{The concept of a defining a buffer zone around crop to avoid damage during an negative treatment will be discussed later in this document}, and crop may appear in places other than the main cropline, making it \textit{undesirable}, but referring to it as a weed would lead to confusion.

Crops will most often have a distance from the cropline very close to zero. Weeds, on the other hand, may have a distance close to zero if they appear within the line of crop, but often appear far from the crop line.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\linewidth]{./figures/normalized-distance.jpg}
	\caption[Distance from Cropline]{Distance from Cropline. Vegetation far from the cropline are most often weeds. In this somewhat simplified example, the cropline is determined by considering the two largest plants in the image. The distance from each plant to this cropline is easily computed.}
	\label{fig:normalized-distance}
\end{figure}
Figure~\ref{fig:normalized-distance} illustrates the concept of a cropline and the distance of vegetation from it. In this image we see two growths of lettuce that are very close to the cropline (distances here are in pixels, but the units are not significant. This could be expressed in millimeters.) at 0 and 0.375 and a weed lying 0.4347 units from the cropline. The values normalized between 0 and 1 by considering the maximum distance the vegetation could be away from the crop line. The line marked \textit{Center Line} is for reference purposes and can be ignored for now.  There are two additional items that are worth noting about this image: the dots connecting the plant to the cropline are the {\it centroids} mentioned earlier, and the colored bounding boxes signify the class of the object, something we will return to in a later section.

While images obtained in close proximity to the planting beds may contain only a single cropline, images contained from higher distances AGL will often contain multiple croplines. In the former case, it is often the case that fairly simple metrics can be used to determine the cropline -- size ratios between vegetation.  The line connecting the two largest plants, for instance, is, more often than not, the cropline. This is the case, however, only for those images taken from very low distances above ground (40 cm or so) where the image contains only a single row. For imagery taken from higher distances AGL, a different approach is called for. \citeauthor*{Dian_Bah2017-kd} discuss the use of a Hough Transform \parencite{Illingworth1988-nw} to identify planting rows in AUV imagery; a technique that will be used here. \citeauthor{Ji2011-qb} analyze Hough transforms and \textit{random} hough transforms \parencite{Ji2011-qb} for three different plant densities, showing that the random hough transform was more computational efficient than a simple hough transform. Exploiting a hough transform to find the cropline would involve a bit of pre-processing that is fairly easy, replacing the vegetation with a small square occupying the center, and a caveat that is difficult to overcome, depending on the development phase, it may be difficult to determine the center of or point of ground emergence of vining plants (such as cantaloupe).

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.45\linewidth}
		\centering
		\includegraphics[width=6cm]{./figures/cantaloupe-early.jpg}
		\caption{Early stage of development}
		\label{subfig:cantaloupe-early}	
	\end{subfigure}
	\begin{subfigure}[h]{0.45\linewidth}
		\centering
		\includegraphics[width=6cm]{./figures/cantaloupe-late.jpg}
		\caption{Later stage of development}
		\label{subfig:cantaloupe-late}		
	\end{subfigure}%
	\caption[Early and later stages of vining plant development]{Early and later stages of plant development complicate identification of the crop row using a hough transform. While the vegetation in \ref{subfig:cantaloupe-early} can be easily replaced by a square centered on the plant, both the center and the point where the plant emerges from the ground is not easily obtained in the later phase shown in \ref{subfig:cantaloupe-late}.}.
	\label{fig:cantaloupe}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{./figures/hough-transform-rectangles.jpg}
	\caption[Rectangles for hough transform]{Rather than perform a hough transform on the raw image of the vegetation, each plant is first transformed into a series of rectangles, the center of which is the centroid of each plant. }
	\label{fig:hough-transform}
\end{figure}



\subsection{Overlapping Vegetation}
\label{problem-overlap}

Before considering the details of classification, it is worth considering a problem that is sometimes seen: overlapping vegetation. While vegetation can overlap itself (in that leaves may obscure other leaves in the same plant), this becomes problematic when desired and undesired vegetation overlap, creating the appearance of two plants being seen as one. Normal overlap within a plant's leaves renders edge detection ineffective in this instance -- that is, distinguishing between expected, normally occurring overlap within a plant's leaves, and overlap between plants is problematic. Overlapping vegetation is problematic in the current processing flow, as a plant is identified by the pixels within a contiguous  perimeter. In this case it is detected as part of the adjoining vegetation, distorting the calculations detailed in previous sections.  Consider this portion of an image:

\begin{figure*}[h]
	\centering
	\subfloat[Raw Image\centering]{\includegraphics[width=6cm]{./figures/overlapping-weed.jpg}}
	\hfill
	\subfloat[Segmented Image\centering]{\includegraphics[width=6cm]{./figures/overlapping-weed-segmented.jpg}}
	\caption[Crop and Weed overlap]{Crop and Weed overlap -- this image contains a problem commonly encountered, the situation where there is overlap between the crop and weeds. In this instance, the crop partially obscures the weed. The blob detection algorithm does not use factors such as color differences to distinguish one blob from another, resulting in two plants being processed as a single one, as can be observed in~\ref{fig:overlap-segmented}. As the weed has been falsely identified as belonging to the crop plant, its characteristics contribute to the overall calculations for the blob. Most notably, the shape is distorted, but this also affects color and texture calculations. While the weed in question in the image is probably too close to the crop to treat, it is nevertheless a problem for classification that will be addressed in a later section of this document.}
	\label{fig:overlap}
\end{figure*}


The object detection algorithms are based on shapes of binary images, leading to the case where a few leaves of the weed are incorrectly identified as part of the crop. In this case,some leaves are identified as separate plants and are marked as requiring treatment, but this may not be the case with a larger plant requiring treatment that happens to overlap with desired vegetation. This affects not only the inclusion of weeds with crop, but the misidentification of several weeds as a single plant, as this image demonstrates.  Two of the plants in the image are identified as a single blob as the overlapping leaves of multiple plants makes it appear as one. While this a mistake that will result in the distortion of metrics, the weed occupies a space so close to the crop that it would not be treated even if properly identified.

\subsection{Weed Shapes and Color}
There are two complications with the identification of weeds by considering shape.  Computation of the shape index of objects within the image are complicated by poor lighting requiring more pixel dilation to make the vegetation appear to be a single plant. This, in turn, distorts the computation of the shape index of the object. The indices covered in Table~\ref{table:segmentation} fail to capture elements of the vegetation that are other than the commonly encountered green.

Consider a sample of Purslane detailed in Figure~\ref{fig:segmentation-problem}. As the red stems are not detected by the green-centric index, the resulting segmented image appears to show multiple plants where there is only one.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.40\textwidth}
		\centering
		\includegraphics[width=6cm]{./figures/purslane.png}
		\caption{Purslane}
		\label{fig:purslane}
	\end{subfigure}
	\begin{subfigure}[h]{.40\textwidth}
		\centering
		\includegraphics[width=6cm]{./figures/purslane-segmented.jpg}
		\caption{Segmented image (NDI)}
		\label{fig:purslane-segmented}
	\end{subfigure}
	\caption[Color problems complicate segmentation]{The red stems found in Purslane complicate use of indices calculated to emphasize green vegetation. As seen in Figure~\ref{fig:purslane-segmented}, the prominent red stems are not present in the final image. The impact of this is that that multiple plants are identified where there is only one.}
	\label{fig:segmentation-problem}
\end{figure}

Even weeds that are predominantly green have portions that are less green than others. The color of the Purslane stem is a shade of red that is not isolated by considering only a single threshold of the vegetation indices discussed earlier in this document. While using a second threshold value to capture the stems is an option, this approach is complicated by the red content of the background -- the ground may have a significant red hue, a situation not frequently encountered with green vegetation. This problem is not unique to purslane, but can be seen in other vegetation with significant amounts of red in edges of leaves such as found in redroot pigweed (\textit{Amaranthus retroflexus}).  The presence of red in leaves mis-identification of leaves as separate plants (stems) and to the distortion of shape metrics (leaves). While this tends toward a negative impact on calculations that involve the entire plant, calculations involving only the leaves does not appear to have detrimentally affect classification.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[h]{.40\textwidth}
		\centering
		\includegraphics[width=6cm]{./figures/redroot-pigweed.jpg}
		\caption{Redroot pigweed seedling}
		\label{subfig:redroot-before}
	\end{subfigure}
	\begin{subfigure}[h]{.40\textwidth}
		\centering
		\includegraphics[width=6cm]{./figures/redroot-pigweed-COM1.jpg}
		\caption{After segmentation using COM1}
		\label{subfig:redroot-after}
	\end{subfigure}
	\caption[Red coloration within the leaves of redroot pigweed]{The red appearing in the leaves of the redroot pigweed plant (\ref{subfig:redroot-before}) are not present in the segmented image (\ref{subfig:redroot-after}), distorting the shape calculations. Those represent the edge of the plant, a feature that is now not representative of the actual edge. Textural and color calculations are not as affected to the same extent, as the red coloration is not as pronounced in the interior of the leaves. This does not mean that those calculations are unaffected, only that they now represent only the green portions of the plant.}
	\label{fig:redroot}
\end{figure}

\section{Manual Classification}
As the classification methods for the vegetation are supervised, producing a classified dataset is required. That is, a human must first mark up each entry in the set of identified vegetation to indicate the class (crop or weed) to which each belongs.  While the specific number of images will vary with the altitude, overlap, and ground area, the fairly small ground area imaged will result in a small set of images (the total for each collection is shown in Table \ref{tab:acquisition}), even those taken at altitudes as low as \SI{2.0}{\meter} AGL.

\begin{wraptable}{R}{4.5cm}
	\def\arraystretch{0.70}
	\caption{Image Counts}
 	\label{tab:acquisition}
 	\centering
 	\begin{tabular}[t]{ll} 
		\textbf{AGL} & \textbf{Images} \\
		\midrule
		      0.5m & 85  \\
		      1.0m & 65  \\
		      2.0m & 52 \\
		      5.0m & 6  \\
		      20cm & 20 
	\end{tabular}
\end{wraptable}


To allow for (relatively) painless class assignment for the images, a custom label application was developed\footnote{Development details are supplied elsewhere, but the application was developed in Python using the PyQt5 framework for GUI applications.} that presented each segmented image for manual classification. Each of the blobs in the image are assigned a class by the user. That is, a human establishes the ground truth of each plant by setting the ``correct'' type for each blob using the corresponding (highlighted) drop down. The end result of this is to produce a training set that can be used in subsequent algorithmic classification. Table \ref{tab:extract} shows a small sample. Here, the features for each blob are listed, but ignored. Only the type of each blob is set in this step -- 0 representing crop, and 1 representing weed.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.30]{./figures/review-application.jpg}
	\caption[Manual classification application screenshot]{Each blob is manually labeled with the correct class, allowing for the supervised classification approaches to classification presented later in this document to function and to assess accuracy. This application presented each image and initial assignment of the individual plants contained within that image, allowing the human expert to correct the initial assignment. Note that the initial assignment is merely for convenience -- the initial class assignment could just as well have been arbitrary. The correct classification is set by the user with the corresponding highlighted dropdown along the left-hand side of the screen. The output of this program is a CSV file that can subsequently be used to create a model. Table \ref{tab:extract} shows a shortened version of this, omitting most of the attributes -- the highlighted column is the only one that is actually changed by this manual classification.}
	\label{fig:review-application}
\end{figure}

\begin{tiny}
% Highlight the last column
\begin{longtable}{llllllh}
\caption[Vegetation Type after Manual Classification]{Vegetation Type after Manual Classification}\\
\toprule
All other & cie\_b\_contrast\_avg	 & cie\_b\_dissimilarity\_avg	& cie\_b\_ASM\_avg	& name & number & \textbf{type} \\
\midrule
\endfirsthead
\caption[]{Parameter Rankings} \\
\toprule
All other & cie\_b\_contrast\_avg	 & cie\_b\_dissimilarity\_avg	& cie\_b\_ASM\_avg	& name & number & \textbf{type} \\
\midrule
\endhead
\midrule
\multicolumn{5}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\label{tab:extract}
\endlastfoot

- &6301.011981218508	&49.71044205222172	&0.4685115397070943	&image-30-blob-0	&180	&0 \\
- &4175.090715594958	&41.10014962780084	&0.1362153347621408	&image-30-blob-1	&181	&0 \\
- &6156.428566816558	&49.36721780237519	&0.4390896935316736	&image-30-blob-2	&182	&0 \\
- &2488.164269406393	&29.97195585996956	&0.046253458	           &image-30-blob-3	&183	&0 \\
- &4975.8718916524285	&45.74685146509714	&0.2129808994576038 &image-30-blob-6	&186	&0 \\
- &3515.707728494623	&43.87426075268817	&0.028970925	           &image-30-blob-7	&187	&0

\end{longtable}
\end{tiny}

The final product of this manual classification is a collection of blobs found in the image with the shape, color, and texture attributes discussed in prior sections as well as the correct type for that blob, indicating if these attributes are for weed or a crop.
%
% I M B A L A N C E
%
\section{Class Imbalance}
Plantings often exhibit a somewhat inconvenient feature: weeds and crop do not appear in the same proportion. While having a low weed count is probably desirable for crop production, it is not for classification. While this can present itself as a relatively mild imbalance of nine weed plants for every 10 crop plants or a more extreme ratio of 1000 crop plants for every weed. In these extreme cases of imbalance, only a few samples of the minority class are used in classification -- and even more unfortunate case is that some weed species may not be present at all in the set used for training. \citeauthor{Fernandez2018-fw}, in a book discussing imbalanced datasets, detail several correction algorithms, among them SMOTE, ADASYN, Borderline SMOTE, kmeans, and SVM \parencite{Fernandez2018-fw}. These algorithms seek to address the imbalance by the creation of synthetic samples of the minority class. While the imbalance ratio of the training set will most assuredly deviate from the 1:1 ideal, it is not particularly important what the specific ratio is, as the data will be trimmed to reflect the various ratios. That is, while a dataset may have a relatively low imbalance of 10:8, samples of the minority class are discarded to achieve a much higher imbalance. In most cases. of course, the solution is to simply collect sufficient training data such that a severe imbalance does not exist, but this may not be a practical solution. This leaves two approaches: under-sample the majority or over-sample the minority. In cases where weeds (the minority) are relatively few, providing synthetic data to restore data to a 1:1 class ratio may be effective. There are various approaches that all have the same basic approach: generate data from the minority class that is similar -- but not identical -- to the data already present. That is, the minority class is over-sampled.

\subsection{Over-sampling}
\label{section:over}
\subsubsection{SMOTE}
The \textit{Synthetic Minority Oversampling TEchnique} is the basic technique used for rebalancing the data set -- other techniques in this section are extensions to that approach \parencite{Chawla2002-dk}. Synthetic data is generated by selecting examples that are close in terms of the \textit{k} nearest neighbors, and selecting new points along the line connected those peers. This technique is not without drawbacks worthy of consideration: it generates data in the same direction, a complication for classification in that the decision surface is distorted, and tends to generate noisy data. The synthetic data points do not exhibit the same variation of the underlying real data points, leading to an introduction of over-fitting.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35]{./figures/smote.png}
	\caption[SMOTE selection of synthetic data points]{Generating synthetic data points using the SMOTE algorithm considers the \textit{k} nearest neighbors ($N_{1..4}$) to a point under consideration ($X_1$).  The SMOTE algorithm identifies points along the lines connecting a point to its neighbors ($S_{1..5}$). These points are the synthetic samples used to correct the imbalance.}
	\label{fig:smote}
\end{figure}

\subsubsection{ADASYN}
The Adaptive Synthetic Sampling approach considers the distribution of the minority points, giving more emphasis to points that a harder to learn. \cite{He2008-xr} Points that are harder to learn are those that are close to a class border, and in that sense, this approach is close to the \text{borderline} proposal. Contrast this with SMOTE. While sharing the same basic approach (considering the $k$ nearest neighbors), SMOTE samples the points uniformly, leading to an oversampling of dense areas, whereas ADASYN has no fixed ratio, but is based on learning difficulty. ADASYN will generate more points for these samples with high learning when processing the same dataset. The term \textit{harder to learn} is a bit imprecise, and warrants some further discussion. ADASYN creates as difficulty ratio for each point, representing the imbalance level in the local neighborhood of the point. This is done by comparing the number of majority class instances (non-minority) to the number of minority class instances within a specified radius around a point. Minority points with a large set of majority class neighbors are said to be harder to learn than those. A disadvantage of using ADASYN is that outlier points tend to have greater representation in the resulting dataset.

\subsubsection{Borderline}
In the standard SMOTE approach all members of the minority class are considered in synthetic data generation. In this variant first proposed by \citeauthor{Han2005-ui}, only those points far from the class border are considered. The rationale behind this approach is that points close to the border contribute little to distinguishing one class from the other, and should not form the basis for new data \cite{Han2005-ui}. In this scheme, points are considered noise if all of their neighbors are of the majority class. To be eligible for resampling, however, a point must have both majority and minority class neighbors. The rationale here is that samples close to a class border tend to be misclassified, and that by limiting the resampling to those points with the smallest risk of misclassification, the overall correct classification rate would be improved.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.30]{./figures/borderline.png}
	\caption[Borderline selection of synthetic data points]{In the borderline variant of SMOTE, points close to the border having only majority class neighbors are considered noise, and are not considered as candidates for selection. }
	\label{fig:borderline}
\end{figure}
%
% This article seems to be available only for purchase as the library does not subscribe to that journal
% This is the reference both for borderline and SVM
%


\subsubsection{KMeans}
This variant, like many of the others described here, addresses a weakness in basic SMOTE: points are selected randomly for oversampling consideration. A further downside of the base algorithm is the noise it introduces. As the \citeauthor{Last2017-rh} state in a document introducing the algorithm:
\begin{quote}
Another major concern is that SMOTE may further amplify noise present in the data. This is likely
to happen when linearly interpolating a noisy minority sample, which is located among majority class
instances, and its nearest minority neighbor. \parencite{Last2017-rh}
\end{quote}
This approach differs from algorithms such as \textit{borderline} in that this approach views the class to be adjusted in terms of cluster membership.  In this approach, the clusters are formed with the \textit{kmeans} approach and SMOTE is applied to those clusters with a high portion of the minority class.

\subsubsection{Support Vector Machine}
SVM SMOTE increases the points for the minority class along the decision boundary by generating new points along the lines connecting the support vectors and nearest neighbors \cite{Nguyen2011-cb}. In contrast with the KMeans approach, but in the same spirit as the borderline approach, this approach considers those points more important for estimating the best decision boundary, and therefor the best candidates for synthetic data generation. This approach first uses SMOTE to create new minority class samples and then uses the new minority class to train SVM. The samples that are identified as the most difficult to learn are then candidates for oversampling.

\subsection{Combined Under-sampling and Over-sampling}
\label{section:under}
Correcting the imbalance by under-sampling the majority class can have an unfortunate side-effect: information loss. Under-sampling, in its most basic form, involves discarding data to achieve the desired balance. Before the details of under-sampling are covered, perhaps this is a topic best addressed by a simple example. Consider the case where the majority class has 1000 samples, but only a few of those samples are representative without regard to the relationship between them (random discard), the discard may eliminate enough of those samples to have an appreciable effect when presented with a novel dataset. In weed/crop classification, however, members of the majority class (crop) are not likely to have small sets representative of a subset, especially if the sample set is taken from a single crop.
First proposed in 1976, \citeauthor{Tomek1976-bg} described a mechanism to choose discard samples based on their relationship to members of the minority class \cite{Tomek1976-bg}. Unlike the random selection of candidates, samples must have these characteristics to be considered a \textit{Tomek link}:
\begin{enumerate}
\item{Sample a’s nearest neighbor is b.}
\item{Sample b’s nearest neighbor is a.}
\item{Sample a and b belong to a different class.}
\end{enumerate}
Links with the lowest euclidean distance to  minority class are eliminated.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.30]{./figures/tomek-a.png}
	\caption[Tomek links]{Tomek links are identified by the proximity of minority and majority class samples. Items with the lowest euclidean distance are eliminated.}
	\label{fig:tomek}
\end{figure}
A second common processing technique is using \textit{edited nearest neighbors} (ENN). The fundamental principle underlying ENN is to remove instances that differ from their nearest neighbors, with the assumption that such instances are likely to be mislabeled (differ from their neighbors) or noisy. The subsequent clarification of the decision boundary may contribute to better outcomes when the dataset is used for training.
\begin{enumerate}
\item{Compute the $k$ nearest neighbors for each observation in the dataset.}
\item{Compare the class label (crop or weed) of each instance with the class label of its $k$ nearest neighbors.}
\item{Remove instances from the majority class that differ from their neighbors}
\end{enumerate}

Typically, both under-sampling and oversampling techniques are used to address the imbalance, as \citeauthor{Batista2004-qz} describes \cite{Batista2004-qz}. 

\subsection{Assessing Techniques}
To assess the efficacy of over-sampling the minority and a combined over- and under-sampling approach, random instances in a dataset were first dropped to achieve five imbalance ratios and then restored to a 1:1 ratio between the two classes using the over-sampling techniques discussed. Models using the imbalanced data and the artificially balanced data were then compared in terms of the improvement (or degradation) of the Area Under the Curve (AUC) using nine different techniques: Random Forest, Extra Trees, Gradient Boosting, KNN, Linear Discriminant Analysis, Logistic Regression, Multi-Layer Perceptron, Random Forest, and Support Vector Machine. The Receiver Operating Characteristic (ROC) curve represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative. The AUC is the area underneath this curve -- the closer this area is to 1.0, the better a model is said to be.

As Figure \ref{fig:imbalance} shows, substituting synthetic data improves the AUC of various classification techniques in most cases, but some of the impact is quite trivial and -- in cases -- detrimental (note the case of using a \textit{Random Forest} approach to classification was almost always made worse by these approaches). 
%
% The imbalanced data and analysis requires 4 steps
%
%# 1) Copy the files to be analyzed
%# 2) Classify the vegetation automatically
%# 3) Correct the classifications manually
%# 4) Analyze for imbalance
%
% In post directory:
% make data-for-imbalance
% make classify-for-imbalance
% make review-for-imbalance
% This step should be run twice, 
% make analyze-imbalance INPUT=/cygdrive/d/maricopa-test/imbalance/processed/final/corrected.csv OUTPUT=imbalance.csv RATIO=30:5-10 STEPS=6 IMBALANCE=ALL-COMBINED
% make analyze-imbalance INPUT=/cygdrive/d/maricopa-test/imbalance/processed/final/corrected.csv OUTPUT=imbalance.csv RATIO=30:5-10 STEPS=6 IMBALANCE=ALL-OVER
%
% The actual analysis is performed by this command
% python ./imbalance.py -df ../jetson/training-from-dataframe.csv -ini ../jetson/standalone.ini -lg ../jetson/standalone-logging.ini -a ALL --classifier ALL
%
% The graph for the figure is created with the imbalanced.R script
% imbalance.R
%
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{./figures/imbalance.png}
	\caption[Class imbalance correction techniques]{The impact on AUC of class over-sampling imbalance correction techniques for various ratios and classification strategies can be seen in this visualization. Each facet of this display shows the effect of a different over-sampling technique. While many classification techniques are improved, the effect on Decision Tree classification is most pronounced. This dataset was first manipulated to achieve the desired ratio before the algorithms were applied. That is, to achieve a 30:1 imbalance, rows in the minority class were randomly dropped. The change in the AUC scores is shown in this plot for various ML techniques and for each of the correction approaches discussed.}
	\label{fig:imbalance}
\end{figure}

While the difference in the AUC achieved with classification using MLP and SVM (some clarification may be in order: SVM refers to a classification, and SVM-SMOTE refers to the correction technique.) stands out, almost all classification techniques were improved by the correction with a few exceptions. Correcting low-imbalance sets (30:5) yielded worse results in several instances. The postive impacts, however, can be characterized as minimal, however. Consider the ROC curves shown in Figure \ref{fig:auc}, showing the ROC curve achieved before and after correction using ADASYN.

\begin{figure*}[h]
	\centering
	\includegraphics[height=0.25\textheight]{figures/roc-corrected.png}
	\caption[Before and after correction]{Before and after correction by oversampling the minority class with the ADASYN approach. Note that in many instances the effect of the correction on the ROC curve is quite modest. The dashed green line appearing in each of the graphs is representative of a random classifier, and appears only as a reference. A positive impact on classification using a gradient boosted technique is present, and that technique can certainly benefit from correction. }
	\label{fig:auc}
\end{figure*}

Combining over- and under-sampling techniques yielded worse results in many instances, but produced better results in most. As Figure \ref{fig:combined} shows, the impact tended to be more positive than negative.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{./figures/combined.png}
	\caption[Combined Over and Under-sampling]{The impact on AUC of using a combined over- and under-sampling approach to class imbalance correction techniques for various ratios and classification strategies can be seen in this visualization. The strategy of using SMOTE+ENN has a particularly negative effect for many of the classification techniques, but the improvement in Decision Tree classification is a clear standout for both techniques.}
	\label{fig:combined}
\end{figure}

As with over-sampling the minority class, the effect was still modest, however. As seen in Figure \ref{fig:auc-tomek}, the ROC curves of uncorrected and corrected are, in general, quite close.  A notable exception was the difference in the curves shown by \textit{Gradient Boosting}. That classification technique visibly benefited from the combined approach of over- and under-sampling.

\begin{figure*}[h]
	\centering
	\includegraphics[height=0.25\textheight]{figures/roc-corrected-tomek.png}
	\caption[Before and after correction]{Before and after correction by a combined approach of over-sampling the minority class and under-sampling the majority class using Tomek links. Note that in many instances the effect of the correction is quite modest. A positive impact on classification using a gradient boosted technique is present, and that technique can certainly benefit from correction. }
	\label{fig:auc-tomek}
\end{figure*}

%
% S Y S T E M  D E S I G N
%
\section{System Design}
\subsection{Database}
The database held two collections\footnote{The database selected for this project is a document-oriented database, MongoDB. While a detailed discussion of a document-oriented database is beyond the scope of this document, an overly simple comparison to the more frequently encountered relational (SQL) database is to consider collections to be a bit like tables in that model.}, \textit{images} and \textit{blobs}.

The \textit{images} collection held records relating to each of the images gathered:
\begin{itemize}
	\item{\textit{Latitude}, the geographic latitude where the image was acquired}
	\item{\textit{Longitude}, the geographic longitude where the image was acquired}
	\item{\textit{AGL}, the distance above the ground where the image was acquired}
	\item{\textit{Crop}, the crop in the image}
	\item{\textit{Date acquired}, the date the image was acquired}
	\item{\textit{Segmentation Results}, the resulting image of various segmentation algorithms given in Table~\ref{tab:segmentation-formulae}}	
	\item{\textit{Processing Results}, the resulting image of various learning approaches}	
	\item{\textit{Hash}, an identifier of the image}	
\end{itemize}
 While the collection holds resulting images, the database holds only information about the resulting image, not the image itself. The image itself is too large to efficiently store in the database, and is stored in a file on disk. While most of the remaining attributes are widely familiar (and many are directly abstracted from the image EXIF data), the notion of an image \textit{hash} warrants mention here. The hash of an image is the unique fingerprint of the image's bits. This allows the software to definitively determine if a specific image is already within the database. This assessment, however, is not without an important caveat: any modification -- even to an image's EXIF metadata -- will result in an image that will be considered distinct to an otherwise nearly identical image. As image records are inserted into the database only after pre-processing steps -- such as color correction-- are complete, this caveat will ordinarily not affect operation.
 
 The \textit{blobs} collection held records relating to each plants:
 \begin{itemize}
 	\item{\textit{Factors}, the color, shape, and texture factors detailed in prior sections}	
	\item{\textit{Classification}, the predicted class of the blob}	
	\item{\textit{Actual}, the actual class of the blob}	
	\item{\textit{Machine Learning Approach}, the machine learning approach used}	
	\item{\textit{Factors Used}, the factors used for classification}	
	\item{\textit{Parent}, the image in which this blob is found}
	\item{\textit{Hash}, an identifier of the blob}	
 \end{itemize}

There is a 1:N relationship between an image and the blobs found within that image, but the relationship is actually between the segmented image and the blobs found within that segmented image, as the size of the blob will vary based on the segmentation approach.

% 
% I M A G E  A C Q U I S I T I O N
%
\section{Acquisition of Images in Study Area and Processing}
Image acquisitions were carried out at the Maricopa Agricultural Center (MAC) on a cantaloupe planting (N 33.061857, E 111.967145) consisting of 3 water treatments: flood, drip, and center-pivot, each watered at 80\% and 100\%. Only the flood and pivot treatments will be imaged and considered for this study. This is mostly motivated by practical considerations: it is not anticipated that the drip treatment plots will have a significant number of weeds.  Image acquisition will take place on at regular intervals beginning in April 2024 and continuing through May 2024 (Cantaloupe), and October 2024 continuing through November 2024 (Broccoli).  Ambient lighting conditions in acquisitions were quite similar, as they were carried out around the same time (mid-morning) and under (almost) cloudless skies.

Images were captured at several distances AGL: 20cm, 1m, 2m, 5m, and 10m.

With the exception of images taken from 20cm, all images were acquired using a DJI Mavic Air 2S with a stock RGB 20 MP sensor. The UAV camera has the following relevant details:

\begin{itemize}
	\item{13.25mm sensor width}
	\item{8.38mm focal length}
	\item{5464 pixel width}
	\item{3640 pixel height}
\end{itemize}

Images gathered at 20cm were taken with an iPhone 14 Pro, using the 48MP main camera.

\begin{itemize}
	\item{7.3mm sensor width}
	\item{8.38mm focal length}
	\item{8063 pixel width}
	\item{6048 pixel height}
\end{itemize}

Missions were planned and executed with Dronelink software, a commercially available package.  Where feasible, missions were conducted in the same ambient light conditions, but an exact match of ambient conditions using uncontrolled lighting is not always possible.
As illustrated by Figure \ref{fig:field-layout} the largest flood irrigated area is \SI{4536}{\metre\squared}, a location where image acquisition took place. While images of this reduced subset will reduce the presence of plants in the adjacent drip irrigation plots, this will not ensure that they are not imaged.  Consider the size of the ground captured with a UAV flight at various heights:

\begin{table*}[h!] \centering
\begin{tabular}{SSSSSSSS} \toprule
    {AGL (m)} & {Captured Width (m)} & {Images} \\ \midrule
    1  & 2 & xxx \\
    2  & 3  & xxx  \\
    5  & 8  & xxx  \\
    10  &16  & xxx     \\ \bottomrule
\end{tabular}
\caption{Ground captured at various distances AGL}
\label{table:captured}
\end{table*}
% Metadata altitude analysis is with this command
% python altitude.py -i d:\maricopa\corrected\2024-05-08\flood-1m d:\maricopa\corrected\2024-05-08\pivot-2m d:\maricopa\corrected\2024-05-08\pivot-5m d:\maricopa\corrected\2024-05-08\flood-10m
As Table \ref{table:captured} shows, the capture area is approaching the width of three plots at 10 meters AGL. While capturing images in the adjacent plots was undesired, doing so had no appreciable affects in classification. While the missions are planned for these heights, environmental factors (wind) and positional accuracy may result in variance of the actual distance AGL achieved. While both factors will vary from one mission execution to another, the image metadata examined from a mission executed on 8 May 2024 shows fairly low standard deviation values for the distance AGL at various heights: 0.06m (1m AGL), 0.08m (2m AGL), 0.08m (5m AGL), and 0.05m (10m AGL). Positional accuracy, as it relates to ground points was not assessed in this study. While and RTK solution could certainly improve positional accuracy along all axes in reporting the exact location of an image, such precision is not particularly relevant to this study, as absolute location will not be used as a factor in classification.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{./figures/test-area.png}
	\caption[Field layout for image acquisition]{The train and test subsets of the 80\% and 100\% water treatment of the flood irrigation is considered for this study. As the drip irrigation is not likely to show many weeds, it was not considered for this study.}
	\label{fig:field-layout}
\end{figure}

Images were color, white-balance, and exposure corrected against images taken before each flight of a Spyder CHECKR24 calibration target using Adobe Lightroom$^{\circledR}$. The digital negative of each image (DNG, a standard format for raw sensor data) was corrected and converted to JPG format for subsequent processing.\footnote{The merits of raw, uncompressed, and unprocessed sensor data versus compressed, processed formats like JPG are beyond the scope of this document, but it is fair to say taking raw sensor data, processing, and then producing a compressed JPG is precisely what consumer products like phone cameras do. So that the color correction step can be inserted, the process of producing an image that can be viewed with most software is altered a bit, but essentially unchanged from what would be expected from a consumer device. The images in all digital cameras start out as the values from a sensor; many cameras simply provide this in a convenient to use format, and JPG is the most widely usable. It is also fair to say that a format like JPG does not actually represent what the sensor ``sees'', but is the result of the in-camera processing of sensor data. This workflow also results in a JPG image set, but separates image acquisition from processing. Color correction, for instance, is more than a bit easier with raw sensor data than a JPG due to  the much wider color spectrum present in the raw sensor data (16.7 million vs 68.7 billion is a big difference)}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{./figures/color-correction.jpg}
	\caption[Color correction target and image set identification]{All images are color corrected for each set. Prior to each collection, this photo was taken with the camera used for the acquisition. The color target captured was subsequently used to create a correction profile for that image set.}
	\label{fig:color-correction}
\end{figure}

As illustrated in Figure \ref{fig:field-layout}, the train and test image sets acquired were physically separate. The goal is simple to state: items in the training subset should never appear in the testing subset. The side-lap and front-lap of UAV acquired images makes this task much more difficult, although any set of images of adjacent objects will exhibit this complication. Consider a UAV image acquisition of three plants, as shown in Figure \ref{fig:uav-overlap}. In this example, plants (even if it is only a portion) appear in multiple images. While this is expected -- and even desired for stitching the images together -- this complicates splitting the image set into train and test segments such that the same plant does not appear in both segments. The most pragmatic approach here is to simply avoid the problem by not performing a split at all. Collecting, instead, a completely separate train and test set of images. This approach has an additional benefit in that a test set can now be acquired from a different part of the field. Consider the case where the test set is taken from the \textit{drip} treatment area. While it will not have a particularly high weed load, it should be possible to classify the crop in that image set with the same accuracy as seen in the image set taken in the \textit{flood} treatment. While this study did not examine classification accuracy across water treatment methods, the clean separation of the train and test segments also means that test segments can be captured for different crops or different portions of the same field.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{./figures/overlap.png}
	\caption[Image overlap in UAV images]{The same plant appears in multiple images. As seen here, plants \#1 and \#3 appear in two (fully in images \#1 and \#2, partially in images \#3 and \#4), and plant \#3 appears in full in four images. A train/test split is possible, of course, but requires knowledge of what an image contains.}
	\label{fig:uav-overlap}
\end{figure}

To identify specific plants from various altitudes, distinct objects were placed on the ground near the plants to be sampled. For this purpose, round bucket lids with a unique set of dots applied to identify each lid were used. The plant nearest the center of the lid could then be identified from various altitudes.
\begin{figure}[h!]
	\centering
	\includegraphics[width=8cm]{./figures/lids.jpg}
	\label{fig:bucket-lids}
	\caption[Identification of specific plants using bucket lids]{Bucket lids with a unique number of round stickers applied to them are used to identify specific plants from various altitudes. At lower distances AGL only a single plant may be in view. At higher distances AGL, however, multiple plants and multiple lids might be in view or the plant may be shifted within the frame. This necessitates having a mechanism to uniquely identify a specific lid. The number of stickers uniquely identifies the lid and stickers are identified in the image by their shape. The intent of this is not so much to identify the lid as it is to identify the closest vegetation to that lid.}
\end{figure}	

Bucket lids are identified by the number of colored stickers they contain -- fortunately the stickers are an easily identifiable, uniform shape: a circle. Identifying a lid consists of two steps: isolating the lids from everything else in the image, and identifying the stickers. For lid isolation, the image is segmented just as would be done with isolating vegetation. In this case, the image is converted to the HSV space and only those pixels with the blue hue of the bucket lid appear in the final image. Round shapes are identified in the segmented image using a variant of the Hough Transform \parencite{Ballard1981-oc}. The most salient limitation to this technique, of course is that a round object that has the same hue of blue as the lid will be identified as a lid. This limitation may be a hindrance to using this technique more widely on a diverse set of images, but will suffice for the purpose of this study.
\begin{figure}[h!]
	\centering
	\includegraphics[width=6cm]{./figures/lid-segmented.jpg}
	\label{fig:bucket-segmented}
	\caption[Identification of specific bucket lids]{The lids themselves are isolated the same way vegetation is, by applying an index. In this case, the index isolates the blue hue found in the lid, but is not found in the study area. The stickers on the lid and the lid itself are both an easily detected shape, a circle. This figure shows both treatments: the lid itself is separated from the background and circles within the image are highlighted in green. This image shows bucket lid \# 2, as three circles are found: one for the lid itself, and two stickers. Identification of the circle that is the lid is more important than it first appears, as only those stickers completely contained within the lid contribute to its identity.}
\end{figure}	


\section{Feature Selection}
The features described in the previous section were generated for a set of segmented images, resulting in each plant being described by 715 attributes. Several techniques were then used to explore the relationship between these attributes and the labeled class: Recursive Feature Elimination, Principle Component Analysis (PCA), Feature Importance, and Univariate feature selection.  Specifically, only the most important variables will be selected in the predictions, and those parameters with a weak association are dropped. Unfortunately, different feature ranking techniques identified sets of the most significant features without a clear overlap. There were exceptions to this, of course, as can be seen with the feature \textit{saturation\_mean} identified with the in multiple of the techniques. Among the four techniques there are 10 unique parameters.

%
% MARICOPA
%
% This is what the parameter selection technique shows
%
% Output of this command
% make selections TRAINING=d:\maricopa\processed\2024-05-01\pivot-2m\final\training.csv FACTORS=4 PREFIX=maricopa TYPE=all SUBTYPE=all OUTPUT=d:\maricopa\processed\2024-05-01\pivot-2m\final\classification FORMAT=LATEX
% python Selection.py -df d:\maricopa\processed\2024-05-01\pivot-2m\final\training.csv -fs ALL -f 4 -l -lg logging.ini -of LATEX -co -ty all -s all -p maricopa
% 
% Manual edits
% 1) Remove leading spaces in header
% 2) Remove leading spaces in first data column

{
\begin{small}
% Reduce the row spacing, so to table looks a bit more packed
\renewcommand{\arraystretch}{0.9}

% Begin copied table
\begin{longtable}{lllll}
\caption[Parameter Rankings]{Parameter Rankings}\\
\toprule
{} & Recursive &          PCA &                   Importance &                   Univariate \\
\midrule
\endfirsthead
\caption[]{Parameter Rankings} \\
\toprule
{} & Recursive &          PCA &                   Importance &                   Univariate \\
\midrule
\endhead
\midrule
\multicolumn{5}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
0& greyscale\_contrast\_avg &  shape\_index &  greyscale\_dissimilarity\_avg &              saturation\_mean \\
1 & saturation\_mean &  compactness &                     solidity &  greyscale\_dissimilarity\_avg \\
2 & greyscale\_dissimilarity\_avg &    convexity &    greyscale\_correlation\_avg &       greyscale\_contrast\_avg \\
3 & greyscale\_correlation\_avg &   elongation &                    roundness &                    roundness \\
\end{longtable}

% End copied table
\end{small}
}


 To clarify the names of the parameters
 \begin{itemize}
 	\item{\textit{in\_phase} -- the in-phase component of the YIQ colorspace}
 	\item{\textit{ycbcr\_y\_contrast\_avg} -- the mean of the contrast GLCM computation of the $Y$ component of the YCbCr colorspace}
 \end{itemize}
 	

The set of unique parameters across the top 10 selection techniques was relatively small, yielding a search space of 348,330,136 combinations. This parameter space was exhaustively searched to find the set of parameters best suited for each of the  seven classification techniques:
\begin{itemize}
	\item{Random Forest (Abbreviated to RF in sections of this document)}
	\item{K Nearest Neighbors (KNN)}
	\item{Gradient Boosting (GRADIENT}
	\item{Logistic Regression (LR)}
	\item{Decision Tree (DT)}
	\item{Support Vector Machine (SVM)}
	\item{Linear Discriminate Analysis (LDA)}
	\item{Multi-layer Perceptron (MLP)}
	\item{Extra Trees (Extra)}
\end{itemize}

As there was no clear agreement between the parameters identified by the selection mechanisms, the results were analyzed to assess the outcome of using a given selection technique and how similar 
\footnote{The similarity between two lists is given here by the \textit{cosine similarity}, a technique that considers the elements of an array as vectors in n-dimensional space. In this case, there is a bit of pre-processing of the data, as a list of factors is not something that directly lends itself to computation analysis. Instead, the arrays are converted to lists of the frequency of each term. While this may sound uninteresting at first glance, this information can be used to determine how similar two lists are. The formula $\frac {(A \cdot B)} { \lVert A \rVert * \lVert B \rVert }$ is used to produce that metric.}
the parameters selected by that technique were to the ones identified by other techniques. 

\subsection{Texture}
Table \ref{table:texture} shows the optimal parameters for each classification technique if only texture is considered.

\begin{tiny}

% Copied from this command
% make optimal-results TYPE=TEXTURE SUBTYPE=all FORMAT=latex PARAMETERS=parameters.pickle LOGGING=tucson-logging.ini ALL_RESULTS=/cygdrive/d/maricopa-test/dissertation/ CAPTION_LONG="Classification using texture" CAPTION_SHORT="Classification using texture" LABEL=table:texture PREFIX=dissertation_auc



\begin{longtable}{lcl}
\caption[Classification using texture]{Classification using texture}
\label{table:texture}\\
\toprule
Technique &  AUC &                                                                       Parameters \\
\midrule
\endfirsthead
\caption[]{Classification using texture} \\
\toprule
Technique &  AUC &                                                                       Parameters \\
\midrule
\endhead
\midrule
\multicolumn{3}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
DECISIONTREE & 0.88 &         cie\_b\_lbp\_variance hog\_variance ycbcr\_cr\_lbp\_variance ycbcr\_y\_lbp\_stddev \\
       EXTRA & 0.90 &                     hog\_mean hog\_stddev ycbcr\_cr\_lbp\_variance ycbcr\_y\_lbp\_stddev \\
    GRADIENT & 0.90 & hog\_variance hsi\_intensity\_lbp\_variance ycbcr\_cr\_lbp\_variance ycbcr\_y\_lbp\_stddev \\
         KNN & 0.90 &                hog\_mean hog\_stddev hsi\_intensity\_lbp\_variance ycbcr\_y\_lbp\_stddev \\
         LDA & 0.89 &                     hog\_mean hog\_stddev ycbcr\_cr\_lbp\_variance ycbcr\_y\_lbp\_stddev \\
         MLP & 0.89 &                              hog\_mean hog\_stddev hog\_variance ycbcr\_y\_lbp\_stddev \\
RANDOMFOREST & 0.90 &              hog\_mean hog\_variance hsi\_intensity\_lbp\_variance ycbcr\_y\_lbp\_stddev \\
         SVM & 0.89 &   cie\_b\_lbp\_variance hog\_stddev hsi\_intensity\_lbp\_variance ycbcr\_cr\_lbp\_variance \\
\end{longtable}


% End copy
\end{tiny}

\subsection{Color}
Table \ref{table:color} shows the optimal parameters for each classification technique if only color is considered.

\begin{tiny}

% Generated with this command
% make optimal-results TYPE=COLOR SUBTYPE=all FORMAT=latex PARAMETERS=parameters.pickle LOGGING=tucson-logging.ini ALL_RESULTS=/cygdrive/d/maricopa-test/dissertation/ CAPTION_LONG="Classification using color" CAPTION_SHORT="Classification using color" LABEL=table:color PREFIX=color_auc

\begin{longtable}{lcl}
\caption[Classification using color]{Classification using color}
\label{table:color}\\
\toprule
Technique &  AUC &                           Parameters \\
\midrule
\endfirsthead
\caption[]{Classification using color} \\
\toprule
Technique &  AUC &                           Parameters \\
\midrule
\endhead
\midrule
\multicolumn{3}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
DECISIONTREE & 0.82 & cb\_mean hue in\_phase saturation\_mean \\
EXTRA & 0.91 & cb\_mean hue in\_phase saturation\_mean \\
GRADIENT & 0.89 & cb\_mean hue in\_phase saturation\_mean \\
KNN & 0.79 & cb\_mean hue in\_phase saturation\_mean \\
LDA & 0.84 & cb\_mean hue in\_phase saturation\_mean \\
LOGISTIC & 0.87 & cb\_mean hue in\_phase saturation\_mean \\
MLP & 0.87 & cb\_mean hue in\_phase saturation\_mean \\
RANDOMFOREST & 0.91 & cb\_mean hue in\_phase saturation\_mean \\
SVM & 0.71 & cb\_mean hue in\_phase saturation\_mean \\
\end{longtable}



\end{tiny}

\subsection{Shape}
\subsection{Optimal}

%
% MARICOPA 
%
%

% Generate the optimal combination data
% make optimal TRAINING=d:\maricopa\processed\2024-05-01\pivot-2m\final\training.csv FACTORS=4 PREFIX=maricopa TYPE=all SUBTYPE=all OUTPUT=d:\maricopa\processed\2024-05-01\pivot-2m\final\classification
%
% Produce the similarity report
% 
\begin{small}
{
% Reduce the row spacing, so to table looks a bit more packed
\renewcommand{\arraystretch}{0.9}

% Copied from the output of
% python results.py -c maricopa_parameters.pickle  -n 1 -o latex -p maricopa --summary -od d:\maricopa\processed\2024-05-01\pivot-2m\final\classification --short "This is a short caption" --long "This is a long caption"
% Manual changes
% 1) The headers need the leading spaces removed 
%     Doesn't seem to be a problem when tiny font is used, so perhaps this is space related (?)
% 2) The long and short captions updated

\begin{longtable}{lrrrrr}
\caption[Classification results for various techniques]{Classification results for various techniques}
\label{table:xxxx}\\
\toprule
Technique &  Precision &  Recall &   F1 &  MAP &  AUC \\
\midrule
\endfirsthead
\caption[]{Classification results for various techniques} \\
\toprule
Technique &  Precision &  Recall &   F1 &  MAP &  AUC \\
\midrule
\endhead
\midrule
\multicolumn{6}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
DECISIONTREE &       0.88 &    0.89 & 0.88 & 0.87 & 0.87 \\
       EXTRA &       0.90 &    0.93 & 0.91 & 0.90 & 0.96 \\
       EXTRA &       0.88 &    0.93 & 0.90 & 0.89 & 0.95 \\
    GRADIENT &       0.90 &    0.91 & 0.91 & 0.90 & 0.96 \\
    GRADIENT &       0.90 &    0.91 & 0.91 & 0.90 & 0.96 \\
         KNN &       0.85 &    0.88 & 0.86 & 0.85 & 0.92 \\
         KNN &       0.79 &    0.91 & 0.85 & 0.82 & 0.88 \\
         KNN &       0.85 &    0.88 & 0.86 & 0.85 & 0.92 \\
         LDA &       0.89 &    0.94 & 0.92 & 0.90 & 0.95 \\
         LDA &       0.89 &    0.95 & 0.92 & 0.91 & 0.95 \\
    LOGISTIC &       0.90 &    0.94 & 0.92 & 0.91 & 0.95 \\
    LOGISTIC &       0.89 &    0.95 & 0.92 & 0.90 & 0.95 \\
         MLP &       0.90 &    0.92 & 0.91 & 0.90 & 0.96 \\
         MLP &       0.88 &    0.95 & 0.92 & 0.91 & 0.96 \\
         MLP &       0.87 &    0.93 & 0.90 & 0.88 & 0.96 \\
RANDOMFOREST &       0.89 &    0.91 & 0.90 & 0.89 & 0.95 \\
RANDOMFOREST &       0.87 &    0.93 & 0.90 & 0.88 & 0.96 \\
         SVM &       0.89 &    0.94 & 0.91 & 0.90 & 0.95 \\
\end{longtable}


% END MARICOPA
}
\end{small}

In Table \ref{table:matches-selection}, Decision Tree classification, for instance, shows a 98\% accuracy with the parameter set identified by \textit{PCA}, but that parameter set is only 25\% similar to the one identified by \textit{Recursive Feature Elimination}. Of special note here is \textit{Feature Importance}. Top accuracy results never matched any of the parameters identified with that technique -- thus the similarity scores of 0 in the table.

\begin{small}
{
% Reduce the row spacing, so to table looks a bit more packed
\renewcommand{\arraystretch}{0.9}

% Begin copied table from the output of this command and slightly modified
%  python results.py -c maricopa_parameters.pickle  -n 1 -o latex -p maricopa -od d:\maricopa\processed\2024-05-01\pivot-2m\final\classification --short "Similarity to selection technique" --long "Similarity to selection technique" -s 1.0
% Manual modification
% 1) Replace header with the column span version
% 2) Delete leading spaces


\begin{longtable}{lrrrrr}
\caption[Similarity to selection technique]{Similarity to selection technique}
\label{table:matches-selection}\\
\toprule
% Begin edits
& & \multicolumn{4}{c}{Similarity to Parameters Found}\\
\cmidrule{3-6} 
\multicolumn{1}{c}{Classification} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
% End edits
\midrule
\endfirsthead
\caption[]{Similarity to selection} \\
\toprule
% Begin edits
& & \multicolumn{4}{c}{Similarity to Parameters Found}\\
\cmidrule{3-6} 
\multicolumn{1}{c}{Classification} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
% End edits
\midrule
\endhead
\midrule
\multicolumn{6}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
DECISIONTREE & 0.69 &      0.00 & 1.00 &       0.00 &       0.00 \\
DECISIONTREE & 0.81 &      1.00 & 0.00 &       0.50 &       0.75 \\
DECISIONTREE & 0.84 &      0.75 & 0.00 &       0.50 &       1.00 \\
DECISIONTREE & 0.81 &      0.50 & 0.00 &       1.00 &       0.50 \\
       EXTRA & 0.68 &      0.00 & 1.00 &       0.00 &       0.00 \\
       EXTRA & 0.84 &      1.00 & 0.00 &       0.50 &       0.75 \\
       EXTRA & 0.88 &      0.75 & 0.00 &       0.50 &       1.00 \\
       EXTRA & 0.84 &      0.50 & 0.00 &       1.00 &       0.50 \\
    GRADIENT & 0.72 &      0.00 & 1.00 &       0.00 &       0.00 \\
    GRADIENT & 0.85 &      1.00 & 0.00 &       0.50 &       0.75 \\
    GRADIENT & 0.87 &      0.75 & 0.00 &       0.50 &       1.00 \\
    GRADIENT & 0.85 &      0.50 & 0.00 &       1.00 &       0.50 \\
         KNN & 0.70 &      0.00 & 1.00 &       0.00 &       0.00 \\
         KNN & 0.81 &      1.00 & 0.00 &       0.50 &       0.75 \\
         KNN & 0.81 &      0.75 & 0.00 &       0.50 &       1.00 \\
         KNN & 0.84 &      0.50 & 0.00 &       1.00 &       0.50 \\
         LDA & 0.72 &      0.00 & 1.00 &       0.00 &       0.00 \\
         LDA & 0.84 &      1.00 & 0.00 &       0.50 &       0.75 \\
         LDA & 0.88 &      0.75 & 0.00 &       0.50 &       1.00 \\
         LDA & 0.86 &      0.50 & 0.00 &       1.00 &       0.50 \\
    LOGISTIC & 0.72 &      0.00 & 1.00 &       0.00 &       0.00 \\
    LOGISTIC & 0.83 &      1.00 & 0.00 &       0.50 &       0.75 \\
    LOGISTIC & 0.88 &      0.75 & 0.00 &       0.50 &       1.00 \\
    LOGISTIC & 0.87 &      0.50 & 0.00 &       1.00 &       0.50 \\
         MLP & 0.64 &      0.00 & 1.00 &       0.00 &       0.00 \\
         MLP & 0.86 &      1.00 & 0.00 &       0.50 &       0.75 \\
         MLP & 0.90 &      0.75 & 0.00 &       0.50 &       1.00 \\
         MLP & 0.86 &      0.50 & 0.00 &       1.00 &       0.50 \\
RANDOMFOREST & 0.72 &      0.00 & 1.00 &       0.00 &       0.00 \\
RANDOMFOREST & 0.85 &      1.00 & 0.00 &       0.50 &       0.75 \\
RANDOMFOREST & 0.88 &      0.75 & 0.00 &       0.50 &       1.00 \\
RANDOMFOREST & 0.85 &      0.50 & 0.00 &       1.00 &       0.50 \\
         SVM & 0.70 &      0.00 & 1.00 &       0.00 &       0.00 \\
         SVM & 0.84 &      1.00 & 0.00 &       0.50 &       0.75 \\
         SVM & 0.88 &      0.75 & 0.00 &       0.50 &       1.00 \\
         SVM & 0.87 &      0.50 & 0.00 &       1.00 &       0.50 \\
\end{longtable}




%End copied table
}
\end{small}
Table~\ref{table:optimal-auc} contains the result of this search with the accuracy scores using an average across 5 folds.



\begin{tiny}
% -- begin copied table
\begin{longtable}{lllllllll}
\caption[Optimal Parameters for AUC]{Optimal Parameters by Technique (AUC)}
\label{table:optimal-auc}\\
\toprule
RANDOMFOREST &             KNN &        GRADIENT &         LOGISTIC &       DECISIONTREE &             SVM &
        LDA &                             MLP &            EXTRA \\
\midrule
\endfirsthead
\caption[]{Optimal Parameters by Technique (AUC)} \\
\toprule
RANDOMFOREST &             KNN &        GRADIENT &         LOGISTIC &       DECISIONTREE &             SVM &
        LDA &                             MLP &            EXTRA \\
\midrule
\endhead
\midrule
\multicolumn{9}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
0.986736 &         0.99875 &        0.982639 &         0.999167 &           0.998889 &        0.998472 &
    0.99875 &                        0.999722 &         0.999167 \\
cie\_b\_ASM\_avg &         cb\_mean &   cie\_b\_ASM\_avg &       hog\_stddev &         hog\_stddev &         cb\_mean &
        cie\_b\_ASM\_avg &                   cie\_b\_ASM\_avg &          cb\_mean \\
hsi\_intensity\_dissimilarity\_avg &   cie\_b\_ASM\_avg &      hog\_stddev &         in\_phase &           in\_phase &      hog\_stddev & hsi\_inten
sity\_dissimilarity\_avg & hsi\_intensity\_dissimilarity\_avg &    cie\_b\_ASM\_avg \\
hue &      hog\_stddev &             hue &  saturation\_mean &    saturation\_mean &             hue &                 s
aturation\_mean &                             hue &  saturation\_mean \\
saturation\_mean & saturation\_mean & saturation\_mean & ycbcr\_cr\_ASM\_avg & yiq\_q\_contrast\_avg & saturation\_mean &
     yiq\_q\_energy\_avg &                 saturation\_mean & yiq\_q\_energy\_avg \\
\end{longtable}


% -- end copied table
\end{tiny}

% TEMPORARY


%% Begin Drip/Pivot
%%
%\begin{tiny}
%% Begin copied table
%\begin{longtable}{lrrrrr}
%\caption[Parameter selection similarities]{Parameter selection similarities for Drip/Pivot}
%\label{table:matches-selection}\\
%\toprule
%% Begin edits
%& & \multicolumn{4}{c}{Similarity to Parameters Found}\\
%\cmidrule{3-6} 
%\multicolumn{1}{c}{Technique} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
%% End edits
%\midrule
%\endfirsthead
%\caption[]{long caption} \\
%\toprule
%% Begin edits
%& & \multicolumn{2}{c}{Similarity to Parameters Found}\\
%\cmidrule{3-6} 
%\multicolumn{1}{c}{Technique} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
%% End edits
%\midrule
%\endhead
%\midrule
%\multicolumn{6}{r}{{Continued on next page}} \\
%\midrule
%\endfoot
%
%\bottomrule
%\endlastfoot
%DECISIONTREE & 0.69 &      0.00 & 1.00 &       0.00 &       0.00 \\
%EXTRA & 0.66 &      0.00 & 1.00 &       0.00 &       0.00 \\
%GRADIENT & 0.67 &      0.00 & 1.00 &       0.00 &       0.00 \\
%KNN & 0.64 &      0.00 & 1.00 &       0.00 &       0.00 \\
%LDA & 0.65 &      0.00 & 1.00 &       0.00 &       0.00 \\
%LOGISTIC & 0.67 &      0.00 & 1.00 &       0.00 &       0.00 \\
%MLP & 0.71 &      0.00 & 1.00 &       0.00 &       0.00 \\
%RANDOMFOREST & 0.68 &      0.00 & 1.00 &       0.00 &       0.00 \\
%SVM & 0.69 &      0.00 & 1.00 &       0.00 &       0.00 \\
%\end{longtable}
%\end{tiny}
%
%%
%% End Drip/Pivot
%
%
%\begin{tiny}
%% Begin copied table
%\begin{longtable}{lrrrrr}
%\caption[Parameter selection similarities]{Parameter selection similarities}
%\label{table:matches-selection}\\
%\toprule
%% Begin edits
%& & \multicolumn{4}{c}{Similarity to Parameters Found}\\
%\cmidrule{3-6} 
%\multicolumn{1}{c}{Technique} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
%% End edits
%\midrule
%\endfirsthead
%\caption[]{long caption} \\
%\toprule
%% Begin edits
%& & \multicolumn{2}{c}{Similarity to Parameters Found}\\
%\cmidrule{3-6} 
%\multicolumn{1}{c}{Technique} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
%% End edits
%\midrule
%\endhead
%\midrule
%\multicolumn{6}{r}{{Continued on next page}} \\
%\midrule
%\endfoot
%
%\bottomrule
%\endlastfoot
%DECISIONTREE & 0.979454 &      0.25 & 1.00 &        0.0 &       0.50 \\
%DECISIONTREE & 0.964706 &      0.25 & 0.50 &        0.0 &       1.00 \\
%DECISIONTREE & 0.967690 &      1.00 & 0.25 &        0.0 &       0.25 \\
%GRADIENT & 0.982353 &      0.25 & 1.00 &        0.0 &       0.50 \\
%GRADIENT & 0.955925 &      0.25 & 0.50 &        0.0 &       1.00 \\
%GRADIENT & 0.967690 &      1.00 & 0.25 &        0.0 &       0.25 \\
%KNN & 0.955968 &      0.25 & 1.00 &        0.0 &       0.50 \\
%KNN & 0.958866 &      0.25 & 0.50 &        0.0 &       1.00 \\
%KNN & 0.958866 &      1.00 & 0.25 &        0.0 &       0.25 \\
%LDA & 0.970631 &      0.25 & 1.00 &        0.0 &       0.50 \\
%LDA & 0.976471 &      0.25 & 0.50 &        0.0 &       1.00 \\
%LDA & 0.955968 &      1.00 & 0.25 &        0.0 &       0.25 \\
%LOGISTIC & 0.961807 &      0.25 & 1.00 &        0.0 &       0.50 \\
%LOGISTIC & 0.955968 &      0.25 & 0.50 &        0.0 &       1.00 \\
%LOGISTIC & 0.853325 &      1.00 & 0.25 &        0.0 &       0.25 \\
%MLP & 0.975610 &      0.25 & 1.00 &        0.0 &       0.50 \\
%MLP & 0.970732 &      0.25 & 0.50 &        0.0 &       1.00 \\
%MLP & 0.956098 &      1.00 & 0.25 &        0.0 &       0.25 \\
%RANDOMFOREST & 0.994118 &      0.25 & 1.00 &        0.0 &       0.50 \\
%RANDOMFOREST & 0.982353 &      0.25 & 0.50 &        0.0 &       1.00 \\
%RANDOMFOREST & 0.964749 &      1.00 & 0.25 &        0.0 &       0.25 \\
%SVM & 0.950085 &      0.25 & 1.00 &        0.0 &       0.50 \\
%SVM & 0.961850 &      0.25 & 0.50 &        0.0 &       1.00 \\
%SVM & 0.780051 &      1.00 & 0.25 &        0.0 &       0.25 \\
%\end{longtable}
%\end{tiny}
%
%% End copied table

% LBP %
\begin{tiny}
% Begin copied table from the output of this command and slightly modified
%  python results.py -p reviewed -s 1.0 -c parameters.pickle -o show

\begin{longtable}{lcrrrr}
\caption[short caption]{Parameter selection techniques - LBP}
\label{table:matches-selection}\\
\toprule
% Begin edits
& & \multicolumn{4}{c}{Similarity to Parameters Found}\\
\cmidrule{3-6} 
\multicolumn{1}{c}{Classification} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
% End edits
\midrule
\endfirsthead
\caption[]{Parameter selection techniques} \\
\toprule
% Begin edits
& & \multicolumn{4}{c}{Similarity to Parameters Found}\\
\cmidrule{3-6} 
\multicolumn{1}{c}{Classification} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Recursive} &  \multicolumn{1}{c}{PCA} & \multicolumn{1}{c}{Importance} & \multicolumn{1}{c}{Univariate} \\
% End edits
\midrule
\endhead
\midrule
\multicolumn{6}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot
DECISIONTREE & 0.78 &      1.00 & 1.00 &       1.00 &       1.00 \\
       EXTRA & 0.69 &      1.00 & 1.00 &       1.00 &       1.00 \\
    GRADIENT & 0.77 &      1.00 & 1.00 &       1.00 &       1.00 \\
         KNN & 0.77 &      1.00 & 1.00 &       1.00 &       1.00 \\
         LDA & 0.78 &      1.00 & 1.00 &       1.00 &       1.00 \\
    LOGISTIC & 0.78 &      1.00 & 1.00 &       1.00 &       1.00 \\
         MLP & 0.78 &      1.00 & 1.00 &       1.00 &       1.00 \\
RANDOMFOREST & 0.78 &      1.00 & 1.00 &       1.00 &       1.00 \\
         SVM & 0.78 &      1.00 & 1.00 &       1.00 &       1.00 \\
\end{longtable}


%Endcopiedtable
\end{tiny}
% LBP END %
 
\subsection{Principal Component Analysis}
The Principal Component Analysis (PCA) \parencite{Muller2016-ui} seen in Figure~\ref{fig:pca} shows most of the variance can be explained by only 3 features: \textit{hue}, \textit{saturation\_mean}, and \textit{in\_phase}.
\begin{figure}[h]
	\centering
	\begin{subfigure}[h]{.48\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{figures/pca.png}
		  \caption{Features explaining variance}
		  \label{fig:pca}
	\end{subfigure}
	\begin{subfigure}[h]{.32\textwidth}
	  \centering
	  	
		{
		\centering\settowidth\rotheadsize{\bfseries(our proposal)}
		\renewcommand\theadalign{cl}\renewcommand\cellalign{cl}
		\renewcommand\theadfont{\bfseries}
		\renewcommand\tabcolsep{4pt}\renewcommand\arraystretch{1.25}
		% Make this a bit smaller so it will fit on a page.  Still looks a bit nasty in that it extends to the edge of the page.
		% It's that PCA line that is the trouble, as the values are just too long
		\footnotesize
		
		\begin{longtable}[c]{
		    |l |*{12}{c |} }%
		    \hline
		    %\diagbox[height=1.2\rotheadsize, width=\dimexpr\eqboxwidth{AB}+2\tabcolsep\relax]%
		    %{\raisebox{1.2ex}{Feature}}{\raisebox{-5ex}{Feature}} &
		    {\textbf{Feature}} & {\textbf{Importance Rank}}\\
		    %\rothead{Tool X\\\mbox{(our proposal)}}\\
		    \hline
			hue                      &     0.505089 \\
			saturation\_mean          &     0.209646 \\
			in\_phase                 &     0.073444 \\
			cb\_mean                  &     0.036392 \\
			hog\_stddev               &     0.026874 \\
			hog\_mean                 &     0.020135 \\
			hog\_variance             &     0.017359 \\
			greyscale\_homogeneity\_0  &     0.016602 \\
			greyscale\_homogeneity\_45 &     0.014872 \\
			greyscale\_homogeneity\_90 &     0.012052 \\	    
		    
		    \hline
		    %\caption{Feature Importance Ranks using RFE with a Random Forest Classifier}
		    %\label{fig:random-forest}
		  \end{longtable}
		 }
	  \caption{PCA Variances}
  %\label{fig:sub2}
	\end{subfigure}
\caption{Principal Component Analysis of Features}
\label{fig:pca}
\end{figure}




 
  Given these observations, we can use these parameters for our training and prediction:
 \begin{itemize}
	\item{The length/width ratio}
	\item{The shape index}
	\item{The YIQ in-phase mean}
	\item{The distance from the cropline}
\end{itemize}

Plotting these attributes along with their classification via Logistic Regression yields Figure~\ref{fig:factors}. This plot shows that items correctly classified as weeds are are tightly grouped. Vegetation correctly classified as crop shows a somewhat looser, but still quite evident grouping.
 \begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{./figures/plot-factors.png}
	\caption[Factors selected for discrimination]{A plot of the four factors selected with the shape noting the true classification of the blob. In this graph the factors selected occupy space and color, with the shape of the object noting the classification.}
	\label{fig:factors}
\end{figure}
  
\section{Classification}
Before delving into the details of classification, it is worthwhile to consider an aspect of the images that must be considered in classification: most of the vegetation appearing is crop; very few of the plants are actually weeds. While the scarcity of weeds is certainly a good thing for farmer, that scarcity has some odd effects on the data. If a dataset contains attributes of 1000 crop plants and 100 weeds, for instance, a 99\% accurate classifier is easily achieved by simply classifying every plant as a crop (a reminder than overall accuracy claims are almost always dubious, and are rightly treated as suspect -- here, a high accuracy claim is perilously close to useless). \citeauthor{Fernandez2018-fw}  deals with the topic of imbalanced data (sets where a majority class is seen several times more often as a minority class) at length in his book on the subject, outlining details of solutions to this imbalance between majority (crop) and minority (weed) classes: sampling, algorithmic changes, and cost-sensitive learning \parencite{Fernandez2018-fw}. Sampling corrections fall into two categories: under sample the majority to lower the number in the final set or over sample the minority class to lower the imbalance ratio. While over sampling the minority class has a set of problems that are best summarized as \textit{``there may be only a few samples to even use -- we need to synthesize more''}, under sampling the majority class has its own set of problems that comes down to choosing the \textit{best} samples that are representative of the class. Synthesizing new instances of the minority class and the using those in an over sampling technique are frequently addressed by using Synthetic Minority Oversampling Technique (SMOTE). Numerous variants of SMOTE exist as do alternatives\footnote{\citeauthor{Fernandez2018-fw} discuss the details of SMOTE, SMOTE variants and extensions, and alternative approaches (such as ADASYN, which addresses linear dependencies between synthetic data points) in their book on imbalanced learning \parencite[p.~101]{Fernandez2018-fw}. While the details of these techniques is beyond the scope of this paper, the pre-processing techniques described in that publication will be used in this problem space.} While choosing between the two techniques (over-sample minority or under-sample majority) is frequently done, applying a hybrid technique of both reducing the majority and generating new minority class samples to address the imbalance is also performed. Here, \citeauthor{Fernandez2018-fw} describe two techniques: SMOTE+Tomek Link, and SMOTE+ENN. Tomek links exist between instances of different classes that are close to each other. Removing the majority class instances subsequently increases the distance separating the two classes. The Edited Nearest Neighbor (ENN) algorithm removes instances whose class label differs from two of three nearest neighbors.
Cost sensitive learning formalizes the concept that some mistakes are more costly than others. Consider the case of weed control: mistakenly stating that a weed is a crop and not treating it is not immediately costly, as no vegetation is altered, but mistakenly stating that a crop is a weed and then applying a treatment is immediately costly, as marketable vegetation has been damaged.

Using the subset of the parameters identified in the previous section, various approaches were used in classifying the objects isolated in the images and detailed in Table~\ref{fig:learning}. The feature sets were split into a 80\%/20\% for these results: 

{\renewcommand{\arraystretch}{2}%
\begin{table}[H]
	\centering
    \caption{Learning Results}
    \label{fig:learning}
    \begin{tabular}{  l  p{4cm}  p{5cm} }
     %\begin{tabular}{  l  p{3.4cm}  p{3.4cm} }
        \toprule
\textbf{Method}      
& \textbf{Train}   
& \textbf{Test} \\\midrule
Random Forest      
& 1.0
& 0.986 \\\hline
Logistic Regression
& 0.9790       
& 0.9787 \\\hline
Decision Tree
& 1.0
& 0.96 \\\hline
Boosted Gradient     
& 1.0
& 0.957 \\\hline
KNN     
& $1.0$                    
& $0.86$ \\\hline
        \bottomrule
    \end{tabular}
\end{table}

For this sample dataset, all approaches with the exception of KNN yield commercially acceptable results if accuracy were the only criterion used in evaluation. However this case must consider two other criteria that are important, but beyond the scope of this document:
\begin{itemize}
\item{The computational cost of achieving each result. The prediction will be done in real-time under field conditions\footnote{And on a GPU, not on a CPU as was done here} While a result of an approach yielding results above 98\% may sound impressive, if that prediction using logistic regression takes a few hundred milliseconds and a decision tree takes well under 100 milliseconds, it may be the case that using that logistic regression in practice is not viable.\footnote{Some context is probably needed here. This prediction will be done as a tractor is moving along, and the time spent on prediction is part of an overall time budget that will ultimately limit the forward speed of the tractor. 2 MPH is commercially viable. 1 MPH is not.}}
\item{The cost of misclassifying a crop as a weed has very real dollar cost, as the subsequent killing of that plant would negatively impact the total crop suitable for sale. The cost of classifying a weed as a crop (and thus not flagging it for treatment) is much lower. While it may have modest financial impact, as leaving the weed in place allows it to take resources away from marketable vegetation (water, fertilizer), the impact of doing so may be acceptable in most instances.}
\end{itemize}
 
Misclassification rates where crop is identified as a weed is the most pressing concern here, so it is worthwhile evaluating the performance of these approaches with the goal of minimizing that class of misclassification while maximizing the correct classification of weeds as weeds. Figure~\ref{fig:classification-rates} addresses this goal by showing percentages of correct and incorrect classifications. While there several approaches showed quite similar acceptable  results (SVM, Random Forest, and Logistic Regression) and three approaches with unacceptable results (KNN, Boosted Gradient, and Decision Tree). While it is likely the case that the unacceptable approaches can be optimized to the point that their results would migrate them to the acceptable category, this paper will concentrate only on the approaches will acceptable results.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{./figures/classification-rates.png}
	\caption[Misclassification rates of various algorithms]{The correct and misclassification rates of various algorithms are shown here. While an overall accuracy of 95.7\% may sound impressive for the Boosted Gradient algorithm (See Table~\ref{fig:learning}), considering the salient misclassification and correct classification rates, 24.1\% and 98.6\%, respectively, that algorithm does not support the accuracy needed in this use case.}
	\label{fig:classification-rates}
\end{figure}

Figure~\ref{fig:classified} shows an annotated classification result. In this figure, we see desirable (crop) bounded by a green box, and undesirable (not crop) bounded in red. The blue box bounds vegetation for which we can't reliably determine its class. Computations that depend on an unimpeded view of the vegetation ({\it length/width ratio}, for instance) can't be reliably performed, so classification of elements that are not fully captured are not classified. Some color computations could be, but may not be as representative as required. The {\it hue} value, for example, is the mean value across the entire plant, and the hue may not be consistent across the entire plant. The portion that is visible may not be representative of the plant as a whole.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{./figures/classified-result.jpg}
	\caption[A classified and annotated image]{A classified result. Desirable vegetation is bounded in green and undesirable in red. Note that this image also demonstrates a commonly encountered problem, that of overlapping vegetation. In the lower left of the green bounding box there is a weed that is overlapped by crop vegetation. In this case, however, the weed's proximity to the crop makes treatment problematic in that doing so represents a risk of producing crop damage.}
	\label{fig:classified}
\end{figure}


\section{Treatment Maps}

\section{Future Work}

\newpage
\section{References}
\printbibliography[heading=none]

%\cite{Wirth2004-li}
\end{document}

